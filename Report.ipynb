{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collaboration and Competition with MADDPG\n",
    "\n",
    "---\n",
    "In this project, we use the Unity ML-Agents environment to demonstrate how multi-agent deep deterministic policy gradient (MADDPG) can be used to solve collaboration and competition problems. This is the third project of the Deep Reinforcement Learning Nanodegree. Make sure you follow the steps outlined in the README file to set up the necessary packages and environment.\n",
    "\n",
    "In this implementation, we use the two agents environment.\n",
    "\n",
    "\n",
    "### 1. Start the Environment\n",
    "\n",
    "We begin by importing the necessary packages.  If the code cell below returns an error, please revisit the project instructions to double-check that you have installed [Unity ML-Agents](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Installation.md) and [NumPy](http://www.numpy.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import numpy as np\n",
    "\n",
    "import random\n",
    "import torch\n",
    "\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from maddpg import MADDPG, ReplayBuffer\n",
    "\n",
    "import os\n",
    "from utilities import transpose_list, transpose_to_tensor, convert_to_tensor\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will start the environment!  **_Before running the code cell below_**, change the `file_name` parameter to match the location of the Unity environment that you downloaded.\n",
    "\n",
    "- **Mac**: `\"path/to/Tennis.app\"`\n",
    "- **Windows** (x86): `\"path/to/Tennis_Windows_x86/Tennis.exe\"`\n",
    "- **Windows** (x86_64): `\"path/to/Tennis_Windows_x86_64/Tennis.exe\"`\n",
    "- **Linux** (x86): `\"path/to/Tennis_Linux/Tennis.x86\"`\n",
    "- **Linux** (x86_64): `\"path/to/Tennis_Linux/Tennis.x86_64\"`\n",
    "- **Linux** (x86, headless): `\"path/to/Tennis_Linux_NoVis/Tennis.x86\"`\n",
    "- **Linux** (x86_64, headless): `\"path/to/Tennis_Linux_NoVis/Tennis.x86_64\"`\n",
    "\n",
    "For instance, if you are using a Mac, then you downloaded `Tennis.app`.  If this file is in the same folder as the notebook, then the line below should appear as follows:\n",
    "```\n",
    "env = UnityEnvironment(file_name=\"Tennis.app\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: TennisBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 8\n",
      "        Number of stacked Vector Observation: 3\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 2\n",
      "        Vector Action descriptions: , \n"
     ]
    }
   ],
   "source": [
    "env = UnityEnvironment(file_name='env\\Tennis.app')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environments contain **_brains_** which are responsible for deciding the actions of their associated agents. Here we check for the first brain available, and set it as the default brain we will be controlling from Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Examine the State and Action Spaces\n",
    "\n",
    "In this environment, two agents control rackets to bounce a ball over a net. If an agent hits the ball over the net, it receives a reward of +0.1.  If an agent lets a ball hit the ground or hits the ball out of bounds, it receives a reward of -0.01.  Thus, the goal of each agent is to keep the ball in play.\n",
    "\n",
    "The observation space consists of 24 variables corresponding to the position and velocity of the ball and racket. Two continuous actions are available, corresponding to movement toward (or away from) the net, and jumping. \n",
    "\n",
    "Run the code cell below to print some information about the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents: 2\n",
      "Size of each action: 2\n",
      "There are 2 agents. Each observes a state with length: 24\n",
      "The state for the first agent looks like: [[ 0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.         -6.65278625 -1.5\n",
      "  -0.          0.          6.83172083  6.         -0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.         -6.4669857  -1.5\n",
      "   0.          0.         -6.83172083  6.          0.          0.        ]]\n"
     ]
    }
   ],
   "source": [
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "\n",
    "# number of agents \n",
    "num_agents = len(env_info.agents)\n",
    "print('Number of agents:', num_agents)\n",
    "\n",
    "# size of each action\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Size of each action:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "states = env_info.vector_observations\n",
    "state_size = states.shape[1]\n",
    "print('There are {} agents. Each observes a state with length: {}'.format(states.shape[0], state_size))\n",
    "print('The state for the first agent looks like:', states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        , -6.65278625, -1.5       , -0.        ,  0.        ,\n",
       "         6.83172083,  6.        , -0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        , -6.4669857 , -1.5       ,  0.        ,  0.        ,\n",
       "        -6.83172083,  6.        ,  0.        ,  0.        ]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env_info.vector_observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0, 0.0]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env_info.rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[False, False]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env_info.local_done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "actions = [[700,700],[700,-700]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "actions_array = torch.stack(transpose_to_tensor(actions)).detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "actions_for_env = np.rollaxis(actions_array,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "env_info_demo = env.step(actions_for_env)[brain_name] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        , -6.65278625, -1.5       ,\n",
       "        -0.        ,  0.        ,  6.83172083,  6.        , -0.        ,\n",
       "         0.        , -3.65278697, -0.98316395, 30.        ,  6.21520042,\n",
       "         6.83172083,  5.94114017, 30.        ,  6.21520042],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        , -6.4669857 , -1.5       ,\n",
       "         0.        ,  0.        , -6.83172083,  6.        ,  0.        ,\n",
       "         0.        , -3.46698642, -1.55886006, 30.        , -0.98100001,\n",
       "        -6.83172083,  5.94114017, 30.        , -0.98100001]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env_info_demo.vector_observations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Multi-Agent Deep Deterministic Policy Gradient (MADDPG)\n",
    "\n",
    "#### Key Concept:\n",
    "\n",
    "In the previous Continuous Control with DDGP project, we successfully used DDPG to teach a double-jointed arm to move to target locations. The action space was continous and we had to use actor and critic design. However, in this environment, we have 2 agents playing against each other. In order to solve this, we are using a multi-agent version of DDPG in this project.\n",
    "\n",
    "\n",
    "#### Actor and Critic\n",
    "\n",
    "In DDPG, we use a seperate network to determine what the best action is for each state. This is the actor network. We use another network to model the expected value of a state + action combination. As mentioned in the DDPG paper, it is important to use targets for both actor and critic networks to help our models stablize.\n",
    "\n",
    "#### Centralized Critic input\n",
    "\n",
    "In this project, we will train two DDPG agents with their own actor and critic. However, the critics do not act in silo. Instead, each critic will also take \n",
    "\n",
    "\n",
    "##### Normalization\n",
    "Normalization is extrememly important for this task. When state inputs have different dimensions, the differences in magnitude would cause our networks to be unstable. In this implementation, we applied the weight normalization for all non-last layers.  For more details, see `networkforall.py`\n",
    "\n",
    "#### DDPG Agent\n",
    "\n",
    "Just like DQN, having a relay buffer and seperating target and current networks are two key ideas that allow the model to learn. More specifically:\n",
    "\n",
    "1. Relay Buffer:\n",
    "Instead of using (state, action, reward) tuples in their natural order, our agent stores a bunch of such tuples in a relay buffer. In each iteration, at each time step, the agent will put the new (state, action, reward) tuple in to the buffer and pull out a random batch of tuples to update the networks\n",
    "\n",
    "2. Target Q vs. Current Q:\n",
    "At each step, instead of updating the current network according to values in the current network, we use a target network that only gets updated to the current network slowly. This prevents the networks from chasing after a moving target and helps the agent to learn better. We apply this concept to both actor and critic.\n",
    "\n",
    "##### Learning Steps\n",
    "Having two networks make things slightly more complicated. Here are the major steps that happen during learning:\n",
    "1. Pick the next action using target actor network\n",
    "2. Obtain the corresponding Q-value estimate using target critic network\n",
    "3. Update the current critic using updated target Q values by minimizing the mse between the expected local Q and target Q\n",
    "4. Obtain the predicted action using current actor network\n",
    "5. Update the current actor by following the action-value gradient\n",
    "6. Soft update the target networks with a small fraction of the current networks\n",
    "\n",
    "For more information on DDPG, please see `ddpg_agent.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Train MADDPG!\n",
    "\n",
    "To deploy our agent to solve the navigation problem, we first import the agent class we wrote. When training the environment, set train_mode=True, so that the line for resetting the environment looks like the following:\n",
    "\n",
    "```python\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# setting parameters\n",
    "# number of parallel environment, each environment has 2 agents\n",
    "# this would generate more experience and smooth things out\n",
    "# PARALLEL_ENVS = 1\n",
    "# Here we only have 1 env for simplicity\n",
    "\n",
    "# number of training episodes.\n",
    "# change this to higher number to experiment. say 30000.\n",
    "NUMBER_OF_EPISODES = 6000\n",
    "EPISODE_LENGTH = 1000\n",
    "BATCHSIZE = 200\n",
    "\n",
    "# amplitude of OU noise\n",
    "# this slowly decreases to 0\n",
    "# instead of resetting noise to 0 every episode, we let it decrease to 0 over a few episodes\n",
    "NOISE = 1\n",
    "NO_NOISE_AFTER = 40000\n",
    "NOISE_DECAY = 0.9999\n",
    "MIN_BUFFER_SIZE = 10000\n",
    "BUFFER_SIZE = 1000000\n",
    "\n",
    "IN_ACTOR_DIM = 24 \n",
    "HIDDEN_ACTOR_IN_DIM = 300\n",
    "HIDDEN_ACTOR_OUT_DIM = 100\n",
    "OUT_ACTOR_DIM = 2\n",
    "\n",
    "# Critic input contains both states AND all the actions of all the agents\n",
    "# there are 2 agents, so 24*2 + 2*2 = 28\n",
    "IN_CRIT_DIM = IN_ACTOR_DIM  * num_agents + action_size * num_agents\n",
    "HIDDEN_CRIT_IN_DIM = 200\n",
    "HIDDEN_CRIT_OUT_DIM = 80\n",
    "OUT_CRIT_DIM = 1\n",
    "\n",
    "# how many periods before update\n",
    "UPDATE_EVERY = 20\n",
    "SEED = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\kathl\\\\Desktop\\\\Data Science\\\\Udacity\\\\DeepReinf\\\\CollaborationCompetitionWithMADDPGDraft'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## agent can only have one negative reward each episode\n",
    "## when it gets out, its over\n",
    "## and both agetns are over at the same time\n",
    "## but when one gets a postive, it can still go on!\n",
    "## so now, going to push the rewards only when we have at least one success hit. If not don't push."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1 1 28 30 427 [(-0.03193012, 0.2692544), (0.20518495, 0.17144242)]\n",
      "0.1 1 42 48 722 [(0.9137176, -0.30156916), (0.16899443, -0.54591906)]\n",
      "0.1 1 44 30 866 [(-0.18241909, -0.26968563), (-0.12184572, -0.08567905)]\n",
      "0.1 1 62 44 1215 [(-0.8653891, 0.33459708), (0.04567094, -0.9818289)]\n",
      "0.1 1 75 31 1514 [(0.36852983, -0.20088026), (0.16863576, 0.5808885)]\n",
      "Episode 100\tAverage Last 100 Episodes Score: 0.00\n",
      "times_updated 0\n",
      "0.1 1 148 32 2665 [(0.37457782, 0.09285021), (0.47647592, -0.33167535)]\n",
      "0.1 1 159 32 2924 [(0.54501146, 0.20213464), (0.20685694, 0.72580034)]\n",
      "0.1 1 173 32 3208 [(0.8639217, 0.23231544), (-0.16728444, -0.25159463)]\n",
      "0.1 1 181 20 3394 [(0.11172579, 0.24077916), (-0.8317204, 0.08684449)]\n",
      "0.1 1 182 49 3486 [(0.25240842, -0.11388537), (-0.06006129, -0.5111423)]\n",
      "Episode 200\tAverage Last 100 Episodes Score: 0.00\n",
      "times_updated 0\n",
      "0.1 1 233 31 4328 [(0.33557937, -0.341532), (0.29665688, -0.20772816)]\n",
      "0.1 1 235 31 4439 [(0.67642593, 0.01812917), (0.12637056, -0.35087308)]\n",
      "0.1 1 247 41 4702 [(0.09235664, -0.45375174), (0.26359025, -0.51395833)]\n",
      "0.1 1 270 48 5155 [(0.7792102, 0.15680334), (0.04560682, -0.4583922)]\n",
      "0.1 1 300 32 5698 [(0.8628799, 0.23106146), (0.70629424, -0.09657493)]\n",
      "Episode 300\tAverage Last 100 Episodes Score: 0.01\n",
      "times_updated 0\n",
      "0.1 1 335 44 6314 [(-0.34213898, -0.10655114), (0.41502774, -0.45672256)]\n",
      "0.1 1 341 32 6508 [(-0.052963227, -0.36389887), (-0.3260694, -0.43287778)]\n",
      "0.1 1 348 31 6698 [(0.692037, -0.37854666), (0.7367434, 0.1798836)]\n",
      "0.1 1 361 32 6969 [(0.89057064, -0.01572229), (-0.5725622, 0.04371476)]\n",
      "Episode 400\tAverage Last 100 Episodes Score: 0.00\n",
      "times_updated 0\n",
      "0.1 1 439 28 8179 [(-0.5220717, 0.34916627), (0.9625403, -0.22857206)]\n",
      "0.1 1 444 32 8327 [(0.84143376, -0.13604319), (-0.16332547, -1.0)]\n",
      "0.1 1 450 44 8509 [(-0.18944645, -0.35553533), (0.33921012, -0.77670985)]\n",
      "0.1 1 455 29 8686 [(0.7711647, 0.49162546), (-0.7372583, -0.15468423)]\n",
      "0.1 1 476 32 9068 [(0.339728, 0.6156583), (-0.1235884, -0.35124132)]\n",
      "0.1 1 493 27 9395 [(0.34308636, -0.37981057), (-0.31933263, -0.07524282)]\n",
      "Episode 500\tAverage Last 100 Episodes Score: 0.01\n",
      "times_updated 0\n",
      "0.1 1 509 31 9704 [(0.9988232, -0.37666756), (-0.15255281, -0.3254817)]\n",
      "0.1 1 515 31 9871 [(-1.0, -0.09920936), (0.25101775, 0.30249208)]\n",
      "0.1 1 534 29 10226 [(0.38811743, -0.16383456), (-0.093945056, -0.033220142)]\n",
      "0.1 1 538 32 10362 [(-0.8072994, 0.274764), (-0.566786, -0.3690746)]\n",
      "0.1 1 543 19 10509 [(0.5125571, -0.19370686), (-0.30083862, -0.29460818)]\n",
      "0.1 1 548 28 10636 [(-1.0, -0.425471), (0.29716504, -0.033026516)]\n",
      "0.1 1 585 27 11233 [(-1.0, 0.06363879), (0.35155472, 0.24428217)]\n",
      "0.1 1 595 30 11448 [(0.9305994, -0.4967949), (-0.16596645, -0.3382347)]\n",
      "0.1 1 600 32 11606 [(-0.6380584, 0.59395915), (0.32133427, -0.36090976)]\n",
      "Episode 600\tAverage Last 100 Episodes Score: 0.01\n",
      "times_updated 64\n",
      "0.1 1 606 43 11787 [(-0.02260089, 0.05712548), (0.13216308, -0.087115586)]\n",
      "0.1 1 615 38 12027 [(0.9531087, 0.024276726), (-0.58760405, 0.063382745)]\n",
      "0.1 1 622 26 12217 [(0.46885625, 0.13530481), (-0.12722926, -0.3975939)]\n",
      "0.1 1 631 32 12418 [(-0.3449064, -0.014078438), (0.5305046, 0.27327257)]\n",
      "0.1 1 637 22 12578 [(0.8046833, 0.029250775), (-0.47058958, -0.8170624)]\n",
      "0.1 1 642 66 12748 [(0.07353264, -0.15369992), (-0.5425702, -0.5252864)]\n",
      "0.1 1 647 32 12972 [(-0.081605956, 0.58636576), (-0.21663362, 0.070757)]\n",
      "0.1 1 653 30 13140 [(0.19517496, -0.24186376), (0.2511562, 0.40938637)]\n",
      "0.1 1 664 31 13383 [(-0.8629685, -0.2127425), (0.54979706, -0.10235638)]\n",
      "0.1 1 665 30 13478 [(-0.55719143, -0.023151591), (0.66286385, -0.4188057)]\n",
      "0.1 1 682 32 13800 [(0.015294194, -0.19825219), (0.57717144, 0.667172)]\n",
      "0.1 1 694 33 14056 [(-1.0, 0.53208417), (0.5361438, 0.6189581)]\n",
      "Episode 700\tAverage Last 100 Episodes Score: 0.01\n",
      "times_updated 148\n",
      "0.1 1 706 28 14310 [(-0.28851414, 0.35935566), (0.10017791, -0.34832862)]\n",
      "0.1 1 711 30 14456 [(0.41710156, -0.29258895), (-0.5787091, 0.30735844)]\n",
      "0.1 1 731 32 14842 [(-0.74051344, 0.013144016), (0.73958683, 0.40899867)]\n",
      "0.1 1 739 30 15038 [(-0.6752003, -0.13336203), (0.38571525, -0.10827895)]\n",
      "0.1 1 757 50 15396 [(-0.24074009, 0.08642657), (0.24148713, 0.44987106)]\n",
      "0.1 1 760 28 15555 [(-0.018936343, 0.058923684), (0.15785837, -0.08308983)]\n",
      "0.1 1 768 31 15745 [(0.13161086, -0.25165194), (-0.115840614, 0.43217623)]\n",
      "0.1 1 775 49 15948 [(-0.63222015, -0.07026437), (0.47603214, -0.24968831)]\n",
      "0.1 1 785 31 16208 [(-0.28957415, -0.0986562), (0.024387747, 0.14709336)]\n",
      "Episode 800\tAverage Last 100 Episodes Score: 0.01\n",
      "times_updated 230\n",
      "0.1 1 806 32 16589 [(0.51555616, -0.68068117), (-0.24129549, -0.05666642)]\n",
      "0.1 1 838 31 17151 [(-0.9701008, 0.029088633), (-0.1254785, 0.24155137)]\n",
      "0.1 1 854 27 17459 [(-0.80247194, 0.1449447), (0.0625737, -0.47852537)]\n",
      "0.1 1 857 28 17572 [(-0.37270498, 0.44630903), (-0.27796394, -0.108726114)]\n",
      "0.1 1 876 32 17919 [(0.5786358, -0.067363776), (-0.44446126, -0.23194411)]\n",
      "0.1 1 880 27 18055 [(0.42352703, 0.29190728), (-0.14969888, 0.28723246)]\n",
      "0.1 1 886 32 18220 [(-0.33616066, 0.17017412), (0.2806398, -0.5428305)]\n",
      "Episode 900\tAverage Last 100 Episodes Score: 0.01\n",
      "times_updated 309\n",
      "0.1 1 901 32 18542 [(-0.5044843, 0.06489486), (0.1604055, -0.6604444)]\n",
      "0.1 1 926 28 18981 [(-0.76554954, 0.24831477), (-0.2701732, -0.22700168)]\n",
      "0.1 1 947 32 19362 [(-0.05955203, -0.18389772), (-0.14405262, -0.51892304)]\n",
      "0.1 1 959 32 19617 [(0.02510532, 0.24174386), (0.30275092, -0.36283255)]\n",
      "0.1 1 993 30 20192 [(-0.30556643, -1.0), (-0.064000964, -0.3461445)]\n",
      "Episode 1000\tAverage Last 100 Episodes Score: 0.00\n",
      "times_updated 386\n",
      "0.1 1 1015 31 20584 [(0.06574838, -0.15380794), (-0.44804245, -0.6712198)]\n",
      "0.1 1 1017 32 20695 [(0.011144415, 0.19932488), (-0.35881263, -0.114251226)]\n",
      "0.1 1 1027 47 20941 [(-0.037155747, 0.40016872), (-0.38100487, -0.04737088)]\n",
      "0.1 1 1046 32 21325 [(-0.12533236, 0.13583031), (0.39626193, -0.11987174)]\n",
      "0.1 1 1059 32 21597 [(-0.24573241, 0.19812176), (-0.01381633, -0.20878772)]\n",
      "0.1 1 1068 51 21830 [(-0.18364227, -0.10143273), (-0.53418255, -0.30847472)]\n",
      "0.1 1 1083 65 22210 [(-0.13776404, 0.10528726), (0.3297634, 0.13615525)]\n",
      "0.1 1 1087 32 22417 [(-0.4215803, 0.1970643), (-0.36662632, 0.7672724)]\n",
      "Episode 1100\tAverage Last 100 Episodes Score: 0.01\n",
      "times_updated 469\n",
      "0.1 1 1102 29 22721 [(-0.1321604, 0.6466829), (-0.13240029, 0.09961581)]\n",
      "0.1 1 1116 32 23018 [(0.51523757, 0.30775273), (0.2307441, -0.4837975)]\n",
      "0.1 1 1124 32 23217 [(0.90867835, 0.23790729), (-0.49623296, 0.5333113)]\n",
      "0.1 1 1125 31 23315 [(-0.9909188, 0.021418452), (0.39666396, -0.96116275)]\n",
      "0.1 1 1126 32 23412 [(0.56846553, 0.091542415), (0.21169499, 0.08186497)]\n",
      "0.1 1 1147 32 23795 [(0.7351365, -0.06315288), (0.45124722, -0.41729596)]\n",
      "0.1 1 1180 32 24366 [(-0.52133906, 0.3063252), (0.35281646, -0.38198766)]\n",
      "0.1 1 1199 30 24747 [(0.19029392, 0.33065188), (0.12617034, 0.36678392)]\n",
      "Episode 1200\tAverage Last 100 Episodes Score: 0.01\n",
      "times_updated 550\n",
      "0.1 1 1218 17 25084 [(-0.38388953, 0.08034777), (0.20263761, 0.44821683)]\n",
      "0.1 1 1234 34 25371 [(-0.4607054, -0.10715544), (-0.64228666, 0.042362124)]\n",
      "0.1 1 1236 28 25484 [(0.3215375, 0.4501254), (-0.07324211, -0.13213034)]\n",
      "0.1 1 1287 20 26284 [(-0.32203004, 0.2968611), (-0.537044, -0.6004744)]\n",
      "0.1 1 1297 27 26481 [(-0.14811629, 0.26672643), (-0.12058661, 0.3525988)]\n",
      "Episode 1300\tAverage Last 100 Episodes Score: 0.00\n",
      "times_updated 625\n",
      "0.1 1 1322 31 26920 [(0.30430216, -0.037280165), (0.18820147, 0.09198257)]\n",
      "0.1 1 1323 54 27039 [(0.50542086, -0.20119438), (-0.61108273, 0.5307097)]\n",
      "0.1 1 1332 29 27296 [(0.17255908, -0.29623783), (0.5755618, -0.40867102)]\n",
      "0.1 1 1335 21 27407 [(0.43454906, 0.6067596), (-0.08904543, -0.45271072)]\n",
      "0.1 1 1348 24 27646 [(0.27356118, 0.5868325), (0.3365099, -0.25791657)]\n",
      "0.1 1 1368 31 28004 [(0.3728818, 0.0047187954), (0.3079452, 0.38228422)]\n",
      "0.1 1 1369 31 28100 [(-0.2890709, 0.8010772), (0.11246802, -0.1049353)]\n",
      "Episode 1400\tAverage Last 100 Episodes Score: 0.01\n",
      "times_updated 704\n",
      "0.1 1 1404 40 28694 [(-0.124446675, 0.57652247), (0.17924324, -0.3160234)]\n",
      "0.1 1 1406 50 28842 [(-0.6520438, 0.3991096), (0.25144568, -0.69195795)]\n",
      "0.1 1 1409 48 29022 [(-0.24777973, 0.5103217), (-0.5274118, -0.6695398)]\n",
      "0.1 1 1410 31 29152 [(-0.042228207, 0.3870775), (-0.049703598, 0.19921505)]\n",
      "0.1 1 1416 20 29308 [(-0.030305557, -0.07847017), (0.44516098, -0.5339779)]\n",
      "0.1 1 1422 49 29489 [(-0.28060058, -0.078577824), (-0.2745206, -0.090091646)]\n",
      "0.1 1 1431 32 29739 [(0.12917782, 0.047845423), (-0.16798508, -0.08921871)]\n",
      "0.1 1 1433 31 29851 [(0.29441008, -0.05725366), (-0.1890866, -0.14511573)]\n",
      "0.1 1 1447 30 30135 [(0.46847284, 0.408225), (0.40449238, -0.54968345)]\n",
      "0.1 1 1454 31 30314 [(-1.0, 0.19236709), (-0.014735363, -0.13069694)]\n",
      "0.1 1 1455 33 30412 [(0.2201849, 0.08777639), (-0.49403968, -0.43385613)]\n",
      "0.1 1 1462 31 30597 [(0.49439812, -0.31895715), (0.1260849, -0.38627753)]\n",
      "0.1 1 1488 53 31090 [(0.18100592, 0.047927365), (-0.2894218, 0.116576046)]\n",
      "0.1 1 1499 32 31376 [(0.37055027, 0.17958704), (-0.19793886, 0.5648321)]\n",
      "Episode 1500\tAverage Last 100 Episodes Score: 0.01\n",
      "times_updated 794\n",
      "0.1 1 1510 26 31613 [(-0.07477567, -0.5698799), (-0.019485276, 0.45922536)]\n",
      "0.1 1 1521 29 31852 [(0.19759756, 0.29760045), (-0.63393605, -0.43930787)]\n",
      "0.1 1 1523 29 31956 [(0.29204807, 0.2984759), (-0.043899793, 0.0032081902)]\n",
      "0.1 1 1524 30 32047 [(0.018628664, 0.15315005), (0.08695397, 0.017935336)]\n",
      "0.1 1 1567 49 32777 [(-0.06350626, 0.06245672), (-0.11683658, -0.40333867)]\n",
      "0.1 1 1569 43 32935 [(0.22449929, -0.18192185), (0.11829394, -0.6050683)]\n",
      "Episode 1600\tAverage Last 100 Episodes Score: 0.01\n",
      "times_updated 873\n",
      "0.1 1 1621 43 33805 [(0.23255265, 0.4065347), (-0.4443119, 0.5046805)]\n",
      "0.1 1 1655 20 34406 [(-0.065249495, -0.14472479), (-0.21328469, 0.14898011)]\n",
      "0.1 1 1667 32 34637 [(-0.22898674, -0.25282308), (-0.37510198, -0.3399467)]\n",
      "0.1 1 1697 38 35159 [(0.028875023, 0.060688738), (0.13935797, -0.5335485)]\n",
      "Episode 1700\tAverage Last 100 Episodes Score: 0.00\n",
      "times_updated 950\n",
      "0.1 1 1764 26 36251 [(-0.12001349, 0.090607315), (0.40717497, 0.18635193)]\n",
      "0.1 1 1787 20 36647 [(-0.16514106, 0.5003169), (0.35412574, 0.19503568)]\n",
      "Episode 1800\tAverage Last 100 Episodes Score: 0.00\n",
      "times_updated 1025\n",
      "0.1 1 1813 32 37080 [(0.23081428, 0.010652341), (0.24293599, 0.39079446)]\n",
      "0.1 1 1814 28 37175 [(0.67328155, 0.13982096), (-0.3902762, 0.46208426)]\n",
      "0.1 1 1860 27 37929 [(0.22905195, 0.5600666), (-0.07635784, -0.002009064)]\n",
      "0.1 1 1895 25 38494 [(-0.30703008, 0.077695906), (0.44498396, -0.05972079)]\n",
      "Episode 1900\tAverage Last 100 Episodes Score: 0.00\n",
      "times_updated 1100\n",
      "0.1 1 1909 31 38762 [(-0.76342756, -0.16708383), (0.3489715, 0.099007845)]\n",
      "0.1 1 1963 32 39620 [(0.26414847, 0.16949803), (-0.2401645, -0.55933833)]\n",
      "0.1 0.9928255004627402 1989 31 40073 [(0.14994404, -0.39841828), (0.17557439, -0.21830149)]\n",
      "Episode 2000\tAverage Last 100 Episodes Score: 0.00\n",
      "times_updated 1174\n",
      "0.1 0.9379081975196459 2028 19 40706 [(-0.08851069, 0.11530903), (0.1327489, -0.70797706)]\n",
      "0.1 0.8692652142917918 2079 44 41506 [(-0.14377972, 0.18191749), (0.25948718, -0.090888664)]\n",
      "Episode 2100\tAverage Last 100 Episodes Score: 0.00\n",
      "times_updated 1248\n",
      "Episode 2200\tAverage Last 100 Episodes Score: 0.00\n",
      "times_updated 1320\n",
      "Episode 2300\tAverage Last 100 Episodes Score: 0.00\n",
      "times_updated 1391\n",
      "Episode 2400\tAverage Last 100 Episodes Score: 0.00\n",
      "times_updated 1462\n",
      "Episode 2500\tAverage Last 100 Episodes Score: 0.00\n",
      "times_updated 1533\n",
      "Episode 2600\tAverage Last 100 Episodes Score: 0.00\n",
      "times_updated 1604\n",
      "Episode 2700\tAverage Last 100 Episodes Score: 0.00\n",
      "times_updated 1675\n",
      "Episode 2800\tAverage Last 100 Episodes Score: 0.00\n",
      "times_updated 1746\n",
      "Episode 2900\tAverage Last 100 Episodes Score: 0.00\n",
      "times_updated 1817\n",
      "Episode 3000\tAverage Last 100 Episodes Score: 0.00\n",
      "times_updated 1888\n",
      "Episode 3100\tAverage Last 100 Episodes Score: 0.00\n",
      "times_updated 1959\n",
      "Episode 3200\tAverage Last 100 Episodes Score: 0.00\n",
      "times_updated 2030\n",
      "Episode 3300\tAverage Last 100 Episodes Score: 0.00\n",
      "times_updated 2101\n"
     ]
    }
   ],
   "source": [
    "# main function that sets up environments\n",
    "# perform training loop\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "t = 0\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# torch.set_num_threads(PARALLEL_ENVS)\n",
    "# env = envs.make_parallel_env(PARALLEL_ENVS)\n",
    "\n",
    "# keep 5000 episodes worth of replay\n",
    "\n",
    "buffer = ReplayBuffer(int(BUFFER_SIZE*EPISODE_LENGTH))\n",
    "# initialize policy and critic through MADDOG\n",
    "maddpg = MADDPG(IN_ACTOR_DIM, HIDDEN_ACTOR_IN_DIM, HIDDEN_ACTOR_OUT_DIM, OUT_ACTOR_DIM, IN_CRIT_DIM, HIDDEN_CRIT_IN_DIM, HIDDEN_CRIT_OUT_DIM)\n",
    "\n",
    "# these will be used to print rewards for agents\n",
    "agent0_reward = []\n",
    "agent1_reward = []\n",
    "scores_deque = deque(maxlen=100)\n",
    "best_scores = []\n",
    "avg_best_score = []\n",
    "update_t = 0\n",
    "#     max_state =  env_info_demo.vector_observations[0]\n",
    "#     max_action = [0,0]\n",
    "times_updated = 0\n",
    "\n",
    "# use keep_awake to keep workspace from disconnecting\n",
    "for episode in range(1, NUMBER_OF_EPISODES+1):\n",
    "\n",
    "\n",
    "\n",
    "    env_info = env.reset(train_mode=True)[brain_name]\n",
    "    states = env_info.vector_observations\n",
    "\n",
    "\n",
    "    # initialize scores for both agents\n",
    "    scores = [0,0]   \n",
    "    # this resets the noise\n",
    "    maddpg.reset()\n",
    "    all_t_episode = []\n",
    "\n",
    "\n",
    "    for episode_t in range(EPISODE_LENGTH):\n",
    "#             max_state =  find_max_state(max_state, states)\n",
    "\n",
    "\n",
    "        # explore = only explore for a certain number of episodes\n",
    "        # action input needs to be transposed\n",
    "\n",
    "        state_tensors = convert_to_tensor(states)\n",
    "        \n",
    "        \n",
    "        actions = maddpg.act(state_tensors, noise = NOISE)\n",
    "        if len(buffer) > NO_NOISE_AFTER:\n",
    "            NOISE *= NOISE_DECAY\n",
    "\n",
    "\n",
    "\n",
    "        actions_array = torch.stack(actions).detach().numpy()\n",
    "\n",
    "\n",
    "        # act (actions)       \n",
    "        # [tensor([ 0.9857,  0.0912]), tensor([ 0.0951, -0.1229])]\n",
    "        # stack (actions_array)             \n",
    "        # [[ 0.98568964  0.09124897]\n",
    "        #  [ 0.0950533  -0.12286544]]\n",
    "\n",
    "        # step forward one frame\n",
    "\n",
    "        env_info = env.step(actions_array)[brain_name] \n",
    "\n",
    "        next_states = env_info.vector_observations\n",
    "\n",
    "        rewards = env_info.rewards\n",
    "        dones  = env_info.local_done\n",
    "\n",
    "        transition = (states, actions_array, rewards, next_states, dones)\n",
    "        all_t_episode.append(transition)\n",
    "\n",
    "        scores = [sum(x) for x in zip(scores, rewards)]\n",
    "\n",
    "        states = next_states    \n",
    "    \n",
    "    \n",
    "\n",
    "        buffer.push(transition)\n",
    "\n",
    "        update_t = (update_t + 1) % UPDATE_EVERY\n",
    "        \n",
    "        if len(buffer) > MIN_BUFFER_SIZE and update_t == 0:\n",
    "            times_updated += 1\n",
    "\n",
    "            for a_i in range(num_agents):\n",
    "                samples = buffer.sample(BATCHSIZE)\n",
    "                maddpg.update(samples, a_i)               \n",
    "\n",
    "            maddpg.update_targets()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        if dones[0]:\n",
    "            if round(max(scores),1) != 0 :\n",
    "                print(round(max(scores),1), NOISE, episode, episode_t, len(buffer), list(zip(*actions_array)))\n",
    "            elif sum(np.isnan(list(zip(*actions_array))[0])) + sum(np.isnan(list(zip(*actions_array))[1]))  >= 1:               \n",
    "                print(round(max(scores),1), NOISE, episode, episode_t, len(buffer), list(zip(*actions_array)), list(states), list(next_states))                    \n",
    "\n",
    "            break\n",
    "            \n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "    if max(scores) > 0.09:\n",
    "        for i in range(1, 5):\n",
    "            if max(scores) > 0.1 * i - 0.01:\n",
    "\n",
    "                for transition in all_t_episode:\n",
    "                    buffer.push(transition)\n",
    "                    buffer.push(transition)\n",
    "\n",
    "\n",
    "    agent0_reward.append(scores[0])\n",
    "    agent1_reward.append(scores[1])\n",
    "\n",
    "    best_scores.append(max(scores))\n",
    "    scores_deque.append(max(scores))    \n",
    "    avg_best_score.append(np.mean(scores_deque))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # print score every 100 episodes and save model \n",
    "    if episode % 100 == 0 or episode == NUMBER_OF_EPISODES-1 or  np.mean(scores_deque)>=0.5:\n",
    "\n",
    "        print('\\rEpisode {}\\tAverage Last 100 Episodes Score: {:.2f}'.format(episode, np.mean(scores_deque)))\n",
    "        print(\"times_updated\", times_updated)\n",
    "\n",
    "\n",
    "    # problem solved\n",
    "    if  np.mean(scores_deque)>=0.5:\n",
    "        print('\\nEnvironment solved in {:d} episodes!\\tAverage Last 100 Episodes Score: {:.2f}'.format(i_episode, np.mean(scores_deque)))            \n",
    "\n",
    "        break             \n",
    "\n",
    "\n",
    "\n",
    "print(len(buffer))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "drlnd",
   "language": "python",
   "name": "drlnd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
