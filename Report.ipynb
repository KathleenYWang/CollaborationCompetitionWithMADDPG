{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collaboration and Competition with MADDPG\n",
    "\n",
    "---\n",
    "In this project, we use the Unity ML-Agents environment to demonstrate how multi-agent deep deterministic policy gradient (MADDPG) can be used to solve collaboration and competition problems. This is the third project of the Deep Reinforcement Learning Nanodegree. Please follow the steps outlined in the README file to set up the necessary packages and environment.\n",
    "\n",
    "In this implementation, we use the two agents in one single environment.\n",
    "\n",
    "\n",
    "### 1. Start the Environment\n",
    "\n",
    "We begin by importing the necessary packages.  If the code cell below returns an error, please revisit the project instructions to double-check that you have installed [Unity ML-Agents](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Installation.md) and [NumPy](http://www.numpy.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import numpy as np\n",
    "\n",
    "import random\n",
    "import torch\n",
    "\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from maddpg import MADDPG, ReplayBuffer\n",
    "\n",
    "import os\n",
    "from utilities import transpose_list, transpose_to_tensor, convert_to_tensor\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will start the environment!  **_Before running the code cell below_**, change the `file_name` parameter to match the location of the Unity environment that you downloaded.\n",
    "\n",
    "- **Mac**: `\"path/to/Tennis.app\"`\n",
    "- **Windows** (x86): `\"path/to/Tennis_Windows_x86/Tennis.exe\"`\n",
    "- **Windows** (x86_64): `\"path/to/Tennis_Windows_x86_64/Tennis.exe\"`\n",
    "- **Linux** (x86): `\"path/to/Tennis_Linux/Tennis.x86\"`\n",
    "- **Linux** (x86_64): `\"path/to/Tennis_Linux/Tennis.x86_64\"`\n",
    "- **Linux** (x86, headless): `\"path/to/Tennis_Linux_NoVis/Tennis.x86\"`\n",
    "- **Linux** (x86_64, headless): `\"path/to/Tennis_Linux_NoVis/Tennis.x86_64\"`\n",
    "\n",
    "For instance, if you are using a Mac, then you downloaded `Tennis.app`.  If this file is in the same folder as the notebook, then the line below should appear as follows:\n",
    "```\n",
    "env = UnityEnvironment(file_name=\"Tennis.app\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: TennisBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 8\n",
      "        Number of stacked Vector Observation: 3\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 2\n",
      "        Vector Action descriptions: , \n"
     ]
    }
   ],
   "source": [
    "env = UnityEnvironment(file_name='env\\Tennis.app')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environments contain **_brains_** which are responsible for deciding the actions of their associated agents. Here we check for the first brain available, and set it as the default brain we will be controlling from Python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Examine the State and Action Spaces\n",
    "\n",
    "In this environment, two agents control rackets to bounce a ball over a net. If an agent hits the ball over the net, it receives a reward of +0.1.  If an agent lets a ball hit the ground or hits the ball out of bounds, it receives a reward of -0.01.  Thus, the goal of each agent is to keep the ball in play.\n",
    "\n",
    "The observation space consists of 24 variables corresponding to the position and velocity of the ball and racket. Two continuous actions are available, corresponding to movement toward (or away from) the net, and jumping. There has not been much information on the details of the 24 variables and how the two agent inputs are related to each other, making model construction difficult.\n",
    "\n",
    "Run the code cell below to print some information about the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents: 2\n",
      "Size of each action: 2\n",
      "There are 2 agents. Each observes a state with length: 24\n",
      "The state for the agents looks like: [[ 0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.         -6.65278625 -1.5\n",
      "  -0.          0.          6.83172083  6.         -0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.         -6.4669857  -1.5\n",
      "   0.          0.         -6.83172083  6.          0.          0.        ]]\n"
     ]
    }
   ],
   "source": [
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "\n",
    "# number of agents \n",
    "num_agents = len(env_info.agents)\n",
    "print('Number of agents:', num_agents)\n",
    "\n",
    "# size of each action\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Size of each action:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "states = env_info.vector_observations\n",
    "state_size = states.shape[1]\n",
    "print('There are {} agents. Each observes a state with length: {}'.format(states.shape[0], state_size))\n",
    "print('The state for the agents looks like:', states)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Multi-Agent Deep Deterministic Policy Gradient (MADDPG)\n",
    "\n",
    "#### Key Concept:\n",
    "\n",
    "In the previous Continuous Control with DDGP project, we successfully used DDPG to teach a double-jointed arm to move to target locations. The action space was continous and we had to use actor and critic design. However, in this environment, we have 2 agents playing against each other. In order to solve this, we are using a multi-agent version of DDPG in this project.\n",
    "\n",
    "\n",
    "#### Actor and Critic\n",
    "\n",
    "In DDPG, we use a seperate network to determine what the best action is for each state. This is the actor network. We use another network to model the expected value of a state + action combination. As mentioned in the DDPG paper, it is important to use targets for both actor and critic networks to help our models stablize.\n",
    "\n",
    "#### Centralized Critic input\n",
    "\n",
    "In this project, we will train two DDPG agents with their own actor and critic. However, the critics do not act in silo. Instead, each critic will take actions and states from both agents as input, so the critics have complete information from both sides of the table. This differentiates MADDPG from simple DDPG.\n",
    "\n",
    "\n",
    "#### DDPG Agent\n",
    "Having a relay buffer and seperating target and current networks are two key ideas that allow the model to learn. More specifically:\n",
    "\n",
    "1. Replay Buffer:\n",
    "Instead of using (state, action, reward) tuples in their natural order, our agent stores a bunch of such tuples in a relay buffer. In each iteration, at each time step, the agent will put the new (state, action, reward) tuple in to the buffer and pull out a random batch of tuples to update the networks\n",
    "\n",
    "2. Target Q vs. Current Q:\n",
    "At each step, instead of updating the current network according to values in the current network, we use a target network that only gets updated to the current network slowly. This prevents the networks from chasing after a moving target and helps the agent to learn better. We apply this concept to both actor and critic.  \n",
    "  \n",
    "For more details, see `networkforall.py` and `ddpg.py`  \n",
    "  \n",
    "\n",
    "#### MADDPG Agent\n",
    "Each MADDPG agent has 2 DDPG agents. For each learning step, MADDPG agent obtains predicted actions and target actions from both agents and feed that information to the critics of both DDPG agents so that they can perform their individual updates.  \n",
    "For more details, see `maddpg.py`  \n",
    "  \n",
    "#### More on Buffering\n",
    "Buffering is very important for this project. Using only random actions, it's difficult to obtain a play session with more than 2 hits. In order to obtain a total score of 0.2, the ball must be hit at least 3 times which would result in a large number of observations of zero reward. Here, we fill the buffer up first with only play sessions that have hits. Furthermore, for sessions with more than 1 hit, we push the frames closer to the hit multiple times into the buffer. \n",
    "   \n",
    "\n",
    "\n",
    "#### Learning Steps\n",
    "Here are the major steps that happen during the learing step:\n",
    "1. MADDPG agent takes in the buffered states, actions and next states\n",
    "2. MADDOG agent obtains predicted actions and target actions from the up to date DDPG agents\n",
    "3. Each DDPG agents feeds the combined states and actions to their critic network to obtain predicted and target Q value\n",
    "4. Update the current critics using updated target Q values by minimizing the mse between the expected local Q and target Q\n",
    "5. Obtain the predicted action using current actor network for each DDPG agent\n",
    "6. Update the current actors by following the action-value gradient\n",
    "7. Soft update the target networks with a small fraction of the current networks\n",
    "\n",
    "There are more than one way to solve this problem, there are many other ways the MADDPG can be structured."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Train MADDPG!\n",
    "\n",
    "To deploy our agent to solve the navigation problem, we first import the agent class we wrote. When training the environment, set train_mode=True, so that the line for resetting the environment looks like the following:\n",
    "\n",
    "```python\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# setting parameters\n",
    "\n",
    "# number of training episodes.\n",
    "NUMBER_OF_EPISODES = 5000\n",
    "EPISODE_LENGTH = 1000\n",
    "BATCHSIZE =200\n",
    "\n",
    "UPDATE_EVERY = 1\n",
    "# how many periods before update\n",
    "NOISE = 1\n",
    "NO_NOISE_AFTER = 3000\n",
    "NOISE_DECAY = 0.9999\n",
    "# amplitude of OU noise\n",
    "# this slowly decreases to 0\n",
    "# instead of resetting noise to 0 every episode, we let it decrease to 0 over a few episodes\n",
    "\n",
    "MIN_BUFFER_SIZE = 60000\n",
    "BUFFER_SIZE = 500000\n",
    "# fill the buffer before training to MIN_BUFFER_SIZE\n",
    "\n",
    "IN_ACTOR_DIM = 24 \n",
    "HIDDEN_ACTOR_IN_DIM = 400\n",
    "HIDDEN_ACTOR_OUT_DIM = 400\n",
    "OUT_ACTOR_DIM = 2\n",
    "# Critic input contains both states AND all the actions of all the agents\n",
    "IN_CRIT_S = IN_ACTOR_DIM  * num_agents \n",
    "IN_CRIT_A = action_size * num_agents\n",
    "HIDDEN_CRIT_IN_DIM = 400\n",
    "HIDDEN_CRIT_OUT_DIM = 400\n",
    "OUT_CRIT_DIM = 1\n",
    "\n",
    "\n",
    "\n",
    "SEED = 6\n",
    "DISC = 0.99\n",
    "TAU = 0.001\n",
    "LR_ACT = 0.0001\n",
    "LR_CRI = 0.001\n",
    "# The learning rate will be decreased during learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Initialization\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "t = 0\n",
    "\n",
    "# initialize policy and critic through MADDOG\n",
    "maddpg = MADDPG(IN_ACTOR_DIM, HIDDEN_ACTOR_IN_DIM, HIDDEN_ACTOR_OUT_DIM, OUT_ACTOR_DIM,\\\n",
    "                IN_CRIT_S, IN_CRIT_A, HIDDEN_CRIT_IN_DIM, HIDDEN_CRIT_OUT_DIM, SEED, LR_ACT, LR_CRI, DISC, TAU)\n",
    "\n",
    "# these will be used to print rewards for agents\n",
    "agent0_reward = []\n",
    "agent1_reward = []\n",
    "scores_deque = deque(maxlen=100)\n",
    "best_scores = []\n",
    "avg_best_score = []\n",
    "update_t = 0\n",
    "log_path = os.getcwd()+\"/log\"\n",
    "model_dir= os.getcwd()+\"/model_dir\"\n",
    "\n",
    "os.makedirs(model_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def process_data (states, actions, rewards, next_states, dones):\n",
    "    full_state = states.flatten()\n",
    "    next_full_state = next_states.flatten()\n",
    "    return (states, full_state, actions, rewards, next_states, next_full_state, dones)\n",
    "\n",
    "def fill_buffer(buffer):\n",
    "    while len(buffer) <= MIN_BUFFER_SIZE:\n",
    "        env_info = env.reset(train_mode=True)[brain_name]\n",
    "        states = env_info.vector_observations\n",
    "        scores = [0,0] \n",
    "        all_t_episode = []\n",
    "        \n",
    "        for t in range(1000): \n",
    "            state_tensors = convert_to_tensor(states)\n",
    "            actions = maddpg.act(state_tensors, noise = NOISE)\n",
    "            actions_array = torch.stack(actions).detach().numpy()\n",
    "\n",
    "            env_info = env.step(actions_array)[brain_name] \n",
    "            next_states = env_info.vector_observations\n",
    "            rewards = env_info.rewards\n",
    "            dones  = env_info.local_done   \n",
    "\n",
    "            transition = process_data(states, actions_array, rewards, next_states, dones)\n",
    "            \n",
    "            all_t_episode.append(transition)\n",
    "\n",
    "            scores = [sum(x) for x in zip(scores, rewards)]\n",
    "            states = next_states  \n",
    "            if dones[0] and max(scores) < 0.05:\n",
    "                break\n",
    "            elif dones[0] and max(scores) > 0.05:\n",
    "                how_many_times = 1 if sum(scores) < 0.11 else int(sum(scores)/ 0.08) ** int(sum(scores)/ 0.08) *10\n",
    "                \n",
    "                # push successful hit multiple times \n",
    "                for t, transition in enumerate(all_t_episode):\n",
    "                    buffer.push(transition) \n",
    "                    if t+3 > len(all_t_episode):\n",
    "                        continue\n",
    "                    elif max(transition[3]) > 0.05 or max(all_t_episode[t+1][3]) > 0.05  or max(all_t_episode[t+2][3]) > 0.05:\n",
    "                        for j in range(min(how_many_times, 500)):\n",
    "                            buffer.push(transition)                 \n",
    "                                                        \n",
    "                break\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "    print(\"finished pushing to buffer, total length now:\", len(buffer)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished pushing to buffer, total length now: 60988\n"
     ]
    }
   ],
   "source": [
    "buffer = ReplayBuffer(int(BUFFER_SIZE))\n",
    "fill_buffer(buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main training block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 100\tBuffer Len 60988\tAverage Last 100 Episodes Score: 0.00\n",
      "Episode 200\tBuffer Len 60988\tAverage Last 100 Episodes Score: 0.00\n",
      "Episode 300\tBuffer Len 60988\tAverage Last 100 Episodes Score: 0.00\n",
      "Episode 400\tBuffer Len 61018\tAverage Last 100 Episodes Score: 0.00\n",
      "Episode 500\tBuffer Len 61374\tAverage Last 100 Episodes Score: 0.01\n",
      "Episode 600\tBuffer Len 61560\tAverage Last 100 Episodes Score: 0.01\n",
      "Episode 700\tBuffer Len 61590\tAverage Last 100 Episodes Score: 0.00\n",
      "Episode 800\tBuffer Len 61650\tAverage Last 100 Episodes Score: 0.00\n",
      "Episode 900\tBuffer Len 62191\tAverage Last 100 Episodes Score: 0.02\n",
      "Episode 1000\tBuffer Len 62792\tAverage Last 100 Episodes Score: 0.02\n",
      "Episode 1100\tBuffer Len 63629\tAverage Last 100 Episodes Score: 0.02\n",
      "Episode 1200\tBuffer Len 67928\tAverage Last 100 Episodes Score: 0.05\n",
      "Episode 1300\tBuffer Len 72454\tAverage Last 100 Episodes Score: 0.05\n",
      "Episode 1400\tBuffer Len 75994\tAverage Last 100 Episodes Score: 0.04\n",
      "Episode 1500\tBuffer Len 80897\tAverage Last 100 Episodes Score: 0.05\n",
      "Episode 1600\tBuffer Len 94711\tAverage Last 100 Episodes Score: 0.08\n",
      "Episode 1700\tBuffer Len 107854\tAverage Last 100 Episodes Score: 0.07\n",
      "Episode 1800\tBuffer Len 122165\tAverage Last 100 Episodes Score: 0.06\n",
      "Episode 1900\tBuffer Len 139909\tAverage Last 100 Episodes Score: 0.07\n",
      "Episode 2000\tBuffer Len 156197\tAverage Last 100 Episodes Score: 0.07\n",
      "Episode 2100\tBuffer Len 176771\tAverage Last 100 Episodes Score: 0.09\n"
     ]
    }
   ],
   "source": [
    "NOISE = 1\n",
    "for episode in range(1, NUMBER_OF_EPISODES+1):\n",
    "\n",
    "    env_info = env.reset(train_mode=True)[brain_name]\n",
    "    states = env_info.vector_observations\n",
    "\n",
    "    scores = [0,0]   \n",
    "    all_t_episode =[]\n",
    "    \n",
    "    if episode > NO_NOISE_AFTER:\n",
    "        NOISE *= NOISE_DECAY\n",
    "\n",
    "    for episode_t in range(EPISODE_LENGTH):\n",
    "\n",
    "        state_tensors = convert_to_tensor(states)\n",
    "        actions = maddpg.act(state_tensors, noise = NOISE)\n",
    "        actions_array = torch.stack(actions).detach().numpy()\n",
    "\n",
    "        env_info = env.step(actions_array)[brain_name] \n",
    "        next_states = env_info.vector_observations\n",
    "        rewards = env_info.rewards\n",
    "        dones  = env_info.local_done\n",
    "\n",
    "        transition = process_data(states, actions_array, rewards, next_states, dones)\n",
    "        all_t_episode.append(transition)\n",
    "                \n",
    "        scores = [sum(x) for x in zip(scores, rewards)]\n",
    "        states = next_states    \n",
    "        update_t = (update_t + 1) % UPDATE_EVERY\n",
    "        \n",
    "        if len(buffer) > MIN_BUFFER_SIZE and update_t == 0:\n",
    "            samplesa = buffer.sample(BATCHSIZE)\n",
    "            samplesb = buffer.sample(BATCHSIZE)\n",
    "            maddpg.update(samplesa, samplesb)\n",
    "\n",
    "        if dones[0]:\n",
    "            if max(scores) > 0.05 :\n",
    "                how_many_times = 1 if (sum(scores) < 0.11 or len(buffer) == 500000) else int(sum(scores)/ 0.08) ** int(sum(scores)/ 0.08) \n",
    "                for transition in all_t_episode:                                    \n",
    "                    buffer.push(transition) \n",
    "                    if t+3 > len(all_t_episode):\n",
    "                        continue\n",
    "                    elif max(transition[3]) > 0.05 or max(all_t_episode[t+1][3]) > 0.05  or max(all_t_episode[t+2][3]) > 0.05:\n",
    "                        for j in range(min(how_many_times, 200)):\n",
    "                            buffer.push(transition)                       \n",
    "\n",
    "            break                      \n",
    "                    \n",
    "    agent0_reward.append(scores[0])\n",
    "    agent1_reward.append(scores[1])\n",
    "\n",
    "    best_scores.append(max(scores))\n",
    "    scores_deque.append(max(scores))    \n",
    "    avg_best_score.append(np.mean(scores_deque))\n",
    "    \n",
    "    \n",
    "    if np.mean(scores_deque) >= 0.47:\n",
    "        LR_ACT = 0.00001\n",
    "        LR_CRI = 0.00005\n",
    "    elif np.mean(scores_deque) >= 0.45:\n",
    "        LR_ACT = 0.00005\n",
    "        LR_CRI = 0.0001       \n",
    "    elif np.mean(scores_deque) >= 0.40:\n",
    "        LR_ACT = 0.00008\n",
    "        LR_CRI = 0.0005    \n",
    "\n",
    "\n",
    "    # print score every 100 episodes and save model \n",
    "    if episode % 100 == 0 or episode == NUMBER_OF_EPISODES-1 or  np.mean(scores_deque)>=0.5:\n",
    "        print('\\rEpisode {}\\tBuffer Len {}\\tAverage Last 100 Episodes Score: {:.2f}'.format(episode, len(buffer),np.mean(scores_deque)))\n",
    "            \n",
    "        for i in range(num_agents):   \n",
    "            save_dict = {'actor_params' : maddpg.maddpg_agent[i].actor.state_dict(),\n",
    "                         'actor_optim_params': maddpg.maddpg_agent[i].actor_optimizer.state_dict(),\n",
    "                         'critic_params' : maddpg.maddpg_agent[i].critic.state_dict(),\n",
    "                         'critic_optim_params' : maddpg.maddpg_agent[i].critic_optimizer.state_dict()}            \n",
    "\n",
    "            torch.save(save_dict, os.path.join(model_dir, 'checkpoint.pth'))\n",
    "\n",
    "    # problem solved\n",
    "    if  np.mean(scores_deque)>=0.5:\n",
    "        print('\\nEnvironment solved in {:d} episodes!\\tAverage Last 100 Episodes Score: {:.2f}'.format(episode, np.mean(scores_deque)))            \n",
    "        break     \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Observe trained agent\n",
    "\n",
    "After successfully training agent and storing the weights, we can see it in action by running the following cell. The learned agents are able to play with each other quite well!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "for i in range(10):                                         # play game for 10 episodes\n",
    "    env_info = env.reset(train_mode=False)[brain_name]     # reset the environment    \n",
    "    states = env_info.vector_observations                  # get the current state (for each agent)\n",
    "    scores = np.zeros(num_agents)                          # initialize the score (for each agent)\n",
    "    while True:\n",
    "        state_tensors = convert_to_tensor(states)\n",
    "        \n",
    "        actions = maddpg.act(state_tensors, noise = 0)\n",
    "        actions_array = torch.stack(actions).detach().numpy()\n",
    "        env_info = env.step(actions_array)[brain_name]           # send all actions to tne environment\n",
    "        next_states = env_info.vector_observations         # get next state (for each agent)\n",
    "        rewards = env_info.rewards                         # get reward (for each agent)\n",
    "        dones = env_info.local_done                        # see if episode finished\n",
    "        scores += env_info.rewards                         # update the score (for each agent)\n",
    "        states = next_states                               # roll over states to next time step\n",
    "        if np.any(dones):                                  # exit loop if episode finished\n",
    "            break\n",
    "    print('Total score (averaged over agents) this episode: {}'.format(np.mean(scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "ax.axhline(y = 0.5, linewidth=2, color='r')\n",
    "plt.plot(np.arange(1, len(best_scores)+1), best_scores)\n",
    "plt.plot(np.arange(1, len(avg_best_score)+1), avg_best_score)\n",
    "plt.title('Scores by Episode, Target Average Score at 30')\n",
    "\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Future Improvements\n",
    "\n",
    "There are a few ways this solution could be improved. \n",
    "  \n",
    "1. Better understanding of inputs and agents:  \n",
    "Currently the two DDPG agents were treated independently. However, they should be the same because their goal is the same. Better understanding how to transform one agent's learning and observations to be used for the opponent agent, training can be done much faster. In that case, simple DDPG would work because with proper transformation, the two agents can be viewed as playing from the same side and hence be trained using the same agent.    \n",
    "    \n",
    "2. The Deep Neural Nets have not been optimized. We did not experiement with a wide range of hype-parameters to pick and choose the best assumptions that optimize this problem.      \n",
    "    \n",
    "3. Better buffering:   \n",
    "Playing table tennis is difficult, especially with untrained agents. Having good experience in the buffer for training is crucial. We could explore other ways to better incorporate experience in the buffer for better training  \n",
    "    \n",
    "4. Parallel environment:   \n",
    "We could explore training in parallel environment to make training faster    \n",
    "     \n",
    "5. Others: There are other methods that can be used to solve this problem. For example, Trust Region Policy Optimization (TRPO) seems to produce great results for many reinforcement tasks.   \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "drlnd",
   "language": "python",
   "name": "drlnd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
