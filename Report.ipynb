{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collaboration and Competition with MADDPG\n",
    "\n",
    "---\n",
    "In this project, we use the Unity ML-Agents environment to demonstrate how multi-agent deep deterministic policy gradient (MADDPG) can be used to solve collaboration and competition problems. This is the third project of the Deep Reinforcement Learning Nanodegree. Make sure you follow the steps outlined in the README file to set up the necessary packages and environment.\n",
    "\n",
    "In this implementation, we use the two agents environment.\n",
    "\n",
    "\n",
    "### 1. Start the Environment\n",
    "\n",
    "We begin by importing the necessary packages.  If the code cell below returns an error, please revisit the project instructions to double-check that you have installed [Unity ML-Agents](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Installation.md) and [NumPy](http://www.numpy.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import numpy as np\n",
    "\n",
    "import random\n",
    "import torch\n",
    "\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from maddpg import MADDPG, ReplayBuffer\n",
    "\n",
    "import os\n",
    "from utilities import transpose_list, transpose_to_tensor, convert_to_tensor\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will start the environment!  **_Before running the code cell below_**, change the `file_name` parameter to match the location of the Unity environment that you downloaded.\n",
    "\n",
    "- **Mac**: `\"path/to/Tennis.app\"`\n",
    "- **Windows** (x86): `\"path/to/Tennis_Windows_x86/Tennis.exe\"`\n",
    "- **Windows** (x86_64): `\"path/to/Tennis_Windows_x86_64/Tennis.exe\"`\n",
    "- **Linux** (x86): `\"path/to/Tennis_Linux/Tennis.x86\"`\n",
    "- **Linux** (x86_64): `\"path/to/Tennis_Linux/Tennis.x86_64\"`\n",
    "- **Linux** (x86, headless): `\"path/to/Tennis_Linux_NoVis/Tennis.x86\"`\n",
    "- **Linux** (x86_64, headless): `\"path/to/Tennis_Linux_NoVis/Tennis.x86_64\"`\n",
    "\n",
    "For instance, if you are using a Mac, then you downloaded `Tennis.app`.  If this file is in the same folder as the notebook, then the line below should appear as follows:\n",
    "```\n",
    "env = UnityEnvironment(file_name=\"Tennis.app\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: TennisBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 8\n",
      "        Number of stacked Vector Observation: 3\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 2\n",
      "        Vector Action descriptions: , \n"
     ]
    }
   ],
   "source": [
    "env = UnityEnvironment(file_name='env\\Tennis.app')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environments contain **_brains_** which are responsible for deciding the actions of their associated agents. Here we check for the first brain available, and set it as the default brain we will be controlling from Python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Examine the State and Action Spaces\n",
    "\n",
    "In this environment, two agents control rackets to bounce a ball over a net. If an agent hits the ball over the net, it receives a reward of +0.1.  If an agent lets a ball hit the ground or hits the ball out of bounds, it receives a reward of -0.01.  Thus, the goal of each agent is to keep the ball in play.\n",
    "\n",
    "The observation space consists of 24 variables corresponding to the position and velocity of the ball and racket. Two continuous actions are available, corresponding to movement toward (or away from) the net, and jumping. \n",
    "\n",
    "Run the code cell below to print some information about the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents: 2\n",
      "Size of each action: 2\n",
      "There are 2 agents. Each observes a state with length: 24\n",
      "The state for the first agent looks like: [[ 0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.         -6.65278625 -1.5\n",
      "  -0.          0.          6.83172083  6.         -0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.         -6.4669857  -1.5\n",
      "   0.          0.         -6.83172083  6.          0.          0.        ]]\n"
     ]
    }
   ],
   "source": [
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "\n",
    "# number of agents \n",
    "num_agents = len(env_info.agents)\n",
    "print('Number of agents:', num_agents)\n",
    "\n",
    "# size of each action\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Size of each action:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "states = env_info.vector_observations\n",
    "state_size = states.shape[1]\n",
    "print('There are {} agents. Each observes a state with length: {}'.format(states.shape[0], state_size))\n",
    "print('The state for the first agent looks like:', states)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# check weight initilized uncomment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # setting parameters\n",
    "# # number of parallel environment, each environment has 2 agents\n",
    "# # this would generate more experience and smooth things out\n",
    "# # PARALLEL_ENVS = 1\n",
    "# # Here we only have 1 env for simplicity\n",
    "\n",
    "# # number of training episodes.\n",
    "# # change this to higher number to experiment. say 30000.\n",
    "# NUMBER_OF_EPISODES = 6\n",
    "# EPISODE_LENGTH = 1000\n",
    "# BATCHSIZE =3\n",
    "\n",
    "# # amplitude of OU noise\n",
    "# # this slowly decreases to 0\n",
    "# # instead of resetting noise to 0 every episode, we let it decrease to 0 over a few episodes\n",
    "# NOISE = 1\n",
    "# NO_NOISE_AFTER = 10000\n",
    "# NOISE_DECAY = 0.9999\n",
    "# MIN_BUFFER_SIZE = 15000\n",
    "# BUFFER_SIZE = 1000000\n",
    "\n",
    "# IN_ACTOR_DIM = 24 \n",
    "# HIDDEN_ACTOR_IN_DIM = 100\n",
    "# HIDDEN_ACTOR_OUT_DIM = 100\n",
    "# OUT_ACTOR_DIM = 2\n",
    "\n",
    "# # Critic input contains both states AND all the actions of all the agents\n",
    "# # there are 2 agents, so 24*2 + 2*2 = 28\n",
    "# IN_CRIT_S = IN_ACTOR_DIM  * num_agents \n",
    "# IN_CRIT_A = action_size * num_agents\n",
    "# HIDDEN_CRIT_IN_DIM = 100\n",
    "# HIDDEN_CRIT_OUT_DIM = 100\n",
    "# OUT_CRIT_DIM = 1\n",
    "\n",
    "# # how many periods before update\n",
    "# UPDATE_EVERY = 1\n",
    "# SEED = 6\n",
    "# DISC = 0.99\n",
    "# TAU = 0.001\n",
    "# LR_ACT = 1.e-4\n",
    "# LR_CRI = 1.e-3\n",
    "\n",
    "# maddpg = MADDPG(IN_ACTOR_DIM, HIDDEN_ACTOR_IN_DIM, HIDDEN_ACTOR_OUT_DIM, OUT_ACTOR_DIM,\\\n",
    "#                 IN_CRIT_S, IN_CRIT_A, HIDDEN_CRIT_IN_DIM, HIDDEN_CRIT_OUT_DIM, SEED, LR_ACT, LR_CRI, DISC, TAU)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# list(maddpg.maddpg_agent[0].critic.parameters())[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# list(maddpg.maddpg_agent[0].target_critic.parameters())[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Multi-Agent Deep Deterministic Policy Gradient (MADDPG)\n",
    "\n",
    "#### Key Concept:\n",
    "\n",
    "In the previous Continuous Control with DDGP project, we successfully used DDPG to teach a double-jointed arm to move to target locations. The action space was continous and we had to use actor and critic design. However, in this environment, we have 2 agents playing against each other. In order to solve this, we are using a multi-agent version of DDPG in this project.\n",
    "\n",
    "\n",
    "#### Actor and Critic\n",
    "\n",
    "In DDPG, we use a seperate network to determine what the best action is for each state. This is the actor network. We use another network to model the expected value of a state + action combination. As mentioned in the DDPG paper, it is important to use targets for both actor and critic networks to help our models stablize.\n",
    "\n",
    "#### Centralized Critic input\n",
    "\n",
    "In this project, we will train two DDPG agents with their own actor and critic. However, the critics do not act in silo. Instead, each critic will take actions and states from both agents as input, so the critics have complete information from both sides of the table. This differentiates MADDPG from DDPG.\n",
    "\n",
    "\n",
    "#### Normalization\n",
    "Normalization is extrememly important for this task. When state inputs have different dimensions, the differences in magnitude would cause our networks to be unstable. In this implementation, we applied the weight normalization for all non-last layers.  For more details, see `networkforall.py`\n",
    "\n",
    "#### DDPG Agent\n",
    "Having a relay buffer and seperating target and current networks are two key ideas that allow the model to learn. More specifically:\n",
    "\n",
    "1. Relay Buffer:\n",
    "Instead of using (state, action, reward) tuples in their natural order, our agent stores a bunch of such tuples in a relay buffer. In each iteration, at each time step, the agent will put the new (state, action, reward) tuple in to the buffer and pull out a random batch of tuples to update the networks\n",
    "\n",
    "2. Target Q vs. Current Q:\n",
    "At each step, instead of updating the current network according to values in the current network, we use a target network that only gets updated to the current network slowly. This prevents the networks from chasing after a moving target and helps the agent to learn better. We apply this concept to both actor and critic.\n",
    "\n",
    "\n",
    "#### MADDPG Agent\n",
    "Each MADDPG agent has 2 DDPG agents\n",
    "\n",
    "##### Learning Steps\n",
    "Having two networks make things slightly more complicated. Here are the major steps that happen during learning:\n",
    "1. Pick the next action using target actor network\n",
    "2. Obtain the corresponding Q-value estimate using target critic network\n",
    "3. Update the current critic using updated target Q values by minimizing the mse between the expected local Q and target Q\n",
    "4. Obtain the predicted action using current actor network\n",
    "5. Update the current actor by following the action-value gradient\n",
    "6. Soft update the target networks with a small fraction of the current networks\n",
    "\n",
    "For more information on DDPG, please see `ddpg_agent.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Train MADDPG!\n",
    "\n",
    "To deploy our agent to solve the navigation problem, we first import the agent class we wrote. When training the environment, set train_mode=True, so that the line for resetting the environment looks like the following:\n",
    "\n",
    "```python\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\kathl\\\\Desktop\\\\Data Science\\\\Udacity\\\\DeepReinf\\\\CollaborationCompetitionWithMADDPGDraft\\\\updating'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## agent can only have one negative reward each episode\n",
    "## when it gets out, its over\n",
    "## and both agetns are over at the same time\n",
    "## but when one gets a postive, it can still go on!\n",
    "## so now, going to push the rewards only when we have at least one success hit. If not don't push."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set parameters for buffer filling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# setting parameters\n",
    "# number of parallel environment, each environment has 2 agents\n",
    "# this would generate more experience and smooth things out\n",
    "# PARALLEL_ENVS = 1\n",
    "# Here we only have 1 env for simplicity\n",
    "\n",
    "# number of training episodes.\n",
    "# change this to higher number to experiment. say 30000.\n",
    "NUMBER_OF_EPISODES = 3000\n",
    "EPISODE_LENGTH = 1000\n",
    "BATCHSIZE =200\n",
    "MIN_BUFFER_SIZE = 60000\n",
    "UPDATE_EVERY = 1\n",
    "# amplitude of OU noise\n",
    "# this slowly decreases to 0\n",
    "# instead of resetting noise to 0 every episode, we let it decrease to 0 over a few episodes\n",
    "\n",
    "NOISE = 1\n",
    "NO_NOISE_AFTER = 3000\n",
    "NOISE_DECAY = 0.9999\n",
    "\n",
    "BUFFER_SIZE = 300000\n",
    "\n",
    "IN_ACTOR_DIM = 24 \n",
    "HIDDEN_ACTOR_IN_DIM = 400\n",
    "HIDDEN_ACTOR_OUT_DIM = 400\n",
    "OUT_ACTOR_DIM = 2\n",
    "\n",
    "# Critic input contains both states AND all the actions of all the agents\n",
    "# there are 2 agents, so 24*2 + 2*2 = 28\n",
    "IN_CRIT_S = IN_ACTOR_DIM  * num_agents \n",
    "IN_CRIT_A = action_size * num_agents\n",
    "HIDDEN_CRIT_IN_DIM = 400\n",
    "HIDDEN_CRIT_OUT_DIM = 400\n",
    "OUT_CRIT_DIM = 1\n",
    "\n",
    "# how many periods before update\n",
    "\n",
    "SEED = 6\n",
    "DISC = 0.99\n",
    "TAU = 0.001\n",
    "LR_ACT = 0.0001\n",
    "LR_CRI = 0.001\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Initialization\n",
    "def process_data (states, actions, rewards, next_states, dones):\n",
    "    full_state = states.flatten()\n",
    "    next_full_state = next_states.flatten()\n",
    "    return (states, full_state, actions, rewards, next_states, next_full_state, dones)\n",
    "\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "t = 0\n",
    "\n",
    "\n",
    "# torch.set_num_threads(PARALLEL_ENVS)\n",
    "# env = envs.make_parallel_env(PARALLEL_ENVS)\n",
    "\n",
    "# keep 5000 episodes worth of replay\n",
    "\n",
    "\n",
    "# initialize policy and critic through MADDOG\n",
    "maddpg = MADDPG(IN_ACTOR_DIM, HIDDEN_ACTOR_IN_DIM, HIDDEN_ACTOR_OUT_DIM, OUT_ACTOR_DIM,\\\n",
    "                IN_CRIT_S, IN_CRIT_A, HIDDEN_CRIT_IN_DIM, HIDDEN_CRIT_OUT_DIM, SEED, LR_ACT, LR_CRI, DISC, TAU)\n",
    "\n",
    "# these will be used to print rewards for agents\n",
    "agent0_reward = []\n",
    "agent1_reward = []\n",
    "scores_deque = deque(maxlen=100)\n",
    "best_scores = []\n",
    "avg_best_score = []\n",
    "update_t = 0\n",
    "#     max_state =  env_info_demo.vector_observations[0]\n",
    "#     max_action = [0,0]\n",
    "times_updated = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# list(maddpg.maddpg_agent[0].critic.parameters())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def fill_buffer(buffer):\n",
    "    episodes = 0\n",
    "\n",
    "    while len(buffer) <= MIN_BUFFER_SIZE:\n",
    "        env_info = env.reset(train_mode=True)[brain_name]\n",
    "        states = env_info.vector_observations\n",
    "        scores = [0,0] \n",
    "        all_t_episode = []\n",
    "        \n",
    "        for t in range(1000): \n",
    "            state_tensors = convert_to_tensor(states)\n",
    "            actions = maddpg.act(state_tensors, noise = NOISE)\n",
    "            #print(actions)\n",
    "            actions_array = torch.stack(actions).detach().numpy()\n",
    "\n",
    "            env_info = env.step(actions_array)[brain_name] \n",
    "#             env_info = env.step(actions)[brain_name] \n",
    "            next_states = env_info.vector_observations\n",
    "            rewards = env_info.rewards\n",
    "            dones  = env_info.local_done   \n",
    "\n",
    "            transition = process_data(states, actions_array, rewards, next_states, dones)\n",
    "            \n",
    "            all_t_episode.append(transition)\n",
    "\n",
    "            scores = [sum(x) for x in zip(scores, rewards)]\n",
    "            states = next_states  \n",
    "            if dones[0] and max(scores) < 0.05:\n",
    "                break\n",
    "            elif dones[0] and max(scores) > 0.05:\n",
    "                how_many_times = 1 if sum(scores) < 0.11 else int(sum(scores)/ 0.08) ** int(sum(scores)/ 0.08) *10\n",
    "                \n",
    "                print(\"episode\", episodes, \"scores\", scores, \"max t\", t, \"len buf\", len(buffer), int(sum(scores)/ 0.08) )\n",
    "                for t, transition in enumerate(all_t_episode):\n",
    "                    buffer.push(transition) \n",
    "                    if t+3 > len(all_t_episode):\n",
    "                        continue\n",
    "                    elif max(transition[3]) > 0.05 or max(all_t_episode[t+1][3]) > 0.05  or max(all_t_episode[t+2][3]) > 0.05:\n",
    "                        for j in range(min(how_many_times, 500)):\n",
    "                            buffer.push(transition)                 \n",
    "                                                        \n",
    "                break\n",
    "            else:\n",
    "                continue\n",
    "        episodes += 1        \n",
    "        if len(buffer) % 100 == 0 and len(buffer)>1:\n",
    "            print(\"buffer len\", len(buffer),\"episodes\", episodes, \"scores\",scores)\n",
    "    print(len(buffer), episodes, scores)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode 15 scores [-0.009999999776482582, 0.10000000149011612] max t 43 len buf 0 1\n",
      "episode 42 scores [0.0, 0.09000000171363354] max t 29 len buf 47 1\n",
      "episode 50 scores [0.10000000149011612, -0.009999999776482582] max t 41 len buf 80 1\n",
      "episode 54 scores [-0.009999999776482582, 0.10000000149011612] max t 30 len buf 125 1\n",
      "episode 55 scores [-0.009999999776482582, 0.10000000149011612] max t 32 len buf 159 1\n",
      "episode 69 scores [0.10000000149011612, -0.009999999776482582] max t 49 len buf 195 1\n",
      "episode 73 scores [0.10000000149011612, 0.09000000171363354] max t 42 len buf 248 2\n",
      "episode 104 scores [0.10000000149011612, -0.009999999776482582] max t 29 len buf 531 1\n",
      "episode 109 scores [0.10000000149011612, -0.009999999776482582] max t 34 len buf 564 1\n",
      "episode 116 scores [0.10000000149011612, -0.009999999776482582] max t 32 len buf 602 1\n",
      "episode 117 scores [0.10000000149011612, -0.009999999776482582] max t 33 len buf 638 1\n",
      "episode 124 scores [0.10000000149011612, -0.009999999776482582] max t 30 len buf 675 1\n",
      "episode 133 scores [0.09000000171363354, 0.20000000298023224] max t 63 len buf 709 3\n",
      "episode 151 scores [0.10000000149011612, -0.009999999776482582] max t 47 len buf 3203 1\n",
      "episode 167 scores [0.10000000149011612, -0.009999999776482582] max t 32 len buf 3254 1\n",
      "episode 170 scores [-0.009999999776482582, 0.10000000149011612] max t 32 len buf 3290 1\n",
      "episode 181 scores [0.10000000149011612, 0.09000000171363354] max t 47 len buf 3326 2\n",
      "episode 195 scores [-0.009999999776482582, 0.10000000149011612] max t 41 len buf 3614 1\n",
      "episode 198 scores [-0.009999999776482582, 0.10000000149011612] max t 51 len buf 3659 1\n",
      "episode 199 scores [0.09000000357627869, 0.0] max t 18 len buf 3714 1\n",
      "episode 237 scores [0.10000000149011612, 0.09000000171363354] max t 46 len buf 3734 2\n",
      "episode 243 scores [0.09000000171363354, 0.10000000149011612] max t 51 len buf 4021 2\n",
      "episode 292 scores [-0.009999999776482582, 0.10000000149011612] max t 33 len buf 4313 1\n",
      "episode 296 scores [0.10000000149011612, -0.009999999776482582] max t 18 len buf 4350 1\n",
      "episode 310 scores [-0.009999999776482582, 0.10000000149011612] max t 32 len buf 4370 1\n",
      "episode 321 scores [0.0, 0.09000000171363354] max t 29 len buf 4406 1\n",
      "episode 325 scores [-0.009999999776482582, 0.10000000149011612] max t 22 len buf 4439 1\n",
      "episode 335 scores [0.10000000149011612, -0.009999999776482582] max t 34 len buf 4465 1\n",
      "episode 338 scores [0.10000000149011612, -0.009999999776482582] max t 27 len buf 4503 1\n",
      "episode 341 scores [0.10000000149011612, -0.009999999776482582] max t 29 len buf 4534 1\n",
      "episode 352 scores [-0.009999999776482582, 0.10000000149011612] max t 45 len buf 4567 1\n",
      "episode 375 scores [0.0, 0.09000000171363354] max t 31 len buf 4616 1\n",
      "episode 380 scores [-0.009999999776482582, 0.10000000149011612] max t 26 len buf 4651 1\n",
      "episode 399 scores [0.10000000149011612, -0.009999999776482582] max t 53 len buf 4681 1\n",
      "episode 404 scores [-0.009999999776482582, 0.10000000149011612] max t 44 len buf 4738 1\n",
      "episode 418 scores [0.10000000149011612, -0.009999999776482582] max t 33 len buf 4786 1\n",
      "episode 424 scores [0.10000000149011612, -0.009999999776482582] max t 31 len buf 4823 1\n",
      "episode 451 scores [0.10000000149011612, -0.009999999776482582] max t 32 len buf 4858 1\n",
      "episode 458 scores [0.10000000149011612, -0.009999999776482582] max t 30 len buf 4894 1\n",
      "episode 463 scores [-0.009999999776482582, 0.10000000149011612] max t 27 len buf 4928 1\n",
      "episode 464 scores [0.20000000298023224, -0.009999999776482582] max t 48 len buf 4959 2\n",
      "episode 498 scores [0.10000000149011612, 0.09000000171363354] max t 47 len buf 5168 2\n",
      "episode 526 scores [0.09000000171363354, 0.10000000149011612] max t 67 len buf 5456 2\n",
      "episode 548 scores [-0.009999999776482582, 0.10000000149011612] max t 30 len buf 5764 1\n",
      "episode 556 scores [0.10000000149011612, -0.019999999552965164] max t 32 len buf 5798 1\n",
      "episode 567 scores [0.10000000149011612, -0.009999999776482582] max t 32 len buf 5834 1\n",
      "episode 594 scores [-0.009999999776482582, 0.10000000149011612] max t 25 len buf 5870 1\n",
      "episode 596 scores [-0.009999999776482582, 0.10000000149011612] max t 32 len buf 5899 1\n",
      "episode 606 scores [0.0, 0.09000000171363354] max t 29 len buf 5935 1\n",
      "episode 613 scores [0.10000000149011612, -0.009999999776482582] max t 19 len buf 5968 1\n",
      "episode 617 scores [0.09000000171363354, 0.10000000149011612] max t 50 len buf 5989 2\n",
      "episode 627 scores [0.0, 0.09000000171363354] max t 30 len buf 6280 1\n",
      "episode 650 scores [0.09000000171363354, 0.10000000149011612] max t 51 len buf 6314 2\n",
      "episode 664 scores [0.10000000149011612, -0.009999999776482582] max t 30 len buf 6606 1\n",
      "episode 692 scores [0.10000000149011612, -0.009999999776482582] max t 31 len buf 6640 1\n",
      "episode 711 scores [-0.009999999776482582, 0.10000000149011612] max t 24 len buf 6675 1\n",
      "episode 712 scores [0.09000000171363354, 0.10000000149011612] max t 55 len buf 6703 2\n",
      "episode 717 scores [0.10000000149011612, -0.009999999776482582] max t 32 len buf 6999 1\n",
      "episode 719 scores [-0.009999999776482582, 0.10000000149011612] max t 35 len buf 7035 1\n",
      "episode 720 scores [0.10000000149011612, -0.009999999776482582] max t 32 len buf 7074 1\n",
      "episode 767 scores [0.0, 0.09000000357627869] max t 20 len buf 7110 1\n",
      "episode 768 scores [-0.009999999776482582, 0.10000000149011612] max t 32 len buf 7132 1\n",
      "episode 779 scores [0.10000000149011612, -0.009999999776482582] max t 31 len buf 7168 1\n",
      "episode 806 scores [0.10000000149011612, -0.009999999776482582] max t 32 len buf 7203 1\n",
      "episode 813 scores [-0.009999999776482582, 0.10000000149011612] max t 32 len buf 7239 1\n",
      "episode 850 scores [0.10000000149011612, 0.19000000320374966] max t 61 len buf 7275 3\n",
      "episode 870 scores [-0.009999999776482582, 0.10000000149011612] max t 32 len buf 9767 1\n",
      "episode 873 scores [0.0, 0.09000000171363354] max t 32 len buf 9803 1\n",
      "episode 882 scores [-0.009999999776482582, 0.10000000149011612] max t 23 len buf 9839 1\n",
      "episode 894 scores [0.10000000149011612, -0.009999999776482582] max t 29 len buf 9866 1\n",
      "episode 905 scores [-0.009999999776482582, 0.10000000149011612] max t 26 len buf 9899 1\n",
      "episode 911 scores [-0.009999999776482582, 0.10000000149011612] max t 28 len buf 9929 1\n",
      "episode 926 scores [0.10000000149011612, -0.009999999776482582] max t 50 len buf 9961 1\n",
      "episode 930 scores [-0.009999999776482582, 0.10000000149011612] max t 32 len buf 10015 1\n",
      "episode 938 scores [0.09000000171363354, 0.10000000149011612] max t 50 len buf 10051 2\n",
      "episode 939 scores [-0.009999999776482582, 0.10000000149011612] max t 28 len buf 10342 1\n",
      "episode 960 scores [-0.009999999776482582, 0.10000000149011612] max t 25 len buf 10374 1\n",
      "episode 967 scores [-0.009999999776482582, 0.10000000149011612] max t 42 len buf 10403 1\n",
      "episode 973 scores [0.0, 0.09000000171363354] max t 31 len buf 10449 1\n",
      "episode 978 scores [0.10000000149011612, -0.009999999776482582] max t 32 len buf 10484 1\n",
      "episode 989 scores [0.10000000149011612, 0.09000000171363354] max t 67 len buf 10520 2\n",
      "episode 1002 scores [0.10000000149011612, -0.009999999776482582] max t 33 len buf 10828 1\n",
      "episode 1035 scores [-0.009999999776482582, 0.10000000149011612] max t 41 len buf 10865 1\n",
      "episode 1039 scores [0.09000000171363354, 0.10000000149011612] max t 50 len buf 10910 2\n",
      "episode 1043 scores [0.20000000298023224, 0.19000000320374966] max t 81 len buf 11201 4\n",
      "episode 1053 scores [0.10000000149011612, -0.009999999776482582] max t 32 len buf 17283 1\n",
      "episode 1056 scores [0.10000000149011612, -0.009999999776482582] max t 31 len buf 17319 1\n",
      "episode 1062 scores [0.10000000149011612, -0.009999999776482582] max t 31 len buf 17354 1\n",
      "episode 1063 scores [-0.009999999776482582, 0.10000000149011612] max t 29 len buf 17389 1\n",
      "episode 1109 scores [0.10000000149011612, -0.009999999776482582] max t 51 len buf 17422 1\n",
      "episode 1113 scores [0.0, 0.09000000171363354] max t 32 len buf 17477 1\n",
      "episode 1117 scores [-0.009999999776482582, 0.10000000149011612] max t 50 len buf 17513 1\n",
      "episode 1123 scores [0.10000000149011612, -0.009999999776482582] max t 34 len buf 17567 1\n",
      "episode 1137 scores [-0.009999999776482582, 0.10000000149011612] max t 32 len buf 17605 1\n",
      "episode 1151 scores [0.0, 0.09000000357627869] max t 19 len buf 17641 1\n",
      "episode 1163 scores [-0.009999999776482582, 0.10000000149011612] max t 32 len buf 17662 1\n",
      "episode 1165 scores [-0.009999999776482582, 0.10000000149011612] max t 32 len buf 17698 1\n",
      "episode 1167 scores [0.0, 0.09000000171363354] max t 31 len buf 17734 1\n",
      "episode 1174 scores [0.10000000149011612, 0.09000000171363354] max t 50 len buf 17769 2\n",
      "episode 1176 scores [0.10000000149011612, -0.009999999776482582] max t 51 len buf 18060 1\n",
      "episode 1181 scores [0.10000000149011612, -0.009999999776482582] max t 32 len buf 18115 1\n",
      "episode 1199 scores [-0.009999999776482582, 0.10000000149011612] max t 35 len buf 18151 1\n",
      "episode 1208 scores [0.10000000149011612, -0.009999999776482582] max t 29 len buf 18190 1\n",
      "episode 1233 scores [0.10000000149011612, -0.009999999776482582] max t 39 len buf 18223 1\n",
      "episode 1244 scores [0.0, 0.09000000171363354] max t 31 len buf 18266 1\n",
      "episode 1246 scores [0.09000000357627869, 0.0] max t 19 len buf 18301 1\n",
      "episode 1266 scores [0.10000000149011612, -0.009999999776482582] max t 19 len buf 18322 1\n",
      "episode 1272 scores [-0.009999999776482582, 0.10000000149011612] max t 31 len buf 18343 1\n",
      "episode 1289 scores [-0.009999999776482582, 0.10000000149011612] max t 27 len buf 18378 1\n",
      "episode 1299 scores [0.10000000149011612, -0.009999999776482582] max t 28 len buf 18409 1\n",
      "episode 1301 scores [-0.009999999776482582, 0.10000000149011612] max t 31 len buf 18441 1\n",
      "episode 1308 scores [0.09000000357627869, 0.0] max t 17 len buf 18476 1\n",
      "episode 1322 scores [0.0, 0.09000000171363354] max t 29 len buf 18495 1\n",
      "episode 1338 scores [-0.009999999776482582, 0.10000000149011612] max t 22 len buf 18528 1\n",
      "episode 1340 scores [-0.009999999776482582, 0.10000000149011612] max t 32 len buf 18554 1\n",
      "episode 1341 scores [-0.009999999776482582, 0.10000000149011612] max t 26 len buf 18590 1\n",
      "episode 1364 scores [-0.009999999776482582, 0.10000000149011612] max t 31 len buf 18620 1\n",
      "episode 1375 scores [-0.009999999776482582, 0.10000000149011612] max t 31 len buf 18655 1\n",
      "episode 1383 scores [0.0, 0.09000000171363354] max t 28 len buf 18690 1\n",
      "episode 1397 scores [0.10000000149011612, -0.009999999776482582] max t 43 len buf 18722 1\n",
      "episode 1426 scores [-0.009999999776482582, 0.10000000149011612] max t 32 len buf 18769 1\n",
      "episode 1440 scores [0.0, 0.09000000171363354] max t 28 len buf 18805 1\n",
      "episode 1461 scores [0.0, 0.09000000171363354] max t 30 len buf 18837 1\n",
      "episode 1479 scores [0.09000000357627869, 0.0] max t 19 len buf 18871 1\n",
      "episode 1525 scores [0.10000000149011612, -0.009999999776482582] max t 32 len buf 18892 1\n",
      "episode 1531 scores [0.09000000171363354, 0.10000000149011612] max t 51 len buf 18928 2\n",
      "episode 1549 scores [0.10000000149011612, -0.009999999776482582] max t 32 len buf 19220 1\n",
      "episode 1576 scores [-0.009999999776482582, 0.10000000149011612] max t 23 len buf 19256 1\n",
      "episode 1581 scores [0.10000000149011612, 0.09000000171363354] max t 50 len buf 19283 2\n",
      "episode 1597 scores [0.10000000149011612, -0.009999999776482582] max t 32 len buf 19574 1\n",
      "episode 1604 scores [-0.009999999776482582, 0.10000000149011612] max t 32 len buf 19610 1\n",
      "episode 1612 scores [0.0, 0.09000000171363354] max t 31 len buf 19646 1\n",
      "episode 1623 scores [0.10000000149011612, -0.009999999776482582] max t 32 len buf 19681 1\n",
      "episode 1634 scores [0.10000000149011612, -0.009999999776482582] max t 27 len buf 19717 1\n",
      "episode 1649 scores [0.10000000149011612, -0.009999999776482582] max t 32 len buf 19748 1\n",
      "episode 1665 scores [0.10000000149011612, -0.009999999776482582] max t 36 len buf 19784 1\n",
      "episode 1671 scores [0.10000000149011612, -0.009999999776482582] max t 32 len buf 19824 1\n",
      "episode 1715 scores [0.10000000149011612, -0.009999999776482582] max t 33 len buf 19860 1\n",
      "episode 1743 scores [0.09000000171363354, 0.0] max t 32 len buf 19897 1\n",
      "episode 1764 scores [0.09000000171363354, 0.10000000149011612] max t 50 len buf 19933 2\n",
      "episode 1780 scores [0.10000000149011612, -0.009999999776482582] max t 32 len buf 20224 1\n",
      "episode 1788 scores [-0.009999999776482582, 0.10000000149011612] max t 31 len buf 20260 1\n",
      "episode 1803 scores [-0.009999999776482582, 0.10000000149011612] max t 27 len buf 20295 1\n",
      "episode 1814 scores [-0.009999999776482582, 0.10000000149011612] max t 38 len buf 20326 1\n",
      "episode 1821 scores [0.10000000149011612, -0.009999999776482582] max t 48 len buf 20368 1\n",
      "episode 1822 scores [0.10000000149011612, -0.009999999776482582] max t 48 len buf 20420 1\n",
      "episode 1824 scores [0.10000000149011612, -0.009999999776482582] max t 29 len buf 20472 1\n",
      "episode 1833 scores [0.0, 0.09000000357627869] max t 17 len buf 20505 1\n",
      "episode 1838 scores [0.0, 0.09000000171363354] max t 30 len buf 20524 1\n",
      "episode 1848 scores [-0.009999999776482582, 0.10000000149011612] max t 25 len buf 20558 1\n",
      "episode 1850 scores [0.10000000149011612, -0.009999999776482582] max t 50 len buf 20587 1\n",
      "episode 1855 scores [0.10000000149011612, -0.009999999776482582] max t 32 len buf 20641 1\n",
      "episode 1869 scores [-0.009999999776482582, 0.10000000149011612] max t 26 len buf 20677 1\n",
      "episode 1874 scores [-0.009999999776482582, 0.10000000149011612] max t 31 len buf 20707 1\n",
      "episode 1892 scores [0.0, 0.09000000171363354] max t 32 len buf 20742 1\n",
      "episode 1911 scores [0.10000000149011612, -0.009999999776482582] max t 42 len buf 20778 1\n",
      "episode 1912 scores [-0.009999999776482582, 0.10000000149011612] max t 45 len buf 20824 1\n",
      "episode 1926 scores [0.10000000149011612, -0.009999999776482582] max t 31 len buf 20873 1\n",
      "episode 1930 scores [0.10000000149011612, -0.009999999776482582] max t 32 len buf 20908 1\n",
      "episode 1952 scores [0.0, 0.09000000171363354] max t 31 len buf 20944 1\n",
      "episode 1988 scores [0.10000000149011612, -0.009999999776482582] max t 32 len buf 20979 1\n",
      "episode 1995 scores [0.10000000149011612, -0.009999999776482582] max t 29 len buf 21015 1\n",
      "episode 2002 scores [0.10000000149011612, -0.009999999776482582] max t 31 len buf 21048 1\n",
      "episode 2006 scores [0.10000000149011612, -0.009999999776482582] max t 46 len buf 21083 1\n",
      "episode 2008 scores [0.10000000149011612, -0.009999999776482582] max t 31 len buf 21133 1\n",
      "episode 2021 scores [0.10000000149011612, -0.009999999776482582] max t 32 len buf 21168 1\n",
      "episode 2038 scores [0.0, 0.09000000171363354] max t 34 len buf 21204 1\n",
      "episode 2067 scores [0.10000000149011612, -0.009999999776482582] max t 32 len buf 21242 1\n",
      "episode 2104 scores [0.10000000149011612, -0.009999999776482582] max t 32 len buf 21278 1\n",
      "episode 2106 scores [0.10000000149011612, -0.009999999776482582] max t 25 len buf 21314 1\n",
      "episode 2163 scores [0.10000000149011612, -0.009999999776482582] max t 23 len buf 21343 1\n",
      "episode 2168 scores [-0.009999999776482582, 0.10000000149011612] max t 26 len buf 21370 1\n",
      "buffer len 21400 episodes 2169 scores [-0.009999999776482582, 0.10000000149011612]\n",
      "buffer len 21400 episodes 2170 scores [0.0, -0.009999999776482582]\n",
      "buffer len 21400 episodes 2171 scores [0.0, -0.009999999776482582]\n",
      "buffer len 21400 episodes 2172 scores [0.0, -0.009999999776482582]\n",
      "buffer len 21400 episodes 2173 scores [0.0, -0.009999999776482582]\n",
      "buffer len 21400 episodes 2174 scores [0.0, -0.009999999776482582]\n",
      "buffer len 21400 episodes 2175 scores [-0.009999999776482582, 0.0]\n",
      "buffer len 21400 episodes 2176 scores [0.0, -0.009999999776482582]\n",
      "buffer len 21400 episodes 2177 scores [-0.009999999776482582, 0.0]\n",
      "buffer len 21400 episodes 2178 scores [0.0, -0.009999999776482582]\n",
      "buffer len 21400 episodes 2179 scores [0.0, -0.009999999776482582]\n",
      "buffer len 21400 episodes 2180 scores [-0.009999999776482582, 0.0]\n",
      "buffer len 21400 episodes 2181 scores [0.0, -0.009999999776482582]\n",
      "buffer len 21400 episodes 2182 scores [0.0, -0.009999999776482582]\n",
      "buffer len 21400 episodes 2183 scores [0.0, -0.009999999776482582]\n",
      "buffer len 21400 episodes 2184 scores [-0.009999999776482582, 0.0]\n",
      "buffer len 21400 episodes 2185 scores [-0.009999999776482582, 0.0]\n",
      "episode 2185 scores [0.10000000149011612, 0.09000000171363354] max t 51 len buf 21400 2\n",
      "episode 2186 scores [0.0, 0.09000000171363354] max t 29 len buf 21692 1\n",
      "episode 2214 scores [0.10000000149011612, -0.009999999776482582] max t 32 len buf 21725 1\n",
      "episode 2217 scores [0.10000000149011612, -0.009999999776482582] max t 21 len buf 21761 1\n",
      "episode 2236 scores [0.10000000149011612, -0.009999999776482582] max t 31 len buf 21786 1\n",
      "episode 2302 scores [0.10000000149011612, -0.009999999776482582] max t 34 len buf 21821 1\n",
      "episode 2330 scores [0.0, 0.09000000171363354] max t 30 len buf 21859 1\n",
      "episode 2385 scores [-0.009999999776482582, 0.10000000149011612] max t 32 len buf 21893 1\n",
      "episode 2389 scores [0.10000000149011612, -0.009999999776482582] max t 32 len buf 21929 1\n",
      "episode 2393 scores [0.0, 0.09000000171363354] max t 31 len buf 21965 1\n",
      "buffer len 22000 episodes 2394 scores [0.0, 0.09000000171363354]\n",
      "buffer len 22000 episodes 2395 scores [-0.009999999776482582, 0.0]\n",
      "buffer len 22000 episodes 2396 scores [-0.009999999776482582, 0.0]\n",
      "buffer len 22000 episodes 2397 scores [-0.009999999776482582, 0.0]\n",
      "buffer len 22000 episodes 2398 scores [-0.009999999776482582, 0.0]\n",
      "buffer len 22000 episodes 2399 scores [0.0, -0.009999999776482582]\n",
      "buffer len 22000 episodes 2400 scores [-0.009999999776482582, 0.0]\n",
      "buffer len 22000 episodes 2401 scores [0.0, -0.009999999776482582]\n",
      "buffer len 22000 episodes 2402 scores [0.0, -0.009999999776482582]\n",
      "buffer len 22000 episodes 2403 scores [-0.009999999776482582, 0.0]\n",
      "buffer len 22000 episodes 2404 scores [0.0, -0.009999999776482582]\n",
      "buffer len 22000 episodes 2405 scores [0.0, -0.009999999776482582]\n",
      "episode 2405 scores [-0.009999999776482582, 0.10000000149011612] max t 32 len buf 22000 1\n",
      "episode 2424 scores [0.10000000149011612, -0.009999999776482582] max t 33 len buf 22036 1\n",
      "episode 2425 scores [0.10000000149011612, -0.009999999776482582] max t 31 len buf 22073 1\n",
      "episode 2429 scores [-0.009999999776482582, 0.10000000149011612] max t 33 len buf 22108 1\n",
      "episode 2434 scores [-0.009999999776482582, 0.10000000149011612] max t 31 len buf 22145 1\n",
      "episode 2461 scores [0.10000000149011612, -0.009999999776482582] max t 32 len buf 22180 1\n",
      "episode 2467 scores [0.09000000171363354, 0.10000000149011612] max t 55 len buf 22216 2\n",
      "episode 2486 scores [-0.009999999776482582, 0.10000000149011612] max t 51 len buf 22512 1\n",
      "episode 2488 scores [-0.009999999776482582, 0.10000000149011612] max t 32 len buf 22567 1\n",
      "episode 2502 scores [-0.009999999776482582, 0.10000000149011612] max t 45 len buf 22603 1\n",
      "episode 2503 scores [0.10000000149011612, -0.009999999776482582] max t 32 len buf 22652 1\n",
      "episode 2551 scores [0.09000000171363354, 0.20000000298023224] max t 63 len buf 22688 3\n",
      "episode 2570 scores [-0.009999999776482582, 0.10000000149011612] max t 32 len buf 25182 1\n",
      "episode 2594 scores [-0.009999999776482582, 0.10000000149011612] max t 32 len buf 25218 1\n",
      "episode 2622 scores [-0.009999999776482582, 0.10000000149011612] max t 47 len buf 25254 1\n",
      "episode 2644 scores [0.0, 0.09000000171363354] max t 29 len buf 25305 1\n",
      "episode 2654 scores [0.10000000149011612, -0.009999999776482582] max t 26 len buf 25338 1\n",
      "episode 2661 scores [0.10000000149011612, -0.009999999776482582] max t 28 len buf 25368 1\n",
      "buffer len 25400 episodes 2662 scores [0.10000000149011612, -0.009999999776482582]\n",
      "buffer len 25400 episodes 2663 scores [-0.009999999776482582, 0.0]\n",
      "buffer len 25400 episodes 2664 scores [0.0, -0.009999999776482582]\n",
      "buffer len 25400 episodes 2665 scores [-0.009999999776482582, 0.0]\n",
      "buffer len 25400 episodes 2666 scores [-0.009999999776482582, 0.0]\n",
      "buffer len 25400 episodes 2667 scores [0.0, -0.009999999776482582]\n",
      "buffer len 25400 episodes 2668 scores [-0.009999999776482582, 0.0]\n",
      "buffer len 25400 episodes 2669 scores [0.0, -0.009999999776482582]\n",
      "buffer len 25400 episodes 2670 scores [0.0, -0.009999999776482582]\n",
      "buffer len 25400 episodes 2671 scores [-0.009999999776482582, 0.0]\n",
      "buffer len 25400 episodes 2672 scores [-0.009999999776482582, 0.0]\n",
      "buffer len 25400 episodes 2673 scores [-0.009999999776482582, 0.0]\n",
      "buffer len 25400 episodes 2674 scores [0.0, -0.009999999776482582]\n",
      "buffer len 25400 episodes 2675 scores [-0.009999999776482582, 0.0]\n",
      "buffer len 25400 episodes 2676 scores [-0.009999999776482582, 0.0]\n",
      "buffer len 25400 episodes 2677 scores [-0.009999999776482582, 0.0]\n",
      "buffer len 25400 episodes 2678 scores [0.0, -0.009999999776482582]\n",
      "buffer len 25400 episodes 2679 scores [0.0, -0.009999999776482582]\n",
      "buffer len 25400 episodes 2680 scores [0.0, -0.009999999776482582]\n",
      "buffer len 25400 episodes 2681 scores [-0.009999999776482582, 0.0]\n",
      "buffer len 25400 episodes 2682 scores [0.0, -0.009999999776482582]\n",
      "buffer len 25400 episodes 2683 scores [0.0, -0.009999999776482582]\n",
      "buffer len 25400 episodes 2684 scores [0.0, -0.009999999776482582]\n",
      "buffer len 25400 episodes 2685 scores [-0.009999999776482582, 0.0]\n",
      "buffer len 25400 episodes 2686 scores [-0.009999999776482582, 0.0]\n",
      "buffer len 25400 episodes 2687 scores [-0.009999999776482582, 0.0]\n",
      "buffer len 25400 episodes 2688 scores [0.0, -0.009999999776482582]\n",
      "buffer len 25400 episodes 2689 scores [0.0, -0.009999999776482582]\n",
      "buffer len 25400 episodes 2690 scores [0.0, -0.009999999776482582]\n",
      "buffer len 25400 episodes 2691 scores [-0.009999999776482582, 0.0]\n",
      "buffer len 25400 episodes 2692 scores [0.0, -0.009999999776482582]\n",
      "episode 2692 scores [0.0, 0.09000000171363354] max t 33 len buf 25400 1\n",
      "episode 2696 scores [0.10000000149011612, -0.009999999776482582] max t 32 len buf 25437 1\n",
      "episode 2698 scores [-0.009999999776482582, 0.10000000149011612] max t 32 len buf 25473 1\n",
      "episode 2700 scores [-0.009999999776482582, 0.10000000149011612] max t 27 len buf 25509 1\n",
      "episode 2706 scores [-0.009999999776482582, 0.10000000149011612] max t 45 len buf 25540 1\n",
      "episode 2757 scores [0.09000000171363354, 0.10000000149011612] max t 41 len buf 25589 2\n",
      "episode 2758 scores [0.0, 0.09000000357627869] max t 25 len buf 25871 1\n",
      "episode 2759 scores [0.10000000149011612, -0.009999999776482582] max t 32 len buf 25898 1\n",
      "episode 2768 scores [0.0, 0.09000000171363354] max t 31 len buf 25934 1\n",
      "episode 2769 scores [0.10000000149011612, -0.009999999776482582] max t 32 len buf 25969 1\n",
      "episode 2779 scores [-0.009999999776482582, 0.10000000149011612] max t 32 len buf 26005 1\n",
      "episode 2781 scores [0.10000000149011612, -0.009999999776482582] max t 32 len buf 26041 1\n",
      "episode 2817 scores [0.10000000149011612, -0.009999999776482582] max t 28 len buf 26077 1\n",
      "episode 2829 scores [0.10000000149011612, -0.009999999776482582] max t 48 len buf 26109 1\n",
      "episode 2833 scores [-0.009999999776482582, 0.10000000149011612] max t 31 len buf 26161 1\n",
      "episode 2834 scores [-0.009999999776482582, 0.10000000149011612] max t 23 len buf 26196 1\n",
      "episode 2839 scores [0.0, 0.09000000171363354] max t 32 len buf 26223 1\n",
      "episode 2843 scores [0.09000000357627869, 0.10000000149011612] max t 48 len buf 26259 2\n",
      "episode 2852 scores [0.10000000149011612, -0.009999999776482582] max t 18 len buf 26468 1\n",
      "episode 2853 scores [0.10000000149011612, 0.09000000171363354] max t 48 len buf 26488 2\n",
      "episode 2854 scores [-0.009999999776482582, 0.10000000149011612] max t 32 len buf 26777 1\n",
      "episode 2865 scores [0.10000000149011612, -0.009999999776482582] max t 32 len buf 26813 1\n",
      "episode 2879 scores [0.10000000149011612, -0.009999999776482582] max t 32 len buf 26849 1\n",
      "episode 2883 scores [0.0, 0.09000000357627869] max t 19 len buf 26885 1\n",
      "episode 2897 scores [-0.009999999776482582, 0.10000000149011612] max t 32 len buf 26906 1\n",
      "episode 2899 scores [-0.009999999776482582, 0.10000000149011612] max t 32 len buf 26942 1\n",
      "episode 2904 scores [0.10000000149011612, 0.09000000171363354] max t 51 len buf 26978 2\n",
      "episode 2937 scores [0.10000000149011612, -0.009999999776482582] max t 33 len buf 27270 1\n",
      "episode 2956 scores [-0.009999999776482582, 0.10000000149011612] max t 47 len buf 27307 1\n",
      "episode 2957 scores [0.09000000357627869, 0.0] max t 19 len buf 27358 1\n",
      "episode 2995 scores [-0.009999999776482582, 0.10000000149011612] max t 48 len buf 27379 1\n",
      "episode 2998 scores [-0.009999999776482582, 0.10000000149011612] max t 32 len buf 27431 1\n",
      "episode 3006 scores [0.0, 0.09000000171363354] max t 28 len buf 27467 1\n",
      "episode 3014 scores [0.10000000149011612, -0.009999999776482582] max t 30 len buf 27499 1\n",
      "episode 3022 scores [0.10000000149011612, 0.09000000171363354] max t 65 len buf 27533 2\n",
      "episode 3023 scores [-0.009999999776482582, 0.10000000149011612] max t 29 len buf 27839 1\n",
      "episode 3026 scores [0.10000000149011612, -0.009999999776482582] max t 32 len buf 27872 1\n",
      "episode 3040 scores [0.10000000149011612, -0.009999999776482582] max t 30 len buf 27908 1\n",
      "episode 3054 scores [-0.009999999776482582, 0.10000000149011612] max t 32 len buf 27942 1\n",
      "episode 3059 scores [0.10000000149011612, -0.009999999776482582] max t 28 len buf 27978 1\n",
      "episode 3063 scores [0.09000000357627869, 0.10000000149011612] max t 48 len buf 28010 2\n",
      "episode 3074 scores [0.10000000149011612, -0.009999999776482582] max t 34 len buf 28219 1\n",
      "episode 3088 scores [0.10000000149011612, -0.009999999776482582] max t 25 len buf 28257 1\n",
      "episode 3096 scores [-0.009999999776482582, 0.10000000149011612] max t 24 len buf 28286 1\n",
      "episode 3111 scores [0.10000000149011612, -0.009999999776482582] max t 28 len buf 28314 1\n",
      "episode 3115 scores [0.10000000149011612, -0.009999999776482582] max t 33 len buf 28346 1\n",
      "episode 3119 scores [-0.009999999776482582, 0.10000000149011612] max t 29 len buf 28383 1\n",
      "episode 3129 scores [0.10000000149011612, -0.009999999776482582] max t 31 len buf 28416 1\n",
      "episode 3141 scores [0.0, 0.09000000171363354] max t 31 len buf 28451 1\n",
      "episode 3160 scores [-0.009999999776482582, 0.10000000149011612] max t 45 len buf 28486 1\n",
      "episode 3182 scores [0.10000000149011612, -0.009999999776482582] max t 27 len buf 28535 1\n",
      "episode 3231 scores [-0.009999999776482582, 0.10000000149011612] max t 32 len buf 28566 1\n",
      "episode 3268 scores [-0.009999999776482582, 0.10000000149011612] max t 32 len buf 28602 1\n",
      "episode 3281 scores [0.0, 0.09000000171363354] max t 30 len buf 28638 1\n",
      "episode 3300 scores [-0.009999999776482582, 0.10000000149011612] max t 24 len buf 28672 1\n",
      "buffer len 28700 episodes 3301 scores [-0.009999999776482582, 0.10000000149011612]\n",
      "buffer len 28700 episodes 3302 scores [-0.009999999776482582, 0.0]\n",
      "buffer len 28700 episodes 3303 scores [0.0, -0.009999999776482582]\n",
      "buffer len 28700 episodes 3304 scores [0.0, -0.009999999776482582]\n",
      "buffer len 28700 episodes 3305 scores [-0.009999999776482582, 0.0]\n",
      "buffer len 28700 episodes 3306 scores [0.0, -0.009999999776482582]\n",
      "buffer len 28700 episodes 3307 scores [-0.009999999776482582, 0.0]\n",
      "buffer len 28700 episodes 3308 scores [-0.009999999776482582, 0.0]\n",
      "buffer len 28700 episodes 3309 scores [0.0, -0.009999999776482582]\n",
      "buffer len 28700 episodes 3310 scores [-0.009999999776482582, 0.0]\n",
      "buffer len 28700 episodes 3311 scores [-0.009999999776482582, 0.0]\n",
      "buffer len 28700 episodes 3312 scores [-0.009999999776482582, 0.0]\n",
      "buffer len 28700 episodes 3313 scores [0.0, -0.009999999776482582]\n",
      "buffer len 28700 episodes 3314 scores [0.0, -0.009999999776482582]\n",
      "buffer len 28700 episodes 3315 scores [0.0, -0.009999999776482582]\n",
      "buffer len 28700 episodes 3316 scores [-0.009999999776482582, 0.0]\n",
      "buffer len 28700 episodes 3317 scores [-0.009999999776482582, 0.0]\n",
      "buffer len 28700 episodes 3318 scores [-0.009999999776482582, 0.0]\n",
      "buffer len 28700 episodes 3319 scores [-0.009999999776482582, 0.0]\n",
      "buffer len 28700 episodes 3320 scores [0.0, -0.009999999776482582]\n",
      "buffer len 28700 episodes 3321 scores [-0.009999999776482582, 0.0]\n",
      "buffer len 28700 episodes 3322 scores [0.0, -0.009999999776482582]\n",
      "buffer len 28700 episodes 3323 scores [-0.009999999776482582, 0.0]\n",
      "buffer len 28700 episodes 3324 scores [0.0, -0.009999999776482582]\n",
      "buffer len 28700 episodes 3325 scores [-0.009999999776482582, 0.0]\n",
      "buffer len 28700 episodes 3326 scores [0.0, -0.009999999776482582]\n",
      "buffer len 28700 episodes 3327 scores [-0.009999999776482582, 0.0]\n",
      "buffer len 28700 episodes 3328 scores [0.0, -0.009999999776482582]\n",
      "buffer len 28700 episodes 3329 scores [-0.009999999776482582, 0.0]\n",
      "buffer len 28700 episodes 3330 scores [0.0, -0.009999999776482582]\n",
      "buffer len 28700 episodes 3331 scores [-0.009999999776482582, 0.0]\n",
      "episode 3331 scores [0.0, 0.09000000171363354] max t 30 len buf 28700 1\n",
      "episode 3345 scores [0.10000000149011612, -0.009999999776482582] max t 53 len buf 28734 1\n",
      "episode 3353 scores [0.10000000149011612, -0.009999999776482582] max t 30 len buf 28791 1\n",
      "episode 3362 scores [-0.009999999776482582, 0.10000000149011612] max t 32 len buf 28825 1\n",
      "episode 3367 scores [0.10000000149011612, -0.009999999776482582] max t 25 len buf 28861 1\n",
      "episode 3381 scores [0.10000000149011612, 0.09000000171363354] max t 63 len buf 28890 2\n",
      "episode 3400 scores [0.10000000149011612, -0.009999999776482582] max t 22 len buf 29194 1\n",
      "episode 3414 scores [0.10000000149011612, -0.009999999776482582] max t 31 len buf 29220 1\n",
      "episode 3421 scores [-0.009999999776482582, 0.10000000149011612] max t 32 len buf 29255 1\n",
      "episode 3461 scores [0.10000000149011612, -0.009999999776482582] max t 43 len buf 29291 1\n",
      "episode 3464 scores [0.10000000149011612, -0.009999999776482582] max t 33 len buf 29338 1\n",
      "episode 3466 scores [-0.009999999776482582, 0.10000000149011612] max t 33 len buf 29375 1\n",
      "episode 3469 scores [0.10000000149011612, -0.009999999776482582] max t 51 len buf 29412 1\n",
      "episode 3481 scores [-0.009999999776482582, 0.10000000149011612] max t 28 len buf 29467 1\n",
      "episode 3487 scores [0.10000000149011612, -0.009999999776482582] max t 31 len buf 29499 1\n",
      "episode 3492 scores [0.09000000171363354, 0.0] max t 32 len buf 29534 1\n",
      "episode 3511 scores [0.10000000149011612, -0.009999999776482582] max t 50 len buf 29570 1\n",
      "episode 3523 scores [0.10000000149011612, -0.009999999776482582] max t 24 len buf 29624 1\n",
      "episode 3538 scores [0.10000000149011612, -0.009999999776482582] max t 22 len buf 29652 1\n",
      "episode 3551 scores [-0.009999999776482582, 0.10000000149011612] max t 28 len buf 29678 1\n",
      "episode 3572 scores [0.10000000149011612, 0.09000000171363354] max t 66 len buf 29710 2\n",
      "episode 3575 scores [-0.009999999776482582, 0.10000000149011612] max t 31 len buf 30017 1\n",
      "episode 3587 scores [0.10000000149011612, -0.009999999776482582] max t 28 len buf 30052 1\n",
      "episode 3589 scores [-0.009999999776482582, 0.10000000149011612] max t 32 len buf 30084 1\n",
      "episode 3599 scores [0.10000000149011612, -0.009999999776482582] max t 28 len buf 30120 1\n",
      "episode 3606 scores [0.10000000149011612, -0.009999999776482582] max t 27 len buf 30152 1\n",
      "episode 3628 scores [0.10000000149011612, -0.009999999776482582] max t 32 len buf 30183 1\n",
      "episode 3643 scores [0.10000000149011612, -0.009999999776482582] max t 30 len buf 30219 1\n",
      "episode 3648 scores [0.0, 0.09000000357627869] max t 30 len buf 30253 1\n",
      "episode 3658 scores [0.10000000149011612, -0.009999999776482582] max t 32 len buf 30285 1\n",
      "episode 3664 scores [-0.009999999776482582, 0.10000000149011612] max t 45 len buf 30321 1\n",
      "episode 3670 scores [0.10000000149011612, -0.009999999776482582] max t 31 len buf 30370 1\n",
      "episode 3674 scores [0.10000000149011612, -0.009999999776482582] max t 32 len buf 30405 1\n",
      "episode 3700 scores [-0.009999999776482582, 0.10000000149011612] max t 32 len buf 30441 1\n",
      "episode 3706 scores [0.10000000149011612, -0.009999999776482582] max t 31 len buf 30477 1\n",
      "episode 3717 scores [0.0, 0.09000000171363354] max t 43 len buf 30512 1\n",
      "episode 3736 scores [0.10000000149011612, -0.009999999776482582] max t 29 len buf 30559 1\n",
      "episode 3755 scores [0.10000000149011612, -0.009999999776482582] max t 42 len buf 30592 1\n",
      "episode 3778 scores [0.0, 0.09000000171363354] max t 30 len buf 30638 1\n",
      "episode 3790 scores [0.10000000149011612, -0.009999999776482582] max t 30 len buf 30672 1\n",
      "episode 3802 scores [0.10000000149011612, -0.009999999776482582] max t 47 len buf 30706 1\n",
      "episode 3837 scores [0.09000000357627869, 0.0] max t 18 len buf 30757 1\n",
      "episode 3852 scores [0.10000000149011612, -0.009999999776482582] max t 36 len buf 30777 1\n",
      "episode 3882 scores [-0.019999999552965164, 0.10000000149011612] max t 32 len buf 30817 1\n",
      "episode 3894 scores [-0.009999999776482582, 0.10000000149011612] max t 24 len buf 30853 1\n",
      "episode 3910 scores [-0.009999999776482582, 0.10000000149011612] max t 32 len buf 30881 1\n",
      "episode 3916 scores [-0.009999999776482582, 0.10000000149011612] max t 45 len buf 30917 1\n",
      "episode 3933 scores [0.0, 0.09000000171363354] max t 30 len buf 30966 1\n",
      "buffer len 31000 episodes 3934 scores [0.0, 0.09000000171363354]\n",
      "buffer len 31000 episodes 3935 scores [0.0, -0.009999999776482582]\n",
      "buffer len 31000 episodes 3936 scores [0.0, -0.009999999776482582]\n",
      "buffer len 31000 episodes 3937 scores [-0.009999999776482582, 0.0]\n",
      "buffer len 31000 episodes 3938 scores [0.0, -0.009999999776482582]\n",
      "buffer len 31000 episodes 3939 scores [0.0, -0.009999999776482582]\n",
      "buffer len 31000 episodes 3940 scores [-0.009999999776482582, 0.0]\n",
      "buffer len 31000 episodes 3941 scores [0.0, -0.009999999776482582]\n",
      "buffer len 31000 episodes 3942 scores [0.0, -0.009999999776482582]\n",
      "buffer len 31000 episodes 3943 scores [0.0, -0.009999999776482582]\n",
      "buffer len 31000 episodes 3944 scores [-0.009999999776482582, 0.0]\n",
      "buffer len 31000 episodes 3945 scores [-0.009999999776482582, 0.0]\n",
      "buffer len 31000 episodes 3946 scores [-0.009999999776482582, 0.0]\n",
      "buffer len 31000 episodes 3947 scores [-0.009999999776482582, 0.0]\n",
      "buffer len 31000 episodes 3948 scores [-0.009999999776482582, 0.0]\n",
      "buffer len 31000 episodes 3949 scores [-0.009999999776482582, 0.0]\n",
      "buffer len 31000 episodes 3950 scores [0.0, -0.009999999776482582]\n",
      "buffer len 31000 episodes 3951 scores [0.0, -0.009999999776482582]\n",
      "buffer len 31000 episodes 3952 scores [-0.009999999776482582, 0.0]\n",
      "buffer len 31000 episodes 3953 scores [0.0, -0.009999999776482582]\n",
      "episode 3953 scores [0.10000000149011612, -0.009999999776482582] max t 59 len buf 31000 1\n",
      "episode 3967 scores [0.10000000149011612, -0.009999999776482582] max t 31 len buf 31063 1\n",
      "episode 3973 scores [0.10000000149011612, -0.009999999776482582] max t 31 len buf 31098 1\n",
      "episode 3976 scores [0.10000000149011612, 0.09000000171363354] max t 41 len buf 31133 2\n",
      "episode 3986 scores [0.10000000149011612, -0.009999999776482582] max t 35 len buf 31415 1\n",
      "episode 4001 scores [0.10000000149011612, -0.009999999776482582] max t 30 len buf 31454 1\n",
      "episode 4002 scores [0.10000000149011612, 0.09000000357627869] max t 47 len buf 31488 2\n",
      "episode 4021 scores [0.10000000149011612, -0.009999999776482582] max t 25 len buf 31696 1\n",
      "episode 4051 scores [0.10000000149011612, -0.009999999776482582] max t 28 len buf 31725 1\n",
      "episode 4067 scores [-0.009999999776482582, 0.10000000149011612] max t 28 len buf 31757 1\n",
      "episode 4080 scores [0.0, 0.09000000357627869] max t 20 len buf 31789 1\n",
      "episode 4096 scores [0.09000000171363354, 0.20000000298023224] max t 62 len buf 31811 3\n",
      "episode 4135 scores [0.0, 0.09000000171363354] max t 31 len buf 34304 1\n",
      "episode 4140 scores [-0.009999999776482582, 0.10000000149011612] max t 25 len buf 34339 1\n",
      "episode 4148 scores [0.0, 0.09000000171363354] max t 29 len buf 34368 1\n",
      "episode 4152 scores [-0.009999999776482582, 0.10000000149011612] max t 32 len buf 34401 1\n",
      "episode 4180 scores [-0.009999999776482582, 0.10000000149011612] max t 47 len buf 34437 1\n",
      "episode 4191 scores [0.09000000357627869, 0.10000000149011612] max t 48 len buf 34488 2\n",
      "episode 4198 scores [0.10000000149011612, -0.009999999776482582] max t 27 len buf 34697 1\n",
      "episode 4200 scores [0.10000000149011612, -0.009999999776482582] max t 20 len buf 34728 1\n",
      "episode 4209 scores [0.10000000149011612, -0.009999999776482582] max t 50 len buf 34750 1\n",
      "episode 4231 scores [-0.009999999776482582, 0.10000000149011612] max t 32 len buf 34804 1\n",
      "episode 4233 scores [0.20000000298023224, 0.09000000171363354] max t 72 len buf 34840 3\n",
      "episode 4246 scores [-0.009999999776482582, 0.10000000149011612] max t 30 len buf 37343 1\n",
      "episode 4268 scores [0.10000000149011612, -0.009999999776482582] max t 30 len buf 37377 1\n",
      "episode 4273 scores [0.10000000149011612, -0.009999999776482582] max t 33 len buf 37411 1\n",
      "episode 4310 scores [-0.009999999776482582, 0.10000000149011612] max t 53 len buf 37448 1\n",
      "episode 4328 scores [-0.009999999776482582, 0.10000000149011612] max t 32 len buf 37505 1\n",
      "episode 4331 scores [0.0, 0.09000000357627869] max t 18 len buf 37541 1\n",
      "episode 4339 scores [0.20000000298023224, -0.009999999776482582] max t 51 len buf 37561 2\n",
      "episode 4353 scores [0.10000000149011612, -0.009999999776482582] max t 29 len buf 37773 1\n",
      "episode 4369 scores [0.10000000149011612, -0.009999999776482582] max t 31 len buf 37806 1\n",
      "episode 4372 scores [0.10000000149011612, -0.009999999776482582] max t 32 len buf 37841 1\n",
      "episode 4376 scores [0.10000000149011612, -0.009999999776482582] max t 30 len buf 37877 1\n",
      "episode 4389 scores [-0.009999999776482582, 0.10000000149011612] max t 30 len buf 37911 1\n",
      "episode 4420 scores [0.10000000149011612, 0.09000000171363354] max t 44 len buf 37945 2\n",
      "episode 4427 scores [0.10000000149011612, 0.09000000171363354] max t 56 len buf 38230 2\n",
      "episode 4439 scores [0.10000000149011612, -0.009999999776482582] max t 17 len buf 38527 1\n",
      "episode 4443 scores [-0.009999999776482582, 0.10000000149011612] max t 32 len buf 38546 1\n",
      "episode 4445 scores [-0.009999999776482582, 0.10000000149011612] max t 32 len buf 38582 1\n",
      "episode 4452 scores [0.10000000149011612, -0.009999999776482582] max t 26 len buf 38618 1\n",
      "episode 4468 scores [0.10000000149011612, -0.009999999776482582] max t 28 len buf 38648 1\n",
      "episode 4476 scores [0.09000000357627869, 0.0] max t 18 len buf 38680 1\n",
      "buffer len 38700 episodes 4477 scores [0.09000000357627869, 0.0]\n",
      "buffer len 38700 episodes 4478 scores [-0.009999999776482582, 0.0]\n",
      "buffer len 38700 episodes 4479 scores [-0.009999999776482582, 0.0]\n",
      "buffer len 38700 episodes 4480 scores [-0.009999999776482582, 0.0]\n",
      "buffer len 38700 episodes 4481 scores [-0.009999999776482582, 0.0]\n",
      "buffer len 38700 episodes 4482 scores [-0.009999999776482582, 0.0]\n",
      "buffer len 38700 episodes 4483 scores [-0.009999999776482582, 0.0]\n",
      "buffer len 38700 episodes 4484 scores [0.0, -0.009999999776482582]\n",
      "buffer len 38700 episodes 4485 scores [-0.009999999776482582, 0.0]\n",
      "buffer len 38700 episodes 4486 scores [0.0, -0.009999999776482582]\n",
      "buffer len 38700 episodes 4487 scores [0.0, -0.009999999776482582]\n",
      "buffer len 38700 episodes 4488 scores [-0.009999999776482582, 0.0]\n",
      "buffer len 38700 episodes 4489 scores [0.0, -0.009999999776482582]\n",
      "buffer len 38700 episodes 4490 scores [0.0, -0.009999999776482582]\n",
      "buffer len 38700 episodes 4491 scores [-0.009999999776482582, 0.0]\n",
      "episode 4491 scores [0.10000000149011612, 0.09000000171363354] max t 47 len buf 38700 2\n",
      "episode 4501 scores [0.10000000149011612, 0.09000000171363354] max t 51 len buf 38988 2\n",
      "episode 4514 scores [0.0, 0.09000000171363354] max t 31 len buf 39280 1\n",
      "episode 4534 scores [-0.009999999776482582, 0.10000000149011612] max t 26 len buf 39315 1\n",
      "episode 4547 scores [0.10000000149011612, -0.019999999552965164] max t 31 len buf 39345 1\n",
      "episode 4571 scores [0.10000000149011612, -0.009999999776482582] max t 38 len buf 39380 1\n",
      "episode 4667 scores [-0.009999999776482582, 0.10000000149011612] max t 33 len buf 39422 1\n",
      "episode 4700 scores [-0.009999999776482582, 0.10000000149011612] max t 31 len buf 39459 1\n",
      "episode 4720 scores [0.09000000171363354, 0.0] max t 42 len buf 39494 1\n",
      "episode 4738 scores [0.10000000149011612, -0.009999999776482582] max t 28 len buf 39540 1\n",
      "episode 4744 scores [0.10000000149011612, 0.09000000171363354] max t 53 len buf 39572 2\n",
      "episode 4791 scores [0.10000000149011612, -0.009999999776482582] max t 47 len buf 39866 1\n",
      "episode 4806 scores [0.10000000149011612, -0.009999999776482582] max t 33 len buf 39917 1\n",
      "episode 4862 scores [0.09000000357627869, 0.0] max t 20 len buf 39954 1\n",
      "episode 4877 scores [-0.009999999776482582, 0.10000000149011612] max t 32 len buf 39976 1\n",
      "episode 4883 scores [0.10000000149011612, -0.009999999776482582] max t 19 len buf 40012 1\n",
      "episode 4890 scores [0.0, 0.09000000171363354] max t 29 len buf 40033 1\n",
      "episode 4903 scores [0.10000000149011612, 0.09000000171363354] max t 47 len buf 40066 2\n",
      "episode 4914 scores [0.10000000149011612, -0.009999999776482582] max t 30 len buf 40354 1\n",
      "episode 4947 scores [0.10000000149011612, -0.009999999776482582] max t 32 len buf 40388 1\n",
      "episode 4950 scores [-0.009999999776482582, 0.10000000149011612] max t 57 len buf 40424 1\n",
      "episode 4960 scores [0.10000000149011612, -0.009999999776482582] max t 50 len buf 40485 1\n",
      "episode 4968 scores [0.09000000171363354, 0.0] max t 32 len buf 40539 1\n",
      "episode 4980 scores [-0.009999999776482582, 0.10000000149011612] max t 48 len buf 40575 1\n",
      "episode 5004 scores [-0.009999999776482582, 0.10000000149011612] max t 38 len buf 40627 1\n",
      "episode 5018 scores [0.10000000149011612, -0.009999999776482582] max t 32 len buf 40669 1\n",
      "episode 5023 scores [0.10000000149011612, -0.009999999776482582] max t 32 len buf 40705 1\n",
      "episode 5030 scores [0.0, 0.09000000171363354] max t 33 len buf 40741 1\n",
      "episode 5032 scores [0.10000000149011612, -0.009999999776482582] max t 23 len buf 40778 1\n",
      "episode 5034 scores [-0.009999999776482582, 0.10000000149011612] max t 24 len buf 40805 1\n",
      "episode 5064 scores [0.10000000149011612, -0.009999999776482582] max t 32 len buf 40833 1\n",
      "episode 5081 scores [0.10000000149011612, -0.009999999776482582] max t 45 len buf 40869 1\n",
      "episode 5105 scores [0.10000000149011612, -0.009999999776482582] max t 40 len buf 40918 1\n",
      "episode 5113 scores [0.0, 0.09000000171363354] max t 30 len buf 40962 1\n",
      "episode 5139 scores [0.10000000149011612, -0.009999999776482582] max t 29 len buf 40996 1\n",
      "episode 5144 scores [-0.009999999776482582, 0.10000000149011612] max t 29 len buf 41029 1\n",
      "episode 5147 scores [0.09000000357627869, 0.0] max t 25 len buf 41062 1\n",
      "episode 5158 scores [-0.009999999776482582, 0.10000000149011612] max t 24 len buf 41089 1\n",
      "episode 5169 scores [-0.009999999776482582, 0.10000000149011612] max t 26 len buf 41117 1\n",
      "episode 5186 scores [0.10000000149011612, -0.009999999776482582] max t 32 len buf 41147 1\n",
      "episode 5208 scores [0.10000000149011612, -0.009999999776482582] max t 28 len buf 41183 1\n",
      "episode 5229 scores [0.10000000149011612, -0.009999999776482582] max t 45 len buf 41215 1\n",
      "episode 5240 scores [0.0, 0.09000000171363354] max t 31 len buf 41264 1\n",
      "episode 5265 scores [-0.009999999776482582, 0.10000000149011612] max t 32 len buf 41299 1\n",
      "episode 5288 scores [0.10000000149011612, -0.009999999776482582] max t 32 len buf 41335 1\n",
      "episode 5296 scores [0.10000000149011612, -0.009999999776482582] max t 31 len buf 41371 1\n",
      "episode 5297 scores [-0.009999999776482582, 0.10000000149011612] max t 25 len buf 41406 1\n",
      "episode 5305 scores [-0.009999999776482582, 0.10000000149011612] max t 46 len buf 41435 1\n",
      "episode 5314 scores [0.10000000149011612, -0.009999999776482582] max t 31 len buf 41485 1\n",
      "episode 5322 scores [0.10000000149011612, -0.009999999776482582] max t 29 len buf 41520 1\n",
      "episode 5334 scores [0.0, 0.09000000171363354] max t 31 len buf 41553 1\n",
      "episode 5355 scores [-0.009999999776482582, 0.10000000149011612] max t 25 len buf 41588 1\n",
      "episode 5357 scores [0.0, 0.09000000171363354] max t 31 len buf 41617 1\n",
      "episode 5375 scores [-0.009999999776482582, 0.10000000149011612] max t 27 len buf 41652 1\n",
      "episode 5437 scores [-0.009999999776482582, 0.10000000149011612] max t 26 len buf 41683 1\n",
      "episode 5442 scores [0.10000000149011612, -0.009999999776482582] max t 46 len buf 41713 1\n",
      "episode 5490 scores [0.10000000149011612, -0.009999999776482582] max t 48 len buf 41763 1\n",
      "episode 5516 scores [-0.009999999776482582, 0.10000000149011612] max t 32 len buf 41815 1\n",
      "episode 5524 scores [0.10000000149011612, -0.009999999776482582] max t 29 len buf 41851 1\n",
      "episode 5537 scores [-0.009999999776482582, 0.10000000149011612] max t 43 len buf 41884 1\n",
      "episode 5539 scores [0.10000000149011612, -0.009999999776482582] max t 31 len buf 41931 1\n",
      "episode 5545 scores [0.09000000171363354, 0.10000000149011612] max t 59 len buf 41966 2\n",
      "episode 5553 scores [0.10000000149011612, -0.009999999776482582] max t 32 len buf 42266 1\n",
      "episode 5568 scores [0.09000000171363354, 0.10000000149011612] max t 46 len buf 42302 2\n",
      "episode 5583 scores [0.10000000149011612, -0.009999999776482582] max t 31 len buf 42589 1\n",
      "episode 5586 scores [0.10000000149011612, -0.009999999776482582] max t 32 len buf 42624 1\n",
      "episode 5589 scores [-0.009999999776482582, 0.10000000149011612] max t 26 len buf 42660 1\n",
      "episode 5595 scores [0.09000000357627869, 0.0] max t 17 len buf 42690 1\n",
      "episode 5609 scores [0.10000000149011612, -0.009999999776482582] max t 30 len buf 42709 1\n",
      "episode 5613 scores [-0.009999999776482582, 0.10000000149011612] max t 43 len buf 42743 1\n",
      "episode 5620 scores [-0.009999999776482582, 0.10000000149011612] max t 29 len buf 42790 1\n",
      "episode 5622 scores [-0.009999999776482582, 0.10000000149011612] max t 57 len buf 42823 1\n",
      "episode 5652 scores [-0.009999999776482582, 0.10000000149011612] max t 22 len buf 42884 1\n",
      "episode 5667 scores [0.10000000149011612, -0.009999999776482582] max t 32 len buf 42910 1\n",
      "episode 5683 scores [0.10000000149011612, -0.009999999776482582] max t 30 len buf 42946 1\n",
      "episode 5777 scores [0.10000000149011612, -0.009999999776482582] max t 18 len buf 42980 1\n",
      "buffer len 43000 episodes 5778 scores [0.10000000149011612, -0.009999999776482582]\n",
      "buffer len 43000 episodes 5779 scores [-0.009999999776482582, 0.0]\n",
      "buffer len 43000 episodes 5780 scores [-0.009999999776482582, 0.0]\n",
      "buffer len 43000 episodes 5781 scores [0.0, -0.009999999776482582]\n",
      "buffer len 43000 episodes 5782 scores [0.0, -0.009999999776482582]\n",
      "buffer len 43000 episodes 5783 scores [0.0, -0.009999999776482582]\n",
      "buffer len 43000 episodes 5784 scores [0.0, -0.009999999776482582]\n",
      "buffer len 43000 episodes 5785 scores [-0.009999999776482582, 0.0]\n",
      "buffer len 43000 episodes 5786 scores [0.0, -0.009999999776482582]\n",
      "buffer len 43000 episodes 5787 scores [-0.009999999776482582, 0.0]\n",
      "buffer len 43000 episodes 5788 scores [-0.009999999776482582, 0.0]\n",
      "buffer len 43000 episodes 5789 scores [-0.009999999776482582, 0.0]\n",
      "buffer len 43000 episodes 5790 scores [-0.009999999776482582, 0.0]\n",
      "episode 5790 scores [0.10000000149011612, 0.09000000171363354] max t 47 len buf 43000 2\n",
      "episode 5801 scores [0.09000000357627869, 0.0] max t 18 len buf 43288 1\n",
      "episode 5821 scores [0.0, 0.09000000171363354] max t 29 len buf 43308 1\n",
      "episode 5825 scores [-0.009999999776482582, 0.10000000149011612] max t 29 len buf 43341 1\n",
      "episode 5832 scores [-0.009999999776482582, 0.10000000149011612] max t 27 len buf 43374 1\n",
      "episode 5841 scores [0.10000000149011612, 0.09000000171363354] max t 50 len buf 43405 2\n",
      "episode 5869 scores [0.0, 0.09000000171363354] max t 31 len buf 43696 1\n",
      "episode 5873 scores [0.20000000298023224, 0.09000000171363354] max t 59 len buf 43731 3\n",
      "episode 5881 scores [0.10000000149011612, -0.009999999776482582] max t 33 len buf 46221 1\n",
      "episode 5922 scores [-0.009999999776482582, 0.10000000149011612] max t 36 len buf 46258 1\n",
      "episode 5940 scores [-0.009999999776482582, 0.10000000149011612] max t 32 len buf 46298 1\n",
      "episode 5950 scores [-0.009999999776482582, 0.10000000149011612] max t 31 len buf 46334 1\n",
      "episode 5952 scores [-0.009999999776482582, 0.10000000149011612] max t 50 len buf 46369 1\n",
      "episode 5971 scores [-0.009999999776482582, 0.10000000149011612] max t 23 len buf 46423 1\n",
      "episode 5980 scores [-0.009999999776482582, 0.10000000149011612] max t 32 len buf 46450 1\n",
      "episode 6008 scores [0.10000000149011612, -0.009999999776482582] max t 29 len buf 46486 1\n",
      "episode 6015 scores [-0.009999999776482582, 0.10000000149011612] max t 47 len buf 46519 1\n",
      "episode 6019 scores [0.0, 0.09000000171363354] max t 29 len buf 46570 1\n",
      "episode 6036 scores [0.10000000149011612, -0.009999999776482582] max t 26 len buf 46603 1\n",
      "episode 6043 scores [0.10000000149011612, -0.009999999776482582] max t 25 len buf 46633 1\n",
      "episode 6049 scores [-0.009999999776482582, 0.10000000149011612] max t 32 len buf 46662 1\n",
      "episode 6072 scores [-0.009999999776482582, 0.10000000149011612] max t 32 len buf 46698 1\n",
      "episode 6091 scores [0.10000000149011612, -0.009999999776482582] max t 24 len buf 46734 1\n",
      "episode 6099 scores [0.10000000149011612, 0.09000000171363354] max t 55 len buf 46762 2\n",
      "episode 6102 scores [-0.009999999776482582, 0.10000000149011612] max t 32 len buf 47058 1\n",
      "episode 6119 scores [0.09000000171363354, 0.10000000149011612] max t 45 len buf 47094 2\n",
      "episode 6134 scores [-0.009999999776482582, 0.10000000149011612] max t 28 len buf 47380 1\n",
      "episode 6158 scores [0.10000000149011612, -0.009999999776482582] max t 31 len buf 47412 1\n",
      "episode 6171 scores [0.10000000149011612, 0.09000000171363354] max t 50 len buf 47447 2\n",
      "episode 6198 scores [-0.009999999776482582, 0.10000000149011612] max t 34 len buf 47738 1\n",
      "episode 6227 scores [-0.009999999776482582, 0.10000000149011612] max t 25 len buf 47776 1\n",
      "episode 6275 scores [0.10000000149011612, -0.009999999776482582] max t 32 len buf 47805 1\n",
      "episode 6277 scores [-0.009999999776482582, 0.10000000149011612] max t 32 len buf 47841 1\n",
      "episode 6303 scores [-0.009999999776482582, 0.10000000149011612] max t 32 len buf 47877 1\n",
      "episode 6312 scores [0.09000000171363354, 0.0] max t 30 len buf 47913 1\n",
      "episode 6322 scores [0.10000000149011612, -0.009999999776482582] max t 29 len buf 47947 1\n",
      "episode 6362 scores [0.10000000149011612, -0.009999999776482582] max t 31 len buf 47980 1\n",
      "episode 6368 scores [0.20000000298023224, 0.09000000171363354] max t 69 len buf 48015 3\n",
      "episode 6408 scores [0.10000000149011612, -0.009999999776482582] max t 51 len buf 50515 1\n",
      "episode 6456 scores [0.10000000149011612, 0.1900000050663948] max t 60 len buf 50570 3\n",
      "episode 6467 scores [0.10000000149011612, -0.009999999776482582] max t 44 len buf 52521 1\n",
      "episode 6476 scores [0.10000000149011612, -0.009999999776482582] max t 50 len buf 52569 1\n",
      "episode 6504 scores [0.10000000149011612, -0.009999999776482582] max t 40 len buf 52623 1\n",
      "episode 6507 scores [-0.009999999776482582, 0.10000000149011612] max t 32 len buf 52667 1\n",
      "episode 6537 scores [0.10000000149011612, -0.009999999776482582] max t 32 len buf 52703 1\n",
      "episode 6552 scores [0.0, 0.09000000171363354] max t 29 len buf 52739 1\n",
      "episode 6601 scores [-0.009999999776482582, 0.10000000149011612] max t 31 len buf 52772 1\n",
      "episode 6603 scores [0.10000000149011612, -0.009999999776482582] max t 50 len buf 52807 1\n",
      "episode 6638 scores [0.10000000149011612, -0.009999999776482582] max t 24 len buf 52861 1\n",
      "episode 6643 scores [0.10000000149011612, -0.009999999776482582] max t 32 len buf 52889 1\n",
      "episode 6683 scores [0.10000000149011612, -0.009999999776482582] max t 28 len buf 52925 1\n",
      "episode 6728 scores [0.10000000149011612, -0.009999999776482582] max t 28 len buf 52957 1\n",
      "episode 6730 scores [0.10000000149011612, -0.009999999776482582] max t 30 len buf 52989 1\n",
      "episode 6733 scores [0.0, 0.09000000171363354] max t 34 len buf 53023 1\n",
      "episode 6749 scores [0.10000000149011612, -0.009999999776482582] max t 32 len buf 53061 1\n",
      "episode 6769 scores [0.10000000149011612, -0.009999999776482582] max t 21 len buf 53097 1\n",
      "episode 6792 scores [0.0, 0.09000000171363354] max t 29 len buf 53122 1\n",
      "episode 6795 scores [0.09000000357627869, 0.0] max t 20 len buf 53155 1\n",
      "episode 6811 scores [-0.009999999776482582, 0.10000000149011612] max t 24 len buf 53177 1\n",
      "episode 6832 scores [0.10000000149011612, -0.009999999776482582] max t 28 len buf 53205 1\n",
      "episode 6860 scores [0.10000000149011612, -0.009999999776482582] max t 27 len buf 53237 1\n",
      "episode 6864 scores [0.10000000149011612, -0.009999999776482582] max t 42 len buf 53268 1\n",
      "episode 6868 scores [0.10000000149011612, -0.009999999776482582] max t 23 len buf 53314 1\n",
      "episode 6905 scores [0.10000000149011612, -0.009999999776482582] max t 30 len buf 53341 1\n",
      "episode 6910 scores [0.10000000149011612, -0.009999999776482582] max t 27 len buf 53375 1\n",
      "episode 6912 scores [0.10000000149011612, -0.009999999776482582] max t 48 len buf 53406 1\n",
      "episode 6927 scores [0.09000000357627869, 0.0] max t 20 len buf 53458 1\n",
      "episode 6954 scores [0.09000000171363354, 0.10000000149011612] max t 56 len buf 53480 2\n",
      "episode 6971 scores [-0.009999999776482582, 0.10000000149011612] max t 22 len buf 53777 1\n",
      "episode 7011 scores [-0.009999999776482582, 0.10000000149011612] max t 32 len buf 53803 1\n",
      "episode 7063 scores [0.10000000149011612, -0.009999999776482582] max t 30 len buf 53839 1\n",
      "episode 7074 scores [0.09000000171363354, 0.10000000149011612] max t 45 len buf 53873 2\n",
      "episode 7096 scores [0.10000000149011612, 0.09000000171363354] max t 42 len buf 54159 2\n",
      "episode 7106 scores [0.10000000149011612, 0.09000000171363354] max t 49 len buf 54442 2\n",
      "episode 7112 scores [0.10000000149011612, -0.009999999776482582] max t 32 len buf 54732 1\n",
      "episode 7120 scores [-0.009999999776482582, 0.10000000149011612] max t 31 len buf 54768 1\n",
      "episode 7166 scores [0.10000000149011612, -0.009999999776482582] max t 29 len buf 54803 1\n",
      "episode 7172 scores [-0.009999999776482582, 0.10000000149011612] max t 28 len buf 54836 1\n",
      "episode 7178 scores [0.0, 0.09000000171363354] max t 30 len buf 54868 1\n",
      "episode 7186 scores [0.10000000149011612, -0.009999999776482582] max t 29 len buf 54902 1\n",
      "episode 7227 scores [-0.009999999776482582, 0.10000000149011612] max t 26 len buf 54935 1\n",
      "episode 7271 scores [0.10000000149011612, -0.009999999776482582] max t 50 len buf 54965 1\n",
      "episode 7358 scores [0.10000000149011612, -0.009999999776482582] max t 29 len buf 55019 1\n",
      "episode 7359 scores [0.10000000149011612, -0.009999999776482582] max t 32 len buf 55052 1\n",
      "episode 7360 scores [-0.009999999776482582, 0.10000000149011612] max t 50 len buf 55088 1\n",
      "episode 7364 scores [0.09000000171363354, 0.10000000149011612] max t 53 len buf 55142 2\n",
      "episode 7376 scores [0.10000000149011612, -0.009999999776482582] max t 30 len buf 55436 1\n",
      "episode 7381 scores [0.10000000149011612, -0.009999999776482582] max t 29 len buf 55470 1\n",
      "episode 7387 scores [0.10000000149011612, -0.009999999776482582] max t 32 len buf 55503 1\n",
      "episode 7393 scores [0.10000000149011612, -0.009999999776482582] max t 26 len buf 55539 1\n",
      "episode 7394 scores [0.10000000149011612, -0.009999999776482582] max t 31 len buf 55569 1\n",
      "episode 7433 scores [0.10000000149011612, -0.009999999776482582] max t 28 len buf 55604 1\n",
      "episode 7465 scores [-0.009999999776482582, 0.10000000149011612] max t 32 len buf 55636 1\n",
      "episode 7466 scores [0.10000000149011612, -0.009999999776482582] max t 24 len buf 55672 1\n",
      "buffer len 55700 episodes 7467 scores [0.10000000149011612, -0.009999999776482582]\n",
      "buffer len 55700 episodes 7468 scores [-0.009999999776482582, 0.0]\n",
      "buffer len 55700 episodes 7469 scores [0.0, -0.009999999776482582]\n",
      "buffer len 55700 episodes 7470 scores [0.0, -0.009999999776482582]\n",
      "buffer len 55700 episodes 7471 scores [-0.009999999776482582, 0.0]\n",
      "buffer len 55700 episodes 7472 scores [0.0, -0.009999999776482582]\n",
      "buffer len 55700 episodes 7473 scores [-0.009999999776482582, 0.0]\n",
      "buffer len 55700 episodes 7474 scores [-0.009999999776482582, 0.0]\n",
      "buffer len 55700 episodes 7475 scores [0.0, -0.009999999776482582]\n",
      "buffer len 55700 episodes 7476 scores [-0.009999999776482582, 0.0]\n",
      "buffer len 55700 episodes 7477 scores [0.0, -0.009999999776482582]\n",
      "buffer len 55700 episodes 7478 scores [0.0, -0.009999999776482582]\n",
      "buffer len 55700 episodes 7479 scores [0.0, -0.009999999776482582]\n",
      "buffer len 55700 episodes 7480 scores [0.0, -0.009999999776482582]\n",
      "buffer len 55700 episodes 7481 scores [-0.009999999776482582, 0.0]\n",
      "episode 7481 scores [0.10000000149011612, -0.009999999776482582] max t 21 len buf 55700 1\n",
      "episode 7489 scores [-0.009999999776482582, 0.10000000149011612] max t 21 len buf 55723 1\n",
      "episode 7496 scores [-0.009999999776482582, 0.10000000149011612] max t 32 len buf 55748 1\n",
      "episode 7498 scores [0.10000000149011612, -0.009999999776482582] max t 32 len buf 55784 1\n",
      "episode 7512 scores [-0.009999999776482582, 0.10000000149011612] max t 26 len buf 55820 1\n",
      "episode 7519 scores [-0.009999999776482582, 0.10000000149011612] max t 47 len buf 55850 1\n",
      "episode 7529 scores [0.0, 0.09000000171363354] max t 31 len buf 55901 1\n",
      "episode 7533 scores [-0.009999999776482582, 0.10000000149011612] max t 32 len buf 55936 1\n",
      "episode 7535 scores [-0.009999999776482582, 0.10000000149011612] max t 47 len buf 55972 1\n",
      "episode 7549 scores [0.10000000149011612, -0.009999999776482582] max t 28 len buf 56023 1\n",
      "episode 7616 scores [-0.009999999776482582, 0.10000000149011612] max t 25 len buf 56055 1\n",
      "episode 7644 scores [0.0, 0.09000000171363354] max t 29 len buf 56084 1\n",
      "episode 7652 scores [0.10000000149011612, -0.009999999776482582] max t 26 len buf 56117 1\n",
      "episode 7655 scores [-0.009999999776482582, 0.10000000149011612] max t 27 len buf 56147 1\n",
      "episode 7682 scores [-0.009999999776482582, 0.10000000149011612] max t 32 len buf 56178 1\n",
      "episode 7687 scores [0.10000000149011612, 0.09000000171363354] max t 52 len buf 56214 2\n",
      "episode 7695 scores [0.10000000149011612, -0.009999999776482582] max t 32 len buf 56507 1\n",
      "episode 7697 scores [0.10000000149011612, -0.009999999776482582] max t 51 len buf 56543 1\n",
      "episode 7699 scores [-0.009999999776482582, 0.10000000149011612] max t 32 len buf 56598 1\n",
      "episode 7700 scores [0.10000000149011612, -0.009999999776482582] max t 31 len buf 56634 1\n",
      "episode 7719 scores [0.10000000149011612, -0.009999999776482582] max t 31 len buf 56669 1\n",
      "episode 7733 scores [0.10000000149011612, -0.009999999776482582] max t 27 len buf 56704 1\n",
      "episode 7738 scores [-0.009999999776482582, 0.10000000149011612] max t 37 len buf 56735 1\n",
      "episode 7741 scores [0.09000000171363354, 0.0] max t 41 len buf 56776 1\n",
      "episode 7758 scores [0.0, 0.09000000171363354] max t 30 len buf 56821 1\n",
      "episode 7768 scores [-0.009999999776482582, 0.10000000149011612] max t 24 len buf 56855 1\n",
      "episode 7793 scores [0.0, 0.09000000171363354] max t 31 len buf 56883 1\n",
      "episode 7818 scores [0.10000000149011612, -0.009999999776482582] max t 54 len buf 56918 1\n",
      "episode 7829 scores [-0.009999999776482582, 0.10000000149011612] max t 25 len buf 56976 1\n",
      "episode 7840 scores [0.20000000298023224, -0.009999999776482582] max t 48 len buf 57005 2\n",
      "episode 7873 scores [-0.009999999776482582, 0.10000000149011612] max t 32 len buf 57214 1\n",
      "episode 7876 scores [-0.009999999776482582, 0.10000000149011612] max t 32 len buf 57250 1\n",
      "episode 7890 scores [0.10000000149011612, -0.019999999552965164] max t 32 len buf 57286 1\n",
      "episode 7897 scores [0.10000000149011612, -0.009999999776482582] max t 32 len buf 57322 1\n",
      "episode 7904 scores [-0.009999999776482582, 0.10000000149011612] max t 43 len buf 57358 1\n",
      "episode 7962 scores [-0.009999999776482582, 0.10000000149011612] max t 34 len buf 57405 1\n",
      "episode 7975 scores [0.10000000149011612, -0.009999999776482582] max t 52 len buf 57443 1\n",
      "episode 7979 scores [0.10000000149011612, -0.009999999776482582] max t 32 len buf 57499 1\n",
      "episode 7994 scores [0.10000000149011612, -0.009999999776482582] max t 31 len buf 57535 1\n",
      "episode 8023 scores [0.10000000149011612, -0.009999999776482582] max t 27 len buf 57570 1\n",
      "episode 8027 scores [-0.009999999776482582, 0.10000000149011612] max t 50 len buf 57601 1\n",
      "episode 8031 scores [-0.009999999776482582, 0.10000000149011612] max t 24 len buf 57655 1\n",
      "episode 8041 scores [0.10000000149011612, 0.09000000171363354] max t 44 len buf 57683 2\n",
      "episode 8070 scores [0.10000000149011612, -0.009999999776482582] max t 26 len buf 57968 1\n",
      "episode 8076 scores [0.0, 0.09000000171363354] max t 49 len buf 57998 1\n",
      "episode 8082 scores [-0.009999999776482582, 0.10000000149011612] max t 33 len buf 58051 1\n",
      "episode 8092 scores [-0.009999999776482582, 0.10000000149011612] max t 36 len buf 58088 1\n",
      "episode 8103 scores [0.10000000149011612, -0.009999999776482582] max t 43 len buf 58128 1\n",
      "episode 8265 scores [0.10000000149011612, -0.009999999776482582] max t 32 len buf 58175 1\n",
      "episode 8267 scores [0.10000000149011612, -0.009999999776482582] max t 29 len buf 58211 1\n",
      "episode 8272 scores [0.10000000149011612, -0.009999999776482582] max t 43 len buf 58244 1\n",
      "episode 8281 scores [-0.009999999776482582, 0.10000000149011612] max t 32 len buf 58291 1\n",
      "episode 8312 scores [-0.009999999776482582, 0.10000000149011612] max t 28 len buf 58327 1\n",
      "episode 8323 scores [-0.009999999776482582, 0.10000000149011612] max t 32 len buf 58359 1\n",
      "episode 8337 scores [0.10000000149011612, -0.009999999776482582] max t 25 len buf 58395 1\n",
      "episode 8354 scores [0.10000000149011612, -0.009999999776482582] max t 26 len buf 58424 1\n",
      "episode 8359 scores [-0.009999999776482582, 0.10000000149011612] max t 32 len buf 58454 1\n",
      "episode 8365 scores [0.10000000149011612, 0.19000000320374966] max t 67 len buf 58490 3\n",
      "60988 8366 [0.10000000149011612, 0.19000000320374966]\n"
     ]
    }
   ],
   "source": [
    "buffer = ReplayBuffer(int(BUFFER_SIZE))\n",
    "fill_buffer(buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import csv\n",
    "# bufflist = list(buffer.deque)\n",
    "\n",
    "# with open('buffer.csv', 'w', newline='') as myfile:\n",
    "#     wr = csv.writer(myfile, quoting=csv.QUOTE_ALL)\n",
    "#     wr.writerow(bufflist)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# maddpg = MADDPG(IN_ACTOR_DIM, HIDDEN_ACTOR_IN_DIM, HIDDEN_ACTOR_OUT_DIM, OUT_ACTOR_DIM,\\\n",
    "#                 IN_CRIT_S, IN_CRIT_A, HIDDEN_CRIT_IN_DIM, HIDDEN_CRIT_OUT_DIM, SEED, LR_ACT, LR_CRI, DISC, TAU)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# start training little by little\n",
    "# parameters can be tuned here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "NUMBER_OF_EPISODES = 10000\n",
    "EPISODE_LENGTH = 1000\n",
    "BATCHSIZE =200\n",
    "MIN_BUFFER_SIZE = 60000\n",
    "UPDATE_EVERY = 1\n",
    "# amplitude of OU noise\n",
    "# this slowly decreases to 0\n",
    "# instead of resetting noise to 0 every episode, we let it decrease to 0 over a few episodes\n",
    "\n",
    "NOISE = 1\n",
    "NO_NOISE_AFTER = 3000\n",
    "NOISE_DECAY = 0.9999\n",
    "\n",
    "BUFFER_SIZE = 300000\n",
    "\n",
    "IN_ACTOR_DIM = 24 \n",
    "HIDDEN_ACTOR_IN_DIM = 400\n",
    "HIDDEN_ACTOR_OUT_DIM = 400\n",
    "OUT_ACTOR_DIM = 2\n",
    "\n",
    "# Critic input contains both states AND all the actions of all the agents\n",
    "# there are 2 agents, so 24*2 + 2*2 = 28\n",
    "IN_CRIT_S = IN_ACTOR_DIM  * num_agents \n",
    "IN_CRIT_A = action_size * num_agents\n",
    "HIDDEN_CRIT_IN_DIM = 400\n",
    "HIDDEN_CRIT_OUT_DIM = 400\n",
    "OUT_CRIT_DIM = 1\n",
    "\n",
    "# how many periods before update\n",
    "\n",
    "SEED = 6\n",
    "DISC = 0.99\n",
    "TAU = 0.001\n",
    "LR_ACT = 0.0001\n",
    "LR_CRI = 0.001\n",
    "\n",
    "\n",
    "# these will be used to print rewards for agents\n",
    "agent0_reward = []\n",
    "agent1_reward = []\n",
    "scores_deque = deque(maxlen=100)\n",
    "best_scores = []\n",
    "avg_best_score = []\n",
    "update_t = 0\n",
    "#     max_state =  env_info_demo.vector_observations[0]\n",
    "#     max_action = [0,0]\n",
    "times_updated = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # reset network if did not learn well, do not reset buffer\n",
    "# maddpg = MADDPG(IN_ACTOR_DIM, HIDDEN_ACTOR_IN_DIM, HIDDEN_ACTOR_OUT_DIM, OUT_ACTOR_DIM,\\\n",
    "#                 IN_CRIT_S, IN_CRIT_A, HIDDEN_CRIT_IN_DIM, HIDDEN_CRIT_OUT_DIM, SEED, LR_ACT, LR_CRI, DISC, TAU)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "aaaa =[ [ -7.0089,   0.5568,   3.1098,   2.4874,   6.8317,   1.3903, 3.1098,   2.4874,  -6.0478,   0.7466,   9.6109,   1.5064, 6.8317,   0.4707,   9.6109,   1.5064,  -6.4336,   0.8384, -3.8586,   0.5254,   6.8317,  -0.4489,  -3.8586,   0.5254]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bbbb = [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000, 0.0000,   0.0000,  -7.4364,  -1.5000,  -0.0000,   0.0000, 6.8317,   5.8587,  -0.0000,   0.0000,  -7.1158,  -1.5589, 3.2063,  -0.9810,   6.8317,   5.6429,   3.2063,  -0.9810]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "aaaa.append(bbbb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "totest = convert_to_tensor([aaaa,aaaa])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor(1.00000e-02 *\n",
       "        [[-3.6408,  4.4867],\n",
       "         [-4.1375,  4.6489]]), tensor(1.00000e-02 *\n",
       "        [[-3.6408,  4.4867],\n",
       "         [-4.1375,  4.6489]])]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pre-update do not refresh\n",
    "actions = maddpg.act(totest) \n",
    "actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor(1.00000e-02 *\n",
       "        [[-3.6408,  4.4867],\n",
       "         [-4.1375,  4.6489]]), tensor(1.00000e-02 *\n",
       "        [[-3.6408,  4.4867],\n",
       "         [-4.1375,  4.6489]])]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pre-update do not refresh\n",
    "actions = maddpg.target_act(totest) \n",
    "actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# main function that sets up environments\n",
    "# perform training loop\n",
    "\n",
    "\n",
    "\n",
    "# use keep_awake to keep workspace from disconnecting\n",
    "for episode in range(1, NUMBER_OF_EPISODES+1):\n",
    "\n",
    "    env_info = env.reset(train_mode=True)[brain_name]\n",
    "    states = env_info.vector_observations\n",
    "\n",
    "    scores = [0,0]   \n",
    "    all_t_episode =[]\n",
    "    \n",
    "    if episode > NO_NOISE_AFTER:\n",
    "        NOISE *= NOISE_DECAY\n",
    "\n",
    "    for episode_t in range(EPISODE_LENGTH):\n",
    "\n",
    "        state_tensors = convert_to_tensor(states)\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "        actions = maddpg.act(state_tensors, noise = NOISE)\n",
    "        actions_array = torch.stack(actions).detach().numpy()\n",
    "\n",
    "        env_info = env.step(actions_array)[brain_name] \n",
    "        next_states = env_info.vector_observations\n",
    "        rewards = env_info.rewards\n",
    "        dones  = env_info.local_done\n",
    "\n",
    "        transition = process_data(states, actions_array, rewards, next_states, dones)\n",
    "        all_t_episode.append(transition)\n",
    "                \n",
    "        scores = [sum(x) for x in zip(scores, rewards)]\n",
    "\n",
    "        states = next_states    \n",
    "\n",
    "\n",
    "        update_t = (update_t + 1) % UPDATE_EVERY\n",
    "        \n",
    "        if len(buffer) > MIN_BUFFER_SIZE and update_t == 0:\n",
    "            times_updated += 1\n",
    "            samplesa = buffer.sample(BATCHSIZE)\n",
    "            samplesb = buffer.sample(BATCHSIZE)\n",
    "            maddpg.update(samplesa, samplesb)\n",
    "\n",
    "        if dones[0]:\n",
    "            if max(scores) > 0.05 :\n",
    "                how_many_times = 1 if sum(scores) < 0.11 else int(sum(scores)/ 0.08) ** int(sum(scores)/ 0.08) \n",
    "                print(round(max(scores),1), NOISE, episode, episode_t, len(buffer), list(zip(*actions_array)), int(sum(scores)/ 0.08) )\n",
    "                for transition in all_t_episode:                                    \n",
    "                    buffer.push(transition) \n",
    "                    if t+3 > len(all_t_episode):\n",
    "                        continue\n",
    "                    elif max(transition[3]) > 0.05 or max(all_t_episode[t+1][3]) > 0.05  or max(all_t_episode[t+2][3]) > 0.05:\n",
    "                        for j in range(min(how_many_times, 200)):\n",
    "                            buffer.push(transition)          \n",
    "                            \n",
    "            elif sum(np.isnan(list(zip(*actions_array))[0])) + sum(np.isnan(list(zip(*actions_array))[1]))  >= 1:               \n",
    "                print(round(max(scores),1), NOISE, episode, episode_t, len(buffer), list(zip(*actions_array)), list(states), list(next_states))                    \n",
    "\n",
    "            break                      \n",
    "                    \n",
    "    agent0_reward.append(scores[0])\n",
    "    agent1_reward.append(scores[1])\n",
    "\n",
    "    best_scores.append(max(scores))\n",
    "    scores_deque.append(max(scores))    \n",
    "    avg_best_score.append(np.mean(scores_deque))\n",
    "    \n",
    "    \n",
    "    if np.mean(scores_deque) >= 0.4:\n",
    "        LR_ACT = 0.00001\n",
    "        LR_CRI = 0.00005\n",
    "    elif np.mean(scores_deque) >= 0.3:\n",
    "        LR_ACT = 0.00005\n",
    "        LR_CRI = 0.0001       \n",
    "    elif np.mean(scores_deque) >= 0.15:\n",
    "        LR_ACT = 0.00008\n",
    "        LR_CRI = 0.0005    \n",
    "\n",
    "\n",
    "    # print score every 100 episodes and save model \n",
    "    if episode % 100 == 0 or episode == NUMBER_OF_EPISODES-1 or  np.mean(scores_deque)>=0.5:\n",
    "\n",
    "        print('\\rEpisode {}\\tBuffer Len {}\\tAverage Last 100 Episodes Score: {:.2f}'.format(episode, len(buffer),np.mean(scores_deque)))\n",
    "        print(\"times_updated\", times_updated)\n",
    "        print()\n",
    "\n",
    "\n",
    "    # problem solved\n",
    "    if  np.mean(scores_deque)>=0.5:\n",
    "        print('\\nEnvironment solved in {:d} episodes!\\tAverage Last 100 Episodes Score: {:.2f}'.format(episode, np.mean(scores_deque)))            \n",
    "\n",
    "        break     \n",
    "#     if times_updated == 10:\n",
    "#         break\n",
    "\n",
    "\n",
    "\n",
    "print(len(buffer))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# refresh\n",
    "actions = maddpg.act(totest) \n",
    "actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "states, full_state, actions, rewards, next_states, next_full_state, dones = map(transpose_to_tensor, samples)\n",
    "full_states = [samples[1], samples[5]]\n",
    "samples = [states, actions, rewards, next_states, dones]\n",
    "samples.extend(convert_to_tensor(full_states))  \n",
    "states, actions, rewards, next_states, dones, full_state, next_full_state = samples\n",
    "states = torch.stack(states)   \n",
    "next_states = torch.stack(next_states) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 24])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import copy\n",
    "totest = copy.deepcopy (states)\n",
    "totest.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.2843,  0.0706],\n",
      "        [ 0.2297,  0.1283],\n",
      "        [ 0.2186,  0.0077]])\n",
      "tensor([[ 0.0456,  0.1667],\n",
      "        [ 0.0543,  0.1656],\n",
      "        [-0.0531,  0.2090]])\n"
     ]
    }
   ],
   "source": [
    "# pre-update do not refresh\n",
    "actions = maddpg.act(totest, noise = 0) \n",
    "print(actions[0][:5])\n",
    "print(actions[1][:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.5040, -0.0112],\n",
      "        [-0.4286,  0.0675],\n",
      "        [ 0.1861,  0.0641]])\n",
      "tensor([[ 0.3712, -0.0869],\n",
      "        [ 0.3318, -0.0746],\n",
      "        [-0.6440, -0.0688]])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# pre-update do not refresh\n",
    "actions = maddpg.target_act(totest) \n",
    "print(actions[0][:5])\n",
    "print(actions[1][:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.2046,  0.1649],\n",
      "        [ 0.1590,  0.2091],\n",
      "        [ 0.1694,  0.0725]])\n",
      "tensor([[-0.0192,  0.0352],\n",
      "        [-0.0088,  0.0342],\n",
      "        [-0.1244,  0.0633]])\n"
     ]
    }
   ],
   "source": [
    "# post-update: refresh\n",
    "actions = maddpg.act(totest) \n",
    "print(actions[0][:5])\n",
    "print(actions[1][:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.5017, -0.0086],\n",
      "        [-0.4288,  0.0687],\n",
      "        [ 0.1877,  0.0620]])\n",
      "tensor([[ 0.3659, -0.0789],\n",
      "        [ 0.3462, -0.0919],\n",
      "        [-0.6502, -0.0618]])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# post update, refresh:\n",
    "actions = maddpg.target_act(totest, noise = 0) \n",
    "print(actions[0][:5])\n",
    "print(actions[1][:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# for i in range(5):                                         # play game for 5 episodes\n",
    "#     env_info = env.reset(train_mode=False)[brain_name]     # reset the environment    \n",
    "#     states = env_info.vector_observations                  # get the current state (for each agent)\n",
    "#     scores = np.zeros(num_agents)                          # initialize the score (for each agent)\n",
    "#     while True:\n",
    "# #         actions = [0.1+i*0.1, -0.1+i*0.1, 0.1+i*0.1, -0.1+i*0.1] # select an action (for each agent)\n",
    "# #         actions = np.clip(actions, -1, 1)                  # all actions between -1 and 1\n",
    "#         state_tensors = convert_to_tensor(states)\n",
    "        \n",
    "#         actions = maddpg.act(state_tensors, noise = 0)\n",
    "#         actions_array = torch.stack(actions).detach().numpy()\n",
    "#         env_info = env.step(actions_array)[brain_name]           # send all actions to tne environment\n",
    "#         next_states = env_info.vector_observations         # get next state (for each agent)\n",
    "#         rewards = env_info.rewards                         # get reward (for each agent)\n",
    "#         dones = env_info.local_done                        # see if episode finished\n",
    "#         scores += env_info.rewards                         # update the score (for each agent)\n",
    "#         states = next_states                               # roll over states to next time step\n",
    "#         if np.any(dones):                                  # exit loop if episode finished\n",
    "#             break\n",
    "#     print('Total score (averaged over agents) this episode: {}'.format(np.mean(scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(list(states).append(list(states)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        , -7.61386442, -1.5       , -0.        ,  0.        ,\n",
       "         6.49473906,  5.85873604, -0.        ,  0.        ]),\n",
       " array([ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        , -6.10514164, -1.5       ,  0.        ,  0.        ,\n",
       "        -6.49473906,  5.85873604,  0.        ,  0.        ]),\n",
       " array([ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        , -7.61386442, -1.5       , -0.        ,  0.        ,\n",
       "         6.49473906,  5.85873604, -0.        ,  0.        ]),\n",
       " array([ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        , -6.10514164, -1.5       ,  0.        ,  0.        ,\n",
       "        -6.49473906,  5.85873604,  0.        ,  0.        ])]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "states = env_info.vector_observations\n",
    "a = list(states)\n",
    "a+a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "state_tensors = convert_to_tensor(states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# target_actions = maddpg.target_act(state_tensors) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([-0.0414,  0.1445]), tensor([-0.0947,  0.2239])]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maddpg.act(state_tensors) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "states, full_state, actions, rewards, next_states, next_full_state, dones = map(transpose_to_tensor, samples)\n",
    "full_states = [samples[1], samples[5]]\n",
    "samples = [states, actions, rewards, next_states, dones]\n",
    "samples.extend(convert_to_tensor(full_states))  \n",
    "states, actions, rewards, next_states, dones, full_state, next_full_state = samples\n",
    "states = torch.stack(states)   \n",
    "next_states = torch.stack(next_states) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[ 0.1795,  0.4920],\n",
       "         [-0.2261,  0.0490],\n",
       "         [-0.0924, -0.0389]]), tensor([[ 0.0824,  0.5297],\n",
       "         [ 0.1877,  0.0659],\n",
       "         [-0.2619, -0.0863]])]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actions = maddpg.act(states, noise = 0) \n",
    "actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[-0.0192,  0.1840],\n",
       "         [-0.2922,  0.1319],\n",
       "         [ 0.1376, -0.2479]]), tensor([[ 0.1147,  0.3136],\n",
       "         [ 0.0838,  0.4019],\n",
       "         [-0.3635, -0.5947]])]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actions = maddpg.target_act(states) \n",
    "actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[tensor([ 0.2401, -0.0017]),\n",
       "  tensor([ 0.3762,  0.3117]),\n",
       "  tensor([-0.0924, -0.1699])],\n",
       " [tensor([ 0.5019,  0.0432]),\n",
       "  tensor([-0.5772, -0.2432]),\n",
       "  tensor([-0.5193, -0.0081])]]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[[b - a for a, b in  zip(target, cur)] for target, cur in zip(target_actions, actions)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000],\n",
       "        [ 0.0000],\n",
       "        [ 0.1263],\n",
       "        [ 0.0000],\n",
       "        [ 0.0986],\n",
       "        [ 0.0911],\n",
       "        [ 0.0246],\n",
       "        [ 0.0000],\n",
       "        [ 0.0000],\n",
       "        [ 0.0000],\n",
       "        [ 0.1755],\n",
       "        [ 0.1201],\n",
       "        [ 0.0000],\n",
       "        [ 0.0631],\n",
       "        [ 0.0000],\n",
       "        [ 0.0000],\n",
       "        [ 0.0000],\n",
       "        [ 0.0000],\n",
       "        [ 0.0000],\n",
       "        [ 0.0000],\n",
       "        [ 0.0000],\n",
       "        [ 0.0000],\n",
       "        [ 0.1428],\n",
       "        [ 0.0000],\n",
       "        [ 0.0000],\n",
       "        [ 0.0000],\n",
       "        [ 0.0028],\n",
       "        [ 0.0000],\n",
       "        [ 0.0000],\n",
       "        [ 0.0000],\n",
       "        [ 0.0000],\n",
       "        [ 0.0000],\n",
       "        [ 0.1209],\n",
       "        [ 0.0676],\n",
       "        [ 0.0241],\n",
       "        [ 0.0000],\n",
       "        [ 0.0000],\n",
       "        [ 0.0049],\n",
       "        [ 0.0000],\n",
       "        [ 0.0459],\n",
       "        [ 0.0374],\n",
       "        [ 0.0857],\n",
       "        [ 0.0000],\n",
       "        [ 0.0008],\n",
       "        [ 0.0000],\n",
       "        [ 0.0000],\n",
       "        [ 0.0794],\n",
       "        [ 0.0027],\n",
       "        [ 0.0000],\n",
       "        [ 0.0000],\n",
       "        [ 0.0000],\n",
       "        [ 0.0000],\n",
       "        [ 0.0000],\n",
       "        [ 0.0000],\n",
       "        [ 0.0000],\n",
       "        [ 0.0000],\n",
       "        [ 0.0000],\n",
       "        [ 0.0000],\n",
       "        [ 0.0000],\n",
       "        [ 0.1173],\n",
       "        [ 0.0000],\n",
       "        [ 0.0000],\n",
       "        [ 0.0650],\n",
       "        [ 0.0000],\n",
       "        [ 0.0000],\n",
       "        [ 0.0000],\n",
       "        [ 0.0010],\n",
       "        [ 0.0000],\n",
       "        [ 0.0000],\n",
       "        [ 0.0000],\n",
       "        [ 0.0000],\n",
       "        [ 0.0230],\n",
       "        [ 0.1161],\n",
       "        [ 0.1239],\n",
       "        [ 0.0000],\n",
       "        [ 0.1124],\n",
       "        [ 0.1101],\n",
       "        [ 0.0000],\n",
       "        [ 0.0000],\n",
       "        [ 0.0000],\n",
       "        [ 0.0000],\n",
       "        [ 0.0000],\n",
       "        [ 0.0000],\n",
       "        [ 0.0000],\n",
       "        [ 0.0952],\n",
       "        [ 0.0000],\n",
       "        [ 0.0080],\n",
       "        [ 0.1149],\n",
       "        [ 0.0000],\n",
       "        [ 0.0000],\n",
       "        [ 0.0000],\n",
       "        [ 0.0000],\n",
       "        [ 0.0000],\n",
       "        [ 0.0000],\n",
       "        [ 0.1101],\n",
       "        [ 0.0000],\n",
       "        [ 0.0000],\n",
       "        [ 0.0000],\n",
       "        [ 0.0000],\n",
       "        [ 0.0657],\n",
       "        [ 0.0000],\n",
       "        [ 0.1131],\n",
       "        [ 0.1552],\n",
       "        [ 0.0000],\n",
       "        [ 0.1637],\n",
       "        [ 0.0000],\n",
       "        [ 0.0000],\n",
       "        [ 0.0000],\n",
       "        [ 0.0000],\n",
       "        [ 0.0000],\n",
       "        [ 0.0586],\n",
       "        [ 0.0000],\n",
       "        [ 0.0000],\n",
       "        [ 0.0000],\n",
       "        [ 0.0000],\n",
       "        [ 0.0000],\n",
       "        [ 0.0000],\n",
       "        [ 0.0000],\n",
       "        [ 0.0000],\n",
       "        [ 0.0000],\n",
       "        [ 0.0000],\n",
       "        [ 0.0000],\n",
       "        [ 0.0000],\n",
       "        [ 0.0000],\n",
       "        [ 0.0000],\n",
       "        [ 0.1567],\n",
       "        [ 0.0000],\n",
       "        [ 0.0000],\n",
       "        [ 0.0000],\n",
       "        [ 0.0011],\n",
       "        [ 0.1612],\n",
       "        [ 0.0000],\n",
       "        [ 0.0000],\n",
       "        [ 0.0000],\n",
       "        [ 0.0000],\n",
       "        [ 0.1231],\n",
       "        [ 0.0000],\n",
       "        [ 0.0000],\n",
       "        [ 0.0000],\n",
       "        [ 0.1211],\n",
       "        [ 0.0000],\n",
       "        [ 0.1630],\n",
       "        [ 0.1196],\n",
       "        [ 0.0000],\n",
       "        [ 0.0000],\n",
       "        [ 0.0635],\n",
       "        [ 0.0149],\n",
       "        [ 0.1296],\n",
       "        [ 0.0085],\n",
       "        [ 0.0000],\n",
       "        [ 0.0000],\n",
       "        [ 0.0299],\n",
       "        [ 0.0000],\n",
       "        [ 0.0095],\n",
       "        [ 0.0000],\n",
       "        [ 0.0000],\n",
       "        [ 0.0000],\n",
       "        [ 0.0000],\n",
       "        [ 0.0000],\n",
       "        [ 0.0000],\n",
       "        [ 0.0000],\n",
       "        [ 0.0000],\n",
       "        [ 0.0082],\n",
       "        [ 0.0000],\n",
       "        [ 0.0000],\n",
       "        [ 0.0418],\n",
       "        [ 0.1284],\n",
       "        [ 0.0000],\n",
       "        [ 0.0000],\n",
       "        [ 0.0000],\n",
       "        [ 0.1121],\n",
       "        [ 0.0343],\n",
       "        [ 0.1159],\n",
       "        [ 0.0000],\n",
       "        [ 0.0000],\n",
       "        [ 0.0000],\n",
       "        [ 0.0000],\n",
       "        [ 0.0187],\n",
       "        [ 0.0000],\n",
       "        [ 0.0000],\n",
       "        [ 0.0747],\n",
       "        [ 0.0076],\n",
       "        [ 0.0000],\n",
       "        [ 0.0000],\n",
       "        [ 0.0798],\n",
       "        [ 0.0947],\n",
       "        [ 0.0000],\n",
       "        [ 0.1107],\n",
       "        [ 0.0000],\n",
       "        [ 0.0000],\n",
       "        [ 0.0747],\n",
       "        [ 0.0000],\n",
       "        [ 0.0000],\n",
       "        [ 0.0078],\n",
       "        [ 0.0000],\n",
       "        [ 0.0000],\n",
       "        [ 0.1192],\n",
       "        [ 0.1204],\n",
       "        [ 0.0000],\n",
       "        [ 0.0000]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actions = torch.cat(actions, dim = -1)\n",
    "maddpg.maddpg_agent[0].critic(full_state, actions)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# parameters before acting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([ 0.0614,  0.0776,  0.0733, -0.0623,  0.0069,  0.0882,  0.1878,\n",
       "         -0.0775,  0.1624, -0.1298,  0.0489,  0.0519,  0.1075,  0.0757,\n",
       "          0.0869,  0.0980,  0.1148,  0.1460, -0.0329, -0.1889, -0.1866,\n",
       "          0.0521, -0.1902, -0.1207, -0.1146,  0.1954, -0.1872,  0.0555,\n",
       "         -0.0057, -0.1293, -0.0743,  0.0381, -0.1553, -0.1234, -0.1482,\n",
       "          0.1442, -0.0027,  0.1010,  0.0871,  0.1273,  0.0889, -0.0430,\n",
       "         -0.1131,  0.1832, -0.1927, -0.1657, -0.1234, -0.0382,  0.1853,\n",
       "         -0.0689,  0.1911,  0.1133, -0.0692,  0.0568, -0.0260,  0.1529,\n",
       "          0.0542, -0.1123,  0.0204, -0.1332, -0.0745, -0.1476,  0.1090,\n",
       "          0.1588,  0.0445,  0.0001, -0.1284, -0.1406, -0.1908, -0.0132,\n",
       "          0.0420,  0.1286,  0.1713, -0.1106,  0.1939, -0.1120,  0.1367,\n",
       "          0.1041,  0.1672, -0.0961, -0.1855, -0.1575,  0.0260, -0.1187,\n",
       "          0.1208, -0.1921,  0.1859,  0.1075, -0.0026, -0.1263, -0.0985,\n",
       "         -0.1018,  0.0053, -0.0533,  0.1475, -0.1649, -0.0449,  0.1579,\n",
       "          0.0129,  0.1808]), Parameter containing:\n",
       " tensor([[ 0.4695],\n",
       "         [ 0.5669],\n",
       "         [ 0.4394],\n",
       "         [ 0.5985],\n",
       "         [ 0.5350],\n",
       "         [ 0.5277],\n",
       "         [ 0.5709],\n",
       "         [ 0.5412],\n",
       "         [ 0.6134],\n",
       "         [ 0.5480],\n",
       "         [ 0.6050],\n",
       "         [ 0.6281],\n",
       "         [ 0.5127],\n",
       "         [ 0.5896],\n",
       "         [ 0.5924],\n",
       "         [ 0.6321],\n",
       "         [ 0.6244],\n",
       "         [ 0.5394],\n",
       "         [ 0.5290],\n",
       "         [ 0.5729],\n",
       "         [ 0.6061],\n",
       "         [ 0.4679],\n",
       "         [ 0.5881],\n",
       "         [ 0.6267],\n",
       "         [ 0.6479],\n",
       "         [ 0.5338],\n",
       "         [ 0.6136],\n",
       "         [ 0.5487],\n",
       "         [ 0.5316],\n",
       "         [ 0.6153],\n",
       "         [ 0.5254],\n",
       "         [ 0.5063],\n",
       "         [ 0.5120],\n",
       "         [ 0.5421],\n",
       "         [ 0.5290],\n",
       "         [ 0.5656],\n",
       "         [ 0.5728],\n",
       "         [ 0.5530],\n",
       "         [ 0.5423],\n",
       "         [ 0.6478],\n",
       "         [ 0.6300],\n",
       "         [ 0.5823],\n",
       "         [ 0.6764],\n",
       "         [ 0.5252],\n",
       "         [ 0.5588],\n",
       "         [ 0.4269],\n",
       "         [ 0.6743],\n",
       "         [ 0.5013],\n",
       "         [ 0.5795],\n",
       "         [ 0.6205],\n",
       "         [ 0.5621],\n",
       "         [ 0.6228],\n",
       "         [ 0.6214],\n",
       "         [ 0.6334],\n",
       "         [ 0.6899],\n",
       "         [ 0.5865],\n",
       "         [ 0.5371],\n",
       "         [ 0.5725],\n",
       "         [ 0.6091],\n",
       "         [ 0.5757],\n",
       "         [ 0.6157],\n",
       "         [ 0.6152],\n",
       "         [ 0.4838],\n",
       "         [ 0.6242],\n",
       "         [ 0.6052],\n",
       "         [ 0.5497],\n",
       "         [ 0.6235],\n",
       "         [ 0.5664],\n",
       "         [ 0.5861],\n",
       "         [ 0.5778],\n",
       "         [ 0.6196],\n",
       "         [ 0.6113],\n",
       "         [ 0.6343],\n",
       "         [ 0.6666],\n",
       "         [ 0.5288],\n",
       "         [ 0.6032],\n",
       "         [ 0.6051],\n",
       "         [ 0.5702],\n",
       "         [ 0.6205],\n",
       "         [ 0.6297],\n",
       "         [ 0.5383],\n",
       "         [ 0.6769],\n",
       "         [ 0.5186],\n",
       "         [ 0.4392],\n",
       "         [ 0.5990],\n",
       "         [ 0.6012],\n",
       "         [ 0.5966],\n",
       "         [ 0.6070],\n",
       "         [ 0.6745],\n",
       "         [ 0.5433],\n",
       "         [ 0.5758],\n",
       "         [ 0.5493],\n",
       "         [ 0.5552],\n",
       "         [ 0.5955],\n",
       "         [ 0.6279],\n",
       "         [ 0.6490],\n",
       "         [ 0.5553],\n",
       "         [ 0.5241],\n",
       "         [ 0.5823],\n",
       "         [ 0.6000]]), Parameter containing:\n",
       " tensor([[ 0.0295,  0.0220,  0.1987,  ..., -0.0885, -0.0466,  0.1363],\n",
       "         [ 0.0479, -0.0440, -0.1275,  ..., -0.0341,  0.0082,  0.1298],\n",
       "         [ 0.0154, -0.0887, -0.0082,  ..., -0.0633,  0.0559, -0.1047],\n",
       "         ...,\n",
       "         [ 0.0916, -0.1068, -0.0432,  ..., -0.1247,  0.1768, -0.0671],\n",
       "         [ 0.0041, -0.1645, -0.1312,  ...,  0.1022,  0.2010,  0.0408],\n",
       "         [ 0.0786,  0.0683,  0.1595,  ...,  0.1605, -0.1104, -0.1671]]), Parameter containing:\n",
       " tensor([ 0.8593,  0.6738,  0.8180,  0.5777,  0.3329,  0.6397,  0.5999,\n",
       "          0.2291,  0.2384,  0.7107,  0.3136,  0.4641,  0.8633,  0.7191,\n",
       "          0.3550,  0.0741,  0.7783,  0.9977,  0.8168,  0.5606,  0.8570,\n",
       "          0.1272,  0.1356,  0.2021,  0.7503,  0.2890,  0.3663,  0.7067,\n",
       "          0.7250,  0.3043,  0.6395,  0.9856,  0.2127,  0.0946,  0.5721,\n",
       "          0.9864,  0.4491,  0.5088,  0.7073,  0.5484,  0.6401,  0.6574,\n",
       "          0.1052,  0.1315,  0.2834,  0.3937,  0.8921,  0.5719,  0.2062,\n",
       "          0.7124,  0.2839,  0.9682,  0.1386,  0.6280,  0.1661,  0.3213,\n",
       "          0.6020,  0.5661,  0.3435,  0.9092,  0.3283,  0.8423,  0.8480,\n",
       "          0.3982,  0.7442,  0.1833,  0.5666,  0.3040,  0.4013,  0.6460,\n",
       "          0.9048,  0.9244,  0.4101,  0.5553,  0.0640,  0.5995,  0.9497,\n",
       "          0.7255,  0.3236,  0.4364,  0.6505,  0.8220,  0.5441,  0.2638,\n",
       "          0.6534,  0.9486,  0.1233,  0.0993,  0.0993,  0.8387,  0.9574,\n",
       "          0.2881,  0.5013,  0.0738,  0.9356,  0.4767,  0.1815,  0.8617,\n",
       "          0.7093,  0.3382]), Parameter containing:\n",
       " tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.]), Parameter containing:\n",
       " tensor(1.00000e-02 *\n",
       "        [ 9.5968,  3.0331, -0.1648,  6.3325,  2.9486, -2.5691, -0.0844,\n",
       "          3.9381, -6.2971, -1.3230,  1.6539, -7.9377,  4.6607,  1.2188,\n",
       "         -9.5690,  7.6403,  1.7529,  5.1379, -7.2387, -3.9019, -6.0586,\n",
       "          0.0238, -1.6716,  4.9077,  8.9157, -0.0508, -0.6343,  4.6408,\n",
       "          8.8291,  0.1772,  2.4683,  5.7235, -0.5844, -3.9422, -3.3837,\n",
       "          2.6423, -7.8770, -1.0159, -3.9616, -4.9694,  6.7877, -6.3628,\n",
       "         -6.0544, -0.3809,  3.7157,  8.7210, -9.6883, -6.6884, -1.7694,\n",
       "          1.7180,  4.5316, -2.0132, -9.0672, -0.4780, -3.5623, -1.0742,\n",
       "          1.0847, -9.5719,  4.0657,  5.1244, -6.7858, -4.2386, -8.3011,\n",
       "          3.5390,  9.3350, -4.3906,  0.5947,  4.4577,  2.4296,  9.5700,\n",
       "          6.5111,  4.4988, -4.0622, -1.2412,  1.9308, -7.1820,  9.3229,\n",
       "          7.0075,  8.0959, -8.7787,  5.9787, -3.6796, -0.4681,  4.1385,\n",
       "          1.8494, -0.3749,  6.7407, -1.4553,  8.1301, -8.8959, -4.4992,\n",
       "          9.3812,  4.8008, -0.3087, -1.9215, -5.2747, -2.7791, -7.4277,\n",
       "          5.7003,  0.5356]), Parameter containing:\n",
       " tensor([[ 0.5954],\n",
       "         [ 0.5763],\n",
       "         [ 0.6377],\n",
       "         [ 0.5798],\n",
       "         [ 0.6010],\n",
       "         [ 0.6105],\n",
       "         [ 0.5765],\n",
       "         [ 0.5779],\n",
       "         [ 0.5850],\n",
       "         [ 0.6029],\n",
       "         [ 0.5811],\n",
       "         [ 0.5939],\n",
       "         [ 0.5530],\n",
       "         [ 0.6140],\n",
       "         [ 0.5871],\n",
       "         [ 0.6250],\n",
       "         [ 0.5881],\n",
       "         [ 0.6329],\n",
       "         [ 0.5869],\n",
       "         [ 0.5474],\n",
       "         [ 0.5892],\n",
       "         [ 0.5784],\n",
       "         [ 0.6154],\n",
       "         [ 0.5764],\n",
       "         [ 0.5651],\n",
       "         [ 0.5848],\n",
       "         [ 0.5679],\n",
       "         [ 0.5975],\n",
       "         [ 0.5514],\n",
       "         [ 0.5909],\n",
       "         [ 0.5542],\n",
       "         [ 0.6194],\n",
       "         [ 0.5789],\n",
       "         [ 0.6097],\n",
       "         [ 0.5533],\n",
       "         [ 0.5585],\n",
       "         [ 0.5544],\n",
       "         [ 0.5997],\n",
       "         [ 0.5666],\n",
       "         [ 0.5631],\n",
       "         [ 0.5426],\n",
       "         [ 0.5222],\n",
       "         [ 0.5652],\n",
       "         [ 0.6318],\n",
       "         [ 0.5406],\n",
       "         [ 0.5582],\n",
       "         [ 0.5435],\n",
       "         [ 0.5703],\n",
       "         [ 0.5795],\n",
       "         [ 0.5926],\n",
       "         [ 0.6079],\n",
       "         [ 0.5899],\n",
       "         [ 0.5720],\n",
       "         [ 0.6223],\n",
       "         [ 0.6394],\n",
       "         [ 0.6079],\n",
       "         [ 0.6042],\n",
       "         [ 0.5973],\n",
       "         [ 0.5991],\n",
       "         [ 0.5491],\n",
       "         [ 0.5723],\n",
       "         [ 0.5858],\n",
       "         [ 0.5951],\n",
       "         [ 0.5385],\n",
       "         [ 0.5738],\n",
       "         [ 0.5894],\n",
       "         [ 0.5360],\n",
       "         [ 0.5565],\n",
       "         [ 0.5548],\n",
       "         [ 0.6083],\n",
       "         [ 0.5718],\n",
       "         [ 0.6208],\n",
       "         [ 0.5234],\n",
       "         [ 0.5936],\n",
       "         [ 0.6131],\n",
       "         [ 0.5621],\n",
       "         [ 0.5977],\n",
       "         [ 0.6083],\n",
       "         [ 0.5665],\n",
       "         [ 0.5747],\n",
       "         [ 0.5937],\n",
       "         [ 0.6404],\n",
       "         [ 0.5694],\n",
       "         [ 0.5402],\n",
       "         [ 0.5423],\n",
       "         [ 0.5690],\n",
       "         [ 0.5939],\n",
       "         [ 0.5781],\n",
       "         [ 0.5735],\n",
       "         [ 0.5824],\n",
       "         [ 0.5472],\n",
       "         [ 0.5755],\n",
       "         [ 0.5984],\n",
       "         [ 0.5724],\n",
       "         [ 0.6011],\n",
       "         [ 0.5375],\n",
       "         [ 0.6161],\n",
       "         [ 0.5341],\n",
       "         [ 0.5713],\n",
       "         [ 0.5300]]), Parameter containing:\n",
       " tensor(1.00000e-02 *\n",
       "        [[ 8.5088, -1.5600, -5.6675,  ...,  2.0301,  2.3178, -3.3647],\n",
       "         [-7.9848, -3.9405,  9.4985,  ..., -3.0883,  8.6849, -9.6646],\n",
       "         [-6.3717, -9.4457,  6.3752,  ...,  8.6937,  3.8632, -6.4120],\n",
       "         ...,\n",
       "         [ 1.1337,  2.6090, -5.2497,  ...,  6.1198, -6.7213,  3.4604],\n",
       "         [-8.3206,  5.1298, -9.5041,  ...,  6.1357, -1.3285, -4.8399],\n",
       "         [ 3.5104, -2.6357,  2.8092,  ..., -5.6834,  1.7366, -5.5056]]), Parameter containing:\n",
       " tensor([ 0.4227,  0.9431,  0.5234,  0.5736,  0.6019,  0.9470,  0.2145,\n",
       "          0.5270,  0.1425,  0.2927,  0.8372,  0.8228,  0.5727,  0.6355,\n",
       "          0.3313,  0.4489,  0.5964,  0.4643,  0.4096,  0.9395,  0.7818,\n",
       "          0.6287,  0.8661,  0.4013,  0.9646,  0.1477,  0.8996,  0.2755,\n",
       "          0.2251,  0.8239,  0.4212,  0.6509,  0.6719,  0.6610,  0.2083,\n",
       "          0.5531,  0.2277,  0.7692,  0.5895,  0.5852,  0.0892,  0.2517,\n",
       "          0.9834,  0.0174,  0.7999,  0.3532,  0.3916,  0.4285,  0.3856,\n",
       "          0.4747,  0.7048,  0.0479,  0.9933,  0.4444,  0.3579,  0.0461,\n",
       "          0.4002,  0.2469,  0.8778,  0.4932,  0.3764,  0.8699,  0.1752,\n",
       "          0.9467,  0.6383,  0.4988,  0.4160,  0.7182,  0.0137,  0.9250,\n",
       "          0.0070,  0.8816,  0.9837,  0.5026,  0.5028,  0.0862,  0.5451,\n",
       "          0.4982,  0.0458,  0.0921,  0.1055,  0.5691,  0.9177,  0.6389,\n",
       "          0.5528,  0.2535,  0.8757,  0.7678,  0.1011,  0.3450,  0.0244,\n",
       "          0.2390,  0.1388,  0.8070,  0.2482,  0.6465,  0.5099,  0.3024,\n",
       "          0.2144,  0.3813]), Parameter containing:\n",
       " tensor([ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
       "          0.,  0.,  0.,  0.]), Parameter containing:\n",
       " tensor(1.00000e-02 *\n",
       "        [ 9.4360,  4.0759]), Parameter containing:\n",
       " tensor([[ 0.5730],\n",
       "         [ 0.6109]]), Parameter containing:\n",
       " tensor(1.00000e-02 *\n",
       "        [[-4.7344, -5.0250,  8.6998, -0.7662, -4.0960,  0.9681, -7.4408,\n",
       "           2.8962, -5.3477,  0.9570,  0.5617,  3.6883, -5.7742,  8.9489,\n",
       "           8.1984,  3.8474,  5.3623,  1.0574,  7.2016,  5.6339, -9.5623,\n",
       "           8.2465,  9.6651,  0.8026, -2.0169,  0.0404,  0.6402, -3.9927,\n",
       "           1.0949, -8.2526,  9.2561,  7.6399, -1.3208, -6.2661, -0.2710,\n",
       "          -6.2231,  0.0840, -5.3064,  2.0573,  9.3214,  0.1396, -9.0959,\n",
       "          -6.7362,  8.7818, -3.5648,  8.4140, -3.8097, -3.0530, -1.6310,\n",
       "          -0.2015, -9.7960, -3.7967,  7.7440, -8.6996,  6.1253, -6.4912,\n",
       "           3.0716,  9.9754, -1.9662,  2.3201,  4.0914,  4.2448, -8.4420,\n",
       "          -6.7818,  3.1279, -8.5768, -5.6501,  3.7007, -0.6991, -5.3730,\n",
       "           3.2270,  9.9411, -3.7047,  2.6547, -1.9324, -1.7148,  1.6393,\n",
       "           2.9194, -6.2332, -3.8513,  8.9580,  9.5335,  6.4114,  6.5939,\n",
       "           7.3929, -4.0887,  0.2262,  1.8285,  8.1049,  5.2380, -6.7277,\n",
       "          -1.8715, -3.6208,  2.0167,  2.8338,  6.8912,  6.7655,  6.6150,\n",
       "          -9.1106,  7.0915],\n",
       "         [ 2.4794,  2.8760, -1.8988, -6.8789,  2.9413, -8.8631,  1.2414,\n",
       "          -8.8289,  2.2097,  2.6438, -7.8134,  9.2953,  8.2424, -1.4578,\n",
       "           1.8461, -4.3512, -7.3362, -5.2565,  4.5203,  7.7348,  4.7667,\n",
       "           9.5635, -7.7319, -0.7606,  8.4250,  1.3711, -3.4719, -6.1709,\n",
       "          -9.5769,  2.3678, -0.3315, -5.3760, -1.7810, -3.1914, -3.9576,\n",
       "          -3.6537,  3.0986,  3.0114, -1.9555,  9.4142, -7.4945, -7.0430,\n",
       "           1.6915,  8.2498, -6.0721, -3.2046,  5.4666, -5.1513, -9.6840,\n",
       "           8.6505,  2.4397, -0.9509,  0.8697, -6.4225,  8.2858,  8.2244,\n",
       "          -1.7334, -4.8250,  6.8590,  9.6735, -1.6574,  7.1484, -9.5450,\n",
       "          -2.4850, -0.0498, -2.5634,  2.9407,  8.6984, -9.4792,  9.8596,\n",
       "          -8.5419,  2.1339,  6.6227, -5.3247, -2.1529,  8.3739, -1.6432,\n",
       "           6.0704,  3.0246, -2.9430, -0.0695,  5.4659, -9.1650, -9.9102,\n",
       "          -9.3142, -3.9917,  9.3493,  9.2034,  0.9054, -9.5172,  5.0755,\n",
       "          -9.8759,  8.2252,  5.3776, -6.0436,  4.5301,  6.0599, -4.1387,\n",
       "          -9.3355,  0.7525]])]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(maddpg.maddpg_agent[0].actor.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.2804,  0.0672]])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maddpg.maddpg_agent[0].act(state_tensors[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(actions[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(torch.cat(actions, dim = -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "next_full_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "isinstance(actions, list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "type(actions[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "target_actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(next_states.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "isinstance(actions[0], torch.Tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Test training parameters step by step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# setting parameters\n",
    "# number of parallel environment, each environment has 2 agents\n",
    "# this would generate more experience and smooth things out\n",
    "# PARALLEL_ENVS = 1\n",
    "# Here we only have 1 env for simplicity\n",
    "\n",
    "# number of training episodes.\n",
    "# change this to higher number to experiment. say 30000.\n",
    "NUMBER_OF_EPISODES = 12\n",
    "EPISODE_LENGTH = 1000\n",
    "BATCHSIZE =3\n",
    "MIN_BUFFER_SIZE = 3\n",
    "UPDATE_EVERY = 1\n",
    "# amplitude of OU noise\n",
    "# this slowly decreases to 0\n",
    "# instead of resetting noise to 0 every episode, we let it decrease to 0 over a few episodes\n",
    "NOISE = 1\n",
    "NO_NOISE_AFTER = 10000\n",
    "NOISE_DECAY = 0.9999\n",
    "\n",
    "BUFFER_SIZE = 1000000\n",
    "\n",
    "IN_ACTOR_DIM = 24 \n",
    "HIDDEN_ACTOR_IN_DIM = 100\n",
    "HIDDEN_ACTOR_OUT_DIM = 100\n",
    "OUT_ACTOR_DIM = 2\n",
    "\n",
    "# Critic input contains both states AND all the actions of all the agents\n",
    "# there are 2 agents, so 24*2 + 2*2 = 28\n",
    "IN_CRIT_S = IN_ACTOR_DIM  * num_agents \n",
    "IN_CRIT_A = action_size * num_agents\n",
    "HIDDEN_CRIT_IN_DIM = 100\n",
    "HIDDEN_CRIT_OUT_DIM = 100\n",
    "OUT_CRIT_DIM = 1\n",
    "\n",
    "# how many periods before update\n",
    "\n",
    "SEED = 6\n",
    "DISC = 0.99\n",
    "TAU = 0.001\n",
    "LR_ACT = 1.e-4\n",
    "LR_CRI = 1.e-3\n",
    "\n",
    "\n",
    "\n",
    "# Initialization\n",
    "def process_data (states, actions, rewards, next_states, dones):\n",
    "    full_state = states.flatten()\n",
    "    next_full_state = next_states.flatten()\n",
    "    return (states, full_state, actions, rewards, next_states, next_full_state, dones)\n",
    "\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "t = 0\n",
    "\n",
    "\n",
    "# torch.set_num_threads(PARALLEL_ENVS)\n",
    "# env = envs.make_parallel_env(PARALLEL_ENVS)\n",
    "\n",
    "# keep 5000 episodes worth of replay\n",
    "\n",
    "\n",
    "# initialize policy and critic through MADDOG\n",
    "maddpg = MADDPG(IN_ACTOR_DIM, HIDDEN_ACTOR_IN_DIM, HIDDEN_ACTOR_OUT_DIM, OUT_ACTOR_DIM,\\\n",
    "                IN_CRIT_S, IN_CRIT_A, HIDDEN_CRIT_IN_DIM, HIDDEN_CRIT_OUT_DIM, SEED, LR_ACT, LR_CRI, DISC, TAU)\n",
    "\n",
    "# these will be used to print rewards for agents\n",
    "agent0_reward = []\n",
    "agent1_reward = []\n",
    "scores_deque = deque(maxlen=100)\n",
    "best_scores = []\n",
    "avg_best_score = []\n",
    "update_t = 0\n",
    "#     max_state =  env_info_demo.vector_observations[0]\n",
    "#     max_action = [0,0]\n",
    "times_updated = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "buffer = ReplayBuffer(int(BUFFER_SIZE))\n",
    "# # maddpg = MADDPG(IN_ACTOR_DIM, HIDDEN_ACTOR_IN_DIM, HIDDEN_ACTOR_OUT_DIM, OUT_ACTOR_DIM,\\\n",
    "# #                 IN_CRIT_S, IN_CRIT_A, HIDDEN_CRIT_IN_DIM, HIDDEN_CRIT_OUT_DIM, SEED, LR_ACT, LR_CRI, DISC, TAU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 1 1 14 0 [(0.2395658, 0.900802), (-0.18102472, 0.79633856)]\n",
      "0.0 1 2 13 0 [(-0.27538237, 0.8652375), (-0.22752392, -0.04210297)]\n",
      "0.0 1 3 13 0 [(0.49065253, -0.2558384), (0.10818676, 0.0653445)]\n",
      "0.0 1 4 13 0 [(0.13746555, -0.20754811), (0.35974222, -0.0018371493)]\n",
      "0.0 1 5 13 0 [(0.8821971, 0.5649713), (0.82261586, 0.37034607)]\n",
      "0.0 1 6 14 0 [(0.45597395, 0.044246167), (-0.40887004, 0.72332627)]\n",
      "0.0 1 7 13 0 [(-0.28096032, 0.000490278), (-0.07019459, -0.153855)]\n",
      "0.0 1 8 13 0 [(-0.08594951, 1.0), (0.14174736, -0.53325504)]\n",
      "0.0 1 9 13 0 [(0.033820122, -0.5606962), (-0.020705104, -0.56146836)]\n",
      "0.0 1 10 13 0 [(-0.4639236, 0.6014853), (0.36051536, 0.37391606)]\n",
      "0.0 1 11 13 0 [(-0.11057241, 0.8189635), (0.075384736, 1.0)]\n",
      "Episode 11\tBuffer Len 0\tAverage Last 100 Episodes Score: 0.00\n",
      "times_updated 0\n",
      "\n",
      "0.0 1 12 14 0 [(0.76398396, -0.1391693), (-0.5609775, -0.24289367)]\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# main function that sets up environments\n",
    "# perform training loop\n",
    "\n",
    "\n",
    "\n",
    "# use keep_awake to keep workspace from disconnecting\n",
    "for episode in range(1, NUMBER_OF_EPISODES+1):\n",
    "\n",
    "    env_info = env.reset(train_mode=True)[brain_name]\n",
    "    states = env_info.vector_observations\n",
    "\n",
    "    scores = [0,0]   \n",
    "    all_t_episode =[]\n",
    "\n",
    "    for episode_t in range(EPISODE_LENGTH):\n",
    "\n",
    "        state_tensors = convert_to_tensor(states)\n",
    "\n",
    "        if len(buffer) > NO_NOISE_AFTER:\n",
    "            NOISE *= NOISE_DECAY\n",
    "\n",
    "        \n",
    "        actions = maddpg.act(state_tensors, noise = NOISE)\n",
    "        actions_array = torch.stack(actions).detach().numpy()\n",
    "\n",
    "        env_info = env.step(actions_array)[brain_name] \n",
    "        next_states = env_info.vector_observations\n",
    "        rewards = env_info.rewards\n",
    "        dones  = env_info.local_done\n",
    "\n",
    "        transition = process_data(states, actions_array, rewards, next_states, dones)\n",
    "        #buffer.push(transition) \n",
    "                \n",
    "        scores = [sum(x) for x in zip(scores, rewards)]\n",
    "\n",
    "        states = next_states    \n",
    "\n",
    "\n",
    "        update_t = (update_t + 1) % UPDATE_EVERY\n",
    "        \n",
    "        if len(buffer) > MIN_BUFFER_SIZE and update_t == 0:\n",
    "            times_updated += 1\n",
    "            samplesa = buffer.sample(BATCHSIZE)\n",
    "            samplesb = buffer.sample(BATCHSIZE)\n",
    "            maddpg.update(samplesa, samplesb)\n",
    "\n",
    "        if dones[0]:\n",
    "            print(round(max(scores),1), NOISE, episode, episode_t, len(buffer), list(zip(*actions_array)))\n",
    "                \n",
    "        \n",
    "            break                      \n",
    "                    \n",
    "    agent0_reward.append(scores[0])\n",
    "    agent1_reward.append(scores[1])\n",
    "\n",
    "    best_scores.append(max(scores))\n",
    "    scores_deque.append(max(scores))    \n",
    "    avg_best_score.append(np.mean(scores_deque))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # print score every 100 episodes and save model \n",
    "    if episode % 100 == 0 or episode == NUMBER_OF_EPISODES-1 or  np.mean(scores_deque)>=0.5:\n",
    "\n",
    "        print('\\rEpisode {}\\tBuffer Len {}\\tAverage Last 100 Episodes Score: {:.2f}'.format(episode, len(buffer),np.mean(scores_deque)))\n",
    "        print(\"times_updated\", times_updated)\n",
    "        print()\n",
    "\n",
    "\n",
    "    # problem solved\n",
    "    if  np.mean(scores_deque)>=0.5 or times_updated >1:\n",
    "        print('\\nEnvironment solved in {:d} episodes!\\tAverage Last 100 Episodes Score: {:.2f}'.format(episode, np.mean(scores_deque)))            \n",
    "\n",
    "        break     \n",
    "#     if times_updated == 10:\n",
    "#         break\n",
    "\n",
    "\n",
    "\n",
    "print(len(buffer))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "states, full_state, actions, rewards, next_states, next_full_state, dones = map(transpose_to_tensor, samples)\n",
    "full_states = [samples[1], samples[5]]\n",
    "samples = [states, actions, rewards, next_states, dones]\n",
    "samples.extend(convert_to_tensor(full_states))  \n",
    "states, actions, rewards, next_states, dones, full_state, next_full_state = samples\n",
    "states = torch.stack(states)   \n",
    "next_states = torch.stack(next_states) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ -7.0089,   0.5568,   3.1098,   2.4874,   6.8317,   1.3903,\n",
       "            3.1098,   2.4874,  -6.0478,   0.7466,   9.6109,   1.5064,\n",
       "            6.8317,   0.4707,   9.6109,   1.5064,  -6.4336,   0.8384,\n",
       "           -3.8586,   0.5254,   6.8317,  -0.4489,  -3.8586,   0.5254],\n",
       "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
       "            0.0000,   0.0000,  -7.4364,  -1.5000,  -0.0000,   0.0000,\n",
       "            6.8317,   5.8587,  -0.0000,   0.0000,  -7.1158,  -1.5589,\n",
       "            3.2063,  -0.9810,   6.8317,   5.6429,   3.2063,  -0.9810],\n",
       "         [ -8.3302,  -1.1838,   0.0600,   6.4114,   6.8317,   4.4069,\n",
       "            0.0600,   6.4114,  -8.0220,  -0.6015,   3.0815,   5.4304,\n",
       "            6.8317,   3.7986,   3.0815,   5.4304,  -7.8264,  -0.1173,\n",
       "            1.9561,   4.4494,   6.8317,   3.0923,   1.9561,   4.4494]],\n",
       "\n",
       "        [[ -0.4000,  -1.8522,  30.0000,  -0.0000,  -6.8317,   1.3903,\n",
       "           30.0000,  -0.0000,  -0.4000,  -1.8522,  30.0000,  -0.0000,\n",
       "           -6.8317,   0.4707,  30.0000,  -0.0000,  -0.4468,  -1.8522,\n",
       "           27.6603,  -0.0000,  -6.8317,  -0.4489,  27.6603,  -0.0000],\n",
       "         [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
       "            0.0000,   0.0000,  -7.7302,  -1.5000,   0.0000,   0.0000,\n",
       "           -6.8317,   5.8587,   0.0000,   0.0000,  -7.2093,  -1.5589,\n",
       "            5.2092,  -0.9810,  -6.8317,   5.6429,   5.2092,  -0.9810],\n",
       "         [ -0.4000,  -1.8522,  30.0000,  -0.0000,  -6.8317,   4.4069,\n",
       "           30.0000,  -0.0000,  -0.4000,  -1.8522,  30.0000,  -0.0000,\n",
       "           -6.8317,   3.7986,  30.0000,  -0.0000,  -0.4110,  -1.8522,\n",
       "           29.4483,  -0.0000,  -6.8317,   3.0923,  29.4483,  -0.0000]]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import copy\n",
    "totest = copy.deepcopy (states)\n",
    "totest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "aaaa =[ [ -7.0089,   0.5568,   3.1098,   2.4874,   6.8317,   1.3903, 3.1098,   2.4874,  -6.0478,   0.7466,   9.6109,   1.5064, 6.8317,   0.4707,   9.6109,   1.5064,  -6.4336,   0.8384, -3.8586,   0.5254,   6.8317,  -0.4489,  -3.8586,   0.5254]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bbbb = [  0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000, 0.0000,   0.0000,  -7.4364,  -1.5000,  -0.0000,   0.0000, 6.8317,   5.8587,  -0.0000,   0.0000,  -7.1158,  -1.5589, 3.2063,  -0.9810,   6.8317,   5.6429,   3.2063,  -0.9810]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "aaaa.append(bbbb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "totest = convert_to_tensor([aaaa,aaaa])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[ 0.1225,  0.0145],\n",
       "         [ 0.0556, -0.0109]]), tensor([[ 0.1225,  0.0145],\n",
       "         [ 0.0556, -0.0109]])]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pre-update do not refresh\n",
    "actions = maddpg.act(totest) \n",
    "actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[ 0.1225,  0.0145],\n",
       "         [ 0.0556, -0.0109]]), tensor([[ 0.1225,  0.0145],\n",
       "         [ 0.0556, -0.0109]])]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# preupdate, do not refresh\n",
    "actions = maddpg.target_act(totest, noise = 0) \n",
    "actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[ 0.9491, -0.9785],\n",
       "         [ 0.9642, -0.9536]]), tensor([[ 0.3634, -0.7679],\n",
       "         [ 0.2861, -0.8780]])]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# post-update: refresh\n",
    "actions = maddpg.act(totest) \n",
    "actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[ 0.6874, -0.9446],\n",
       "         [ 0.7541, -0.8920]]), tensor([[ 0.1387, -0.7869],\n",
       "         [-0.0108, -0.8664]])]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# post update, refresh:\n",
    "actions = maddpg.target_act(totest, noise = 0) \n",
    "actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "drlnd",
   "language": "python",
   "name": "drlnd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
