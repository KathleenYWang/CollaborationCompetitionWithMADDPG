{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collaboration and Competition\n",
    "\n",
    "---\n",
    "\n",
    "In this notebook, you will learn how to use the Unity ML-Agents environment for the third project of the [Deep Reinforcement Learning Nanodegree](https://www.udacity.com/course/deep-reinforcement-learning-nanodegree--nd893) program.\n",
    "\n",
    "### 1. Start the Environment\n",
    "\n",
    "We begin by importing the necessary packages.  If the code cell below returns an error, please revisit the project instructions to double-check that you have installed [Unity ML-Agents](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Installation.md) and [NumPy](http://www.numpy.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import numpy as np\n",
    "\n",
    "import random\n",
    "import torch\n",
    "\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from maddpg import MADDPG, ReplayBuffer\n",
    "\n",
    "import os\n",
    "from utilities import transpose_list, transpose_to_tensor, convert_to_tensor\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will start the environment!  **_Before running the code cell below_**, change the `file_name` parameter to match the location of the Unity environment that you downloaded.\n",
    "\n",
    "- **Mac**: `\"path/to/Tennis.app\"`\n",
    "- **Windows** (x86): `\"path/to/Tennis_Windows_x86/Tennis.exe\"`\n",
    "- **Windows** (x86_64): `\"path/to/Tennis_Windows_x86_64/Tennis.exe\"`\n",
    "- **Linux** (x86): `\"path/to/Tennis_Linux/Tennis.x86\"`\n",
    "- **Linux** (x86_64): `\"path/to/Tennis_Linux/Tennis.x86_64\"`\n",
    "- **Linux** (x86, headless): `\"path/to/Tennis_Linux_NoVis/Tennis.x86\"`\n",
    "- **Linux** (x86_64, headless): `\"path/to/Tennis_Linux_NoVis/Tennis.x86_64\"`\n",
    "\n",
    "For instance, if you are using a Mac, then you downloaded `Tennis.app`.  If this file is in the same folder as the notebook, then the line below should appear as follows:\n",
    "```\n",
    "env = UnityEnvironment(file_name=\"Tennis.app\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: TennisBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 8\n",
      "        Number of stacked Vector Observation: 3\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 2\n",
      "        Vector Action descriptions: , \n"
     ]
    }
   ],
   "source": [
    "env = UnityEnvironment(file_name='env\\Tennis.app')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environments contain **_brains_** which are responsible for deciding the actions of their associated agents. Here we check for the first brain available, and set it as the default brain we will be controlling from Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Examine the State and Action Spaces\n",
    "\n",
    "In this environment, two agents control rackets to bounce a ball over a net. If an agent hits the ball over the net, it receives a reward of +0.1.  If an agent lets a ball hit the ground or hits the ball out of bounds, it receives a reward of -0.01.  Thus, the goal of each agent is to keep the ball in play.\n",
    "\n",
    "The observation space consists of 8 variables corresponding to the position and velocity of the ball and racket. Two continuous actions are available, corresponding to movement toward (or away from) the net, and jumping. \n",
    "\n",
    "Run the code cell below to print some information about the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents: 2\n",
      "Size of each action: 2\n",
      "There are 2 agents. Each observes a state with length: 24\n",
      "The state for the first agent looks like: [[ 0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.         -6.65278625 -1.5\n",
      "  -0.          0.          6.83172083  6.         -0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.         -6.4669857  -1.5\n",
      "   0.          0.         -6.83172083  6.          0.          0.        ]]\n"
     ]
    }
   ],
   "source": [
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "\n",
    "# number of agents \n",
    "num_agents = len(env_info.agents)\n",
    "print('Number of agents:', num_agents)\n",
    "\n",
    "# size of each action\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Size of each action:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "states = env_info.vector_observations\n",
    "state_size = states.shape[1]\n",
    "print('There are {} agents. Each observes a state with length: {}'.format(states.shape[0], state_size))\n",
    "print('The state for the first agent looks like:', states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        , -6.65278625, -1.5       , -0.        ,  0.        ,\n",
       "         6.83172083,  6.        , -0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        , -6.4669857 , -1.5       ,  0.        ,  0.        ,\n",
       "        -6.83172083,  6.        ,  0.        ,  0.        ]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env_info.vector_observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        , -6.65278625, -1.5       , -0.        ,  0.        ,\n",
       "         6.83172083,  6.        , -0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        , -6.4669857 , -1.5       ,  0.        ,  0.        ,\n",
       "        -6.83172083,  6.        ,  0.        ,  0.        ]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env_info.vector_observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[False, False]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env_info.local_done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "actions = [[0.2,0.3],[0.5,0.6]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "actions_array = torch.stack(transpose_to_tensor(actions)).detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "actions_for_env = np.rollaxis(actions_array,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "env_info = env.step(actions_for_env)[brain_name] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        , -6.65278625, -1.5       ,\n",
       "        -0.        ,  0.        ,  6.83172083,  6.        , -0.        ,\n",
       "         0.        , -6.05278683, -1.55886006,  6.        , -0.98100001,\n",
       "         6.83172083,  5.94114017,  6.        , -0.98100001],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        , -6.4669857 , -1.5       ,\n",
       "         0.        ,  0.        , -6.83172083,  6.        ,  0.        ,\n",
       "         0.        , -4.96698475, -0.98316395, 15.        ,  6.21520042,\n",
       "        -6.83172083,  5.94114017, 15.        ,  6.21520042]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env_info.vector_observations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Train MADDPG!\n",
    "\n",
    "To deploy our agent to solve the navigation problem, we first import the agent class we wrote. When training the environment, set train_mode=True, so that the line for resetting the environment looks like the following:\n",
    "\n",
    "```python\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# setting parameters\n",
    "# number of parallel environment, each environment has 2 agents\n",
    "# this would generate more experience and smooth things out\n",
    "# PARALLEL_ENVS = 1\n",
    "# Here we only have 1 env for simplicity\n",
    "\n",
    "# number of training episodes.\n",
    "# change this to higher number to experiment. say 30000.\n",
    "NUMBER_OF_EPISODES = 1000\n",
    "EPISODE_LENGTH = 80\n",
    "BATCHSIZE = 30\n",
    "\n",
    "# amplitude of OU noise\n",
    "# this slowly decreases to 0\n",
    "# instead of resetting noise to 0 every episode, we let it decrease to 0 over a few episodes\n",
    "NOISE = 2\n",
    "NOISE_REDUCTION = 0.9999\n",
    "BUFFER_SIZE = 5000\n",
    "\n",
    "IN_ACTOR_DIM = 24 \n",
    "HIDDEN_ACTOR_IN_DIM = 400\n",
    "HIDDEN_ACTOR_OUT_DIM = 300\n",
    "OUT_ACTOR_DIM = 2\n",
    "\n",
    "# Critic input contains both states AND all the actions of all the agents\n",
    "# there are 2 agents, so 24*2 + 2*2 = 28\n",
    "IN_CRIT_DIM = IN_ACTOR_DIM  * num_agents + action_size * num_agents\n",
    "HIDDEN_CRIT_IN_DIM = 400\n",
    "HIDDEN_CRIT_OUT_DIM = 300\n",
    "OUT_CRIT_DIM = 1\n",
    "\n",
    "# how many episodes before update\n",
    "UPDATE_EVERY = 2\n",
    "SEED = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# main function that sets up environments\n",
    "# perform training loop\n",
    "\n",
    "\n",
    "def pre_process(entity, batchsize):\n",
    "    processed_entity = []\n",
    "    for j in range(3):\n",
    "        list = []\n",
    "        for i in range(batchsize):\n",
    "            b = entity[i][j]\n",
    "            list.append(b)\n",
    "        c = torch.Tensor(list)\n",
    "        processed_entity.append(c)\n",
    "    return processed_entity\n",
    "\n",
    "def run_maddpg():\n",
    "    \n",
    "    np.random.seed(SEED)\n",
    "    torch.manual_seed(SEED)\n",
    "    t = 0\n",
    "    \n",
    "    noise = NOISE\n",
    "    noise_reduction = NOISE_REDUCTION\n",
    "\n",
    "    \n",
    "    log_path = os.getcwd()+\"/log\"\n",
    "    model_dir= os.getcwd()+\"/model_dir\"\n",
    "    \n",
    "    os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "    # torch.set_num_threads(PARALLEL_ENVS)\n",
    "    # env = envs.make_parallel_env(PARALLEL_ENVS)\n",
    "    \n",
    "    # keep 5000 episodes worth of replay\n",
    "\n",
    "    buffer = ReplayBuffer(int(BUFFER_SIZE*EPISODE_LENGTH))\n",
    "    # initialize policy and critic through MADDOG\n",
    "    maddpg = MADDPG(IN_ACTOR_DIM, HIDDEN_ACTOR_IN_DIM, HIDDEN_ACTOR_OUT_DIM, OUT_ACTOR_DIM, IN_CRIT_DIM, HIDDEN_CRIT_IN_DIM, HIDDEN_CRIT_OUT_DIM)\n",
    "    \n",
    "    # these will be used to print rewards for agents\n",
    "    agent0_reward = []\n",
    "    agent1_reward = []\n",
    "    scores_deque = deque(maxlen=100)\n",
    "    best_scores = []\n",
    "    update_t = 0\n",
    "\n",
    "\n",
    "    # training loop\n",
    "    # show progressbar\n",
    "#     import progressbar as pb\n",
    "#     widget = ['episode: ', pb.Counter(),'/',str(NUMBER_OF_EPISODES),' ', \n",
    "#               pb.Percentage(), ' ', pb.ETA(), ' ', pb.Bar(marker=pb.RotatingMarker()), ' ' ]\n",
    "    \n",
    "#     timer = pb.ProgressBar(widgets=widget, maxval=NUMBER_OF_EPISODES).start()\n",
    "\n",
    "    # use keep_awake to keep workspace from disconnecting\n",
    "    for episode in range(1, NUMBER_OF_EPISODES+1):\n",
    "\n",
    "#         timer.update(episode)\n",
    "        \n",
    "        env_info = env.reset(train_mode=True)[brain_name]\n",
    "        states = env_info.vector_observations\n",
    "\n",
    "        \n",
    "        # initialize scores for both agents\n",
    "        scores = [0,0]   \n",
    "        \n",
    "        # in lab, all_obs came directly from env, it has 4 lists, each list has 1 3x14 and 1 1x14\n",
    "        # obs has 4 lists, getting only the 3x14 from all_obs\n",
    "        # obs_full on the other hand got the 1x14\n",
    "        \n",
    "\n",
    "        #for calculating rewards for this particular episode - addition of all time steps\n",
    "\n",
    "        for episode_t in range(EPISODE_LENGTH):\n",
    "\n",
    "            # explore = only explore for a certain number of episodes\n",
    "            # action input needs to be transposed\n",
    "            \n",
    "            state_tensors = convert_to_tensor(states)\n",
    "\n",
    "            \n",
    "\n",
    "            actions = maddpg.act(state_tensors, noise = 2)\n",
    "           \n",
    "            noise *= noise_reduction\n",
    "            \n",
    "            actions_array = torch.stack(actions).detach().numpy()\n",
    "       \n",
    "            \n",
    "            # act (actions)       \n",
    "            # [tensor([ 0.9857,  0.0912]), tensor([ 0.0951, -0.1229])]\n",
    "            # stack (actions_array)             \n",
    "            # [[ 0.98568964  0.09124897]\n",
    "            #  [ 0.0950533  -0.12286544]]\n",
    "\n",
    "            # step forward one frame\n",
    "            \n",
    "            env_info = env.step(actions_array)[brain_name] \n",
    "            \n",
    "            next_states = env_info.vector_observations\n",
    "\n",
    "            rewards = env_info.rewards\n",
    "            dones  = env_info.local_done\n",
    "            \n",
    "            update_t = (update_t + 1) % UPDATE_EVERY\n",
    "            \n",
    "            \n",
    "            # add data to buffer            \n",
    "            transition = (states, actions_for_env, rewards, next_states, dones)\n",
    "            buffer.push(transition)\n",
    "                \n",
    "            # update once after every UPDATE_EVERY (if multiple env, we have to update every time for each env)\n",
    "            if len(buffer) > BATCHSIZE and update_t == 0:\n",
    "\n",
    "                for a_i in range(num_agents):\n",
    "                    samples = buffer.sample(BATCHSIZE)\n",
    "\n",
    "                    maddpg.update(samples, a_i)\n",
    "\n",
    "                maddpg.update_targets() #soft update the target network towards the actual networks\n",
    "\n",
    "            # after updating the networks and getting new scores, update the tracking lists\n",
    "            agent0_reward.append(scores[0])\n",
    "            agent1_reward.append(scores[1])\n",
    "\n",
    "            best_scores.append(max(scores))\n",
    "            scores_deque.append(max(scores))            \n",
    "                    \n",
    "           \n",
    "            states = next_states\n",
    "            scores += rewards\n",
    "            \n",
    "            # if one of the agent is done we are done\n",
    "            if dones[0]:\n",
    "                break  \n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "        # print score every 100 episodes and save model \n",
    "        if episode % 100 == 0 or episode == NUMBER_OF_EPISODES-1 or  np.mean(scores_deque)>=0.5:\n",
    "            \n",
    "            for i in range(num_agents):   \n",
    "                save_dict = {'actor_params' : maddpg.maddpg_agent[i].actor.state_dict(),\n",
    "                             'actor_optim_params': maddpg.maddpg_agent[i].actor_optimizer.state_dict(),\n",
    "                             'critic_params' : maddpg.maddpg_agent[i].critic.state_dict(),\n",
    "                             'critic_optim_params' : maddpg.maddpg_agent[i].critic_optimizer.state_dict()}            \n",
    "\n",
    "                torch.save(save_dict, os.path.join(model_dir, 'checkpoint.pth'))\n",
    "            \n",
    "            print('\\rEpisode {}\\tAverage Last 100 Episodes Score: {:.2f}'.format(episode, np.mean(scores_deque)))\n",
    "\n",
    "        # problem solved\n",
    "        if  np.mean(scores_deque)>=0.5:\n",
    "            print('\\nEnvironment solved in {:d} episodes!\\tAverage Last 100 Episodes Score: {:.2f}'.format(i_episode, np.mean(scores_deque)))            \n",
    "\n",
    "            break             \n",
    "            \n",
    "\n",
    "        \n",
    "   \n",
    "\n",
    "#     timer.finish()\n",
    "    return  agent0_reward,  agent1_reward, best_scores\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 100\tAverage Last 100 Episodes Score: 0.00\n",
      "Episode 200\tAverage Last 100 Episodes Score: 0.03\n",
      "Episode 300\tAverage Last 100 Episodes Score: 0.02\n",
      "Episode 400\tAverage Last 100 Episodes Score: 0.04\n",
      "Episode 500\tAverage Last 100 Episodes Score: 0.00\n",
      "Episode 600\tAverage Last 100 Episodes Score: 0.00\n",
      "Episode 700\tAverage Last 100 Episodes Score: 0.00\n",
      "Episode 800\tAverage Last 100 Episodes Score: 0.00\n",
      "Episode 900\tAverage Last 100 Episodes Score: 0.00\n",
      "Episode 999\tAverage Last 100 Episodes Score: 0.00\n",
      "Episode 1000\tAverage Last 100 Episodes Score: 0.00\n"
     ]
    }
   ],
   "source": [
    "agent0, agent1, best_scores = run_maddpg()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot the scores\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(len(best_scores)), best_scores)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When finished, you can close the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ 0  1  2  3]\n",
      "  [ 4  5  6  7]\n",
      "  [ 8  9 10 11]]\n",
      "\n",
      " [[12 13 14 15]\n",
      "  [16 17 18 19]\n",
      "  [20 21 22 23]]]\n",
      "[[ 0  1  2  3]\n",
      " [ 4  5  6  7]\n",
      " [ 8  9 10 11]\n",
      " [12 13 14 15]\n",
      " [16 17 18 19]\n",
      " [20 21 22 23]]\n"
     ]
    }
   ],
   "source": [
    "a = np.arange(24)\n",
    "\n",
    "print(a.reshape(2,3,4))\n",
    "\n",
    "print(a.reshape(6,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Exception calling application: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\kathl\\AppData\\Local\\conda\\conda\\envs\\drlnd\\lib\\multiprocessing\\connection.py\", line 312, in _recv_bytes\n",
      "    nread, err = ov.GetOverlappedResult(True)\n",
      "BrokenPipeError: [WinError 109] The pipe has been ended\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\kathl\\AppData\\Local\\conda\\conda\\envs\\drlnd\\lib\\site-packages\\grpc\\_server.py\", line 385, in _call_behavior\n",
      "    return behavior(argument, context), True\n",
      "  File \"C:\\Users\\kathl\\AppData\\Local\\conda\\conda\\envs\\drlnd\\lib\\site-packages\\unityagents\\rpc_communicator.py\", line 26, in Exchange\n",
      "    return self.child_conn.recv()\n",
      "  File \"C:\\Users\\kathl\\AppData\\Local\\conda\\conda\\envs\\drlnd\\lib\\multiprocessing\\connection.py\", line 250, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"C:\\Users\\kathl\\AppData\\Local\\conda\\conda\\envs\\drlnd\\lib\\multiprocessing\\connection.py\", line 321, in _recv_bytes\n",
      "    raise EOFError\n",
      "EOFError\n"
     ]
    }
   ],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.          0.          0.          0.          0.          0.\n",
      "  0.          0.         -6.65278625 -1.5        -0.          0.\n",
      "  6.83172083  6.         -0.          0.         -6.05278683 -1.55886006\n",
      "  6.         -0.98100001  6.83172083  5.94114017  6.         -0.98100001]\n",
      "[0.2, 0.3]\n",
      "[ 0.          0.          0.          0.          0.          0.\n",
      "  0.          0.         -6.4669857  -1.5         0.          0.\n",
      " -6.83172083  6.          0.          0.         -5.56698656 -0.98316395\n",
      "  9.          6.21520042 -6.83172083  5.94114017  9.          6.21520042]\n",
      "[0.5, 0.6]\n"
     ]
    }
   ],
   "source": [
    "for state, action in zip(env_info.vector_observations,actions ):\n",
    "    print(state)\n",
    "    print(action)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "drlnd",
   "language": "python",
   "name": "drlnd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
